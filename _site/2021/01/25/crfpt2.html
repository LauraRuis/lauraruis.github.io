<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Structured Prediction part two - Implementing a linear-chain CRF | Ramblings</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Structured Prediction part two - Implementing a linear-chain CRF" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Blog about AI stuff." />
<meta property="og:description" content="Blog about AI stuff." />
<link rel="canonical" href="/2021/01/25/crfpt2.html" />
<meta property="og:url" content="/2021/01/25/crfpt2.html" />
<meta property="og:site_name" content="Ramblings" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-01-25T13:09:17-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Structured Prediction part two - Implementing a linear-chain CRF" />
<script type="application/ld+json">
{"headline":"Structured Prediction part two - Implementing a linear-chain CRF","dateModified":"2021-01-25T13:09:17-05:00","datePublished":"2021-01-25T13:09:17-05:00","url":"/2021/01/25/crfpt2.html","mainEntityOfPage":{"@type":"WebPage","@id":"/2021/01/25/crfpt2.html"},"description":"Blog about AI stuff.","@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="/feed.xml" title="Ramblings" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Ramblings</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Structured Prediction part two - Implementing a linear-chain CRF</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2021-01-25T13:09:17-05:00" itemprop="datePublished">Jan 25, 2021
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<p>In this part of the series of posts on structured prediction with conditional random fields (CRFs) we are going to implement all the ingredients
that were discussed in <a href="/2021/01/25/crfpt1.html">part 1</a>. Recall that we discussed how to model
the dependencies among labels in sequence prediction tasks with a linear-chain CRF. Now, we will put a such a CRF
on top of a neural network feature extractor and use it for
part-of-speech (POS) tagging.</p>

<p>Everything below is inspired by <a href="https://arxiv.org/abs/1603.01354" target="_blank">this paper</a> by Ma &amp; Hovy,
and the implementation has lots of parts that come from
<a href="https://github.com/allenai/allennlp/blob/main/allennlp/modules/conditional_random_field.py" target="_blank">the AllenNLP implementation</a>,
so if it’s simply a good implementation you’re looking for, take a look at that one. If you’d like to understand how it works from scratch, keep on reading. Bear with me here,
I discuss everything rather in detail and try not to skip over anything, from batching and broadcasting to changing the forward-recursion of BP for a cleaner implementation and Viterbi,
so if you rather want a succint blogpost you might want to choose one of the great other options that are also out there! Alternatively, there are some sections you can skip
if they’re clear, like batching and broadcasting. Let’s start!</p>

<p><img src="/images/opener_gif/opener.gif" alt="annotated_example" /></p>

<p>To learn a model that can annotate examples like the one above with POS tags, we need to extract proper
features from the input sequence, which we will do with a bidirectional LSTM (motivated below). 
Then what we’ll implement in this post is:</p>

<ul>
  <li>
    <p>An <code class="language-plaintext highlighter-rouge">Encoder</code> model that holds our feature extractor (the bidirectional LSTM).</p>
  </li>
  <li>
    <p>A <code class="language-plaintext highlighter-rouge">ChainCRF</code> model that implements all the CRF methods, like belief propagation (BP) and Viterbi decoding.</p>
  </li>
</ul>

<p>The end-to-end model we get by walking through this post is depicted in the image below.</p>

<p><img src="/images/bilstmcrf.png" alt="bilstmcrf" width="600" class="center" /></p>

<p>To train this model end-to-end we will use the negative log-likelihood (NLL) loss function, which is simply the negated log-likelihood
that was given in part 1 of this series. Given some example input-output pairs \((\mathbf{x}^{(i)}, \mathbf{y}^{(i)})_{i=1}^N\),
the NLL of the entire dataset is:</p>

\[\begin{aligned}
\text{NLL} = -\log \mathcal{L}(\boldsymbol{\theta}) &amp;= - \sum_{i=1}^{N} \log p(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)}) \\
&amp;= \sum_{i=1}^{N}\log\left(Z(\mathbf{x}^{(i)})\right) - \sum_{i=1}^{N}\left(\sum_{t=1}^m \boldsymbol{\theta}_1 f(y_t^{(i)}, \mathbf{x}^{(i)}, t) + \sum_{t=1}^{m-1} \boldsymbol{\theta}_2 f(y_t^{(i)}, y_{t+1}^{(i)})\right) \\
\end{aligned}\]

<p>Instead of maximizing the log-likelihood of our data, we will minimize the negative log-likelihood, which is equivalent.
We can use stochastic gradient descent (SGD) with automatic differentiation in <a href="https://pytorch.org/" target="_blank">PyTorch</a>, meaning we only need the forward-part of the BP algorithm.
Recall that the forward recursion allows calculation of the partition function (\(Z(\mathbf{x})\)), which we need for the NLL. The backward recursion allows calculating the
marginals (which are needed for the gradients). PyTorch takes care of the latter calculation for us (<code class="language-plaintext highlighter-rouge">&lt;3</code> PyTorch).</p>

<p>Let’s start with feature extraction and defining \(\boldsymbol{\theta}_1\), \(\boldsymbol{\theta}_2\), \(f(y_t, \mathbf{x}, t)\), and \(f(y_t, y_{t+1})\).</p>

<h2 id="preliminaries"><span style="color:#C0392B">Preliminaries</span></h2>

<p>If you want to run the code used in this post yourself, make sure to install <strong>PyTorch &gt;= 1.7.0</strong>,
<strong>Python 3.6</strong>, and <strong>TorchNLP &gt;= 0.5.0</strong>.</p>

<h2 id="feature-extraction"><span style="color:#C0392B">Feature Extraction</span></h2>

<p>There are some desiderata for our features. The function \(f(y_t, \mathbf{x}, t)\) signifies that we want each tag in the output sequence \(y_t\) to be informed about (i.e., depend on) the entire input sequence \(\mathbf{x}\),
but also on the current word at time \(t\).
Furthermore, \(f(y_t, y_{t+1})\) tells us that we want each next output tag \(y_{t+1}\) to depend on the previous tag \(y_{t}\).
Parametrizing the former part, we take \(\boldsymbol{\theta}_1f(y_t, \mathbf{x}, t)\) to be the output of a bidirectional LSTM projected down to the right dimension with a linear layer:</p>

\[\begin{aligned}
\mathbf{\bar{H}} &amp;= \text{biLSTM}(\mathbf{x}) \\
\mathbf{H} &amp;= \mathbf{\bar{H}}\mathbf{W} + \mathbf{b}
\end{aligned}\]

<p>In the above, \(\mathbf{x} \in \mathbb{R}^{m}\) is our input sequence, \(\mathbf{\bar{H}} \in \mathbb{R}^{m \times 2d_h}\) the hidden vectors for each input word \(x_t\) stacked into a matrix, with \(d_h\) the hidden dimension of the LSTM (doubled because
a bidirectional LSTM is essentially two LSTMs processing the input sequence from left-to-right and right-to-left).
\(\mathbf{W} \in \mathbb{R}^{2d_h \times |S|}\) a matrix of parameters that projects the output to the right dimension, namely \(|S|\) values for each input word \(x_t\). The t-th row of \(\mathbf{H} \in \mathbb{R}^{m \times |S|}\)
(let’s define that by \(\mathbf{H}_{t*}\)) then holds the features for the t-th word (\(x_t\)) in the input sequence. 
These values reflect \(\boldsymbol{\theta}_1f(y_t, \mathbf{x}_t, t)\) for each possible \(y_t\), since they depend on the entire input sequence \(\mathbf{x}\), but are specific to the current word at time \(t\).</p>

\[\boldsymbol{\theta}_1f(y_t, \mathbf{x}_t, t) = \mathbf{H}_{t,y_t}\]

<p>Using PyTorch, we can code this up in a few lines. As is common in computational models of language, we will assign each
input token a particular index and use dense word embeddings that represent each word in our input vocabulary. We reserve
index 0 for the special token <code class="language-plaintext highlighter-rouge">&lt;PAD&gt;</code>, which we need later when we will batch our examples, grouping together examples
of different length \(m\).</p>

<p>Strictly speaking btw, our bi-LSTM does not take \(\mathbf{x}\) as input but \(\mathbf{E}_{x_t*}\), where
\(\mathbf{E}\) is the matrix of embedding parameters of size \(|S| \times d_e\). So if \(x_t = 3\) (index 3 in our vocabulary, which might
be mapping to <em>book</em> for example), \(\mathbf{E}_{x_t*}\) takes out the corresponding embedding of size \(d_e\).</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">Encoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="s">"""
  A simple encoder model to encode sentences. Bi-LSTM over word embeddings.
  """</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                     <span class="n">hidden_dimension</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">padding_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Encoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
    <span class="c1"># The word embeddings.
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">num_embeddings</span><span class="o">=</span><span class="n">vocabulary_size</span><span class="p">,</span> 
                                  <span class="n">embedding_dim</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">,</span>
                                  <span class="n">padding_idx</span><span class="o">=</span><span class="n">padding_idx</span><span class="p">)</span>
    
    <span class="c1"># The bi-LSTM.
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">bi_lstm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">,</span> 
                           <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_dimension</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                           <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
  
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentence</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="s">"""
    :param sentence: input sequence of size [batch_size, sequence_length]
    Returns: tensor of size [batch_size, sequence_length, hidden_size * 2] 
    the hidden states of the biLSTM for each time step.
    """</span>
    <span class="n">embedded</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
    <span class="c1"># embedded: [batch_size, sequence_length, embedding_dimension]
</span>    
    <span class="n">output</span><span class="p">,</span> <span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">cell</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">bi_lstm</span><span class="p">(</span><span class="n">embedded</span><span class="p">)</span>
    <span class="c1"># output: [batch_size, sequence_length, hidden_size * 2]
</span>    <span class="c1"># hidden: [batch_size, hidden_size * 2]
</span>    <span class="c1"># cell: [batch_size, hidden_size * 2]
</span>    <span class="k">return</span> <span class="n">output</span></code></pre></figure>

<p>That’s all we need to do to extract features from out input sequences! Later in this post we will define \(\boldsymbol{\theta}_2\) 
and \(f(y_t, y_{t+1})\), which are part of the actual CRF. First, we’ll set up our <code class="language-plaintext highlighter-rouge">Tagger</code>-module which takes the encoder,
the CRF (to implement later), and outputs the negative log-likelihood and a predicted tag sequence.</p>

<h2 id="the-tagger"><span style="color:#C0392B">The Tagger</span></h2>

<p>Below we can find the <code class="language-plaintext highlighter-rouge">Tagger</code> module. This is basically the class that implements the model as depicted in the image
above. The most interesting part is still missing (namely the <code class="language-plaintext highlighter-rouge">ChainCRF</code>-module, but also the <code class="language-plaintext highlighter-rouge">Vocabulary</code>-module), but we’ll get to those. The
 forward pass of the <code class="language-plaintext highlighter-rouge">Tagger</code> takes an input sequence, a target sequence, and an input mask (we’ll get to what that is
 when we discuss batching), puts the input sequence through the encoder and the CRF, and outputs the NLL <code class="language-plaintext highlighter-rouge">loss</code>, the <code class="language-plaintext highlighter-rouge">score</code> (
 which is basically the nominator of \(p(\mathbf{y} \mid \mathbf{x})\)), and the <code class="language-plaintext highlighter-rouge">tag_sequence</code> obtained by decoding with viterbi.
If you don’t have a feeling of what the parameters <code class="language-plaintext highlighter-rouge">input_mask</code> and <code class="language-plaintext highlighter-rouge">input_lengths</code> in the module should be, don’t worry,
that will be discussed in the section ‘sequence batching’ below.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">Tagger</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="s">"""
  A POS tagger.
  """</span>
  
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_vocabulary</span><span class="p">:</span> <span class="n">Vocabulary</span><span class="p">,</span> 
               <span class="n">target_vocabulary</span><span class="p">:</span> <span class="n">Vocabulary</span><span class="p">,</span>
               <span class="n">embedding_dimension</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">hidden_dimension</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Tagger</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
    <span class="c1"># The Encoder to extract features from the input sequence.
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span><span class="n">vocabulary_size</span><span class="o">=</span><span class="n">input_vocabulary</span><span class="p">.</span><span class="n">size</span><span class="p">,</span> 
                           <span class="n">embedding_dim</span><span class="o">=</span><span class="n">embedding_dimension</span><span class="p">,</span> 
                           <span class="n">hidden_dimension</span><span class="o">=</span><span class="n">hidden_dimension</span><span class="p">,</span> 
                           <span class="n">padding_idx</span><span class="o">=</span><span class="n">input_vocabulary</span><span class="p">.</span><span class="n">pad_idx</span><span class="p">)</span>
    
    <span class="c1"># The linear projection (with parameters W and b).  
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">encoder_to_tags</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dimension</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> 
                                     <span class="n">target_vocabulary</span><span class="p">.</span><span class="n">size</span><span class="p">)</span>
    
    <span class="c1"># The linear-chain CRF.
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">tagger</span> <span class="o">=</span> <span class="n">ChainCRF</span><span class="p">(</span><span class="n">num_tags</span><span class="o">=</span><span class="n">target_vocabulary</span><span class="p">.</span><span class="n">size</span><span class="p">,</span> 
                           <span class="n">tag_vocabulary</span><span class="o">=</span><span class="n">target_vocabulary</span><span class="p">)</span>
  
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_sequence</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> 
              <span class="n">target_sequence</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> 
              <span class="n">input_mask</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="s">"""
    :param input_sequence: input sequence of size 
            [batch_size, sequence_length, input_vocabulary_size]
    :param target_sequence: POS tags target, [batch_size, sequence_length]
    :param input_mask: padding-mask, [batch_size, sequence_length]
    :param input_lengths: lengths of each example in the batch [batch_size]
    Returns: ...
    """</span>
    <span class="c1"># input_sequence: [batch_size, sequence_length, input_vocabulary_size]
</span>    <span class="n">lstm_features</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">input_sequence</span><span class="p">)</span>
    <span class="c1"># lstm_features: [batch_size, sequence_length, hidden_dimension*2]
</span>    
    <span class="n">crf_features</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">encoder_to_tags</span><span class="p">(</span><span class="n">lstm_features</span><span class="p">)</span>
    <span class="c1"># crf_features: [batch_size, sequence_length, target_vocabulary_size]
</span>    
    <span class="n">loss</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span> <span class="n">tag_sequence</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">tagger</span><span class="p">(</span><span class="n">input_features</span><span class="o">=</span><span class="n">crf_features</span><span class="p">,</span>
                                            <span class="n">target_tags</span><span class="o">=</span><span class="n">target_sequence</span><span class="p">,</span>
                                            <span class="n">input_mask</span><span class="o">=</span><span class="n">input_mask</span><span class="p">,</span>
                                            <span class="n">input_lengths</span><span class="o">=</span><span class="n">input_lengths</span><span class="p">)</span>
    <span class="c1"># loss, score: scalars
</span>    <span class="c1"># tag_sequence: [batch_size, sequence_length]
</span>    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span> <span class="n">tag_sequence</span></code></pre></figure>

<p>OK, now we can finally get to the definition of \(\boldsymbol{\theta}_2\) and \(f(y_t, y_{t+1})\), and the implementation of the linear-chain CRF!</p>

<h2 id="implementing-a-linear-chain-crf"><span style="color:#C0392B">Implementing a Linear-Chain CRF</span></h2>

<p>We want \(f(y_t, y_{t+1})\) to represent the likelihood of some tag \(y_{t+1}\) following \(y_t\) in the sequence, which
can be interpreted as transition likelihood from one tag to another. We assumed the parameters \(\boldsymbol{\theta}_2\) are shared
over time (meaning they are the same for each \(t \in \{1, \dots, m-1\}\)), and thus we can simply define a matrix of transition ‘probabilities’ from each tag to another tag.
We define \(\boldsymbol{\theta}_2f(y_t, y_{t+1}) = \mathbf{T}_{y_t,y_{t+1}}\), meaning the \(y_t\)-th row (recall that \(y_t\) is an index that represents a tag) and \(y_{t+1}\)-th column of a matrix \(\mathbf{T}\).
This matrix \(\mathbf{T}\) will be of size \((|S| + 2) \times (|S| + 2)\). The 2 extra tags are the <code class="language-plaintext highlighter-rouge">&lt;ROOT&gt;</code> and the <code class="language-plaintext highlighter-rouge">&lt;EOS&gt;</code> tag.
We need some tag to start of the sequence and to end the sequence, because we want to take into account the probability of a particular tag being the
first tag of a sequence, and the probability of a tag being the last tag. For the former we will use <code class="language-plaintext highlighter-rouge">&lt;ROOT&gt;</code>, and a for the latter we’ll use <code class="language-plaintext highlighter-rouge">&lt;EOS&gt;</code>.</p>

<p>In this part we will implement:</p>

<ul>
  <li>
    <p>The forward-pass of belief propagation (<code class="language-plaintext highlighter-rouge">ChainCRF.forward_belief_propagation(...)</code>), calculating the partition function (i.e., the denominator of \(p(\mathbf{y} \mid \mathbf{x})\)).</p>
  </li>
  <li>
    <p>Calculating the log-nominator of \(p(\mathbf{y} \mid \mathbf{x})\) (<code class="language-plaintext highlighter-rouge">ChainCRF.score_sentence(...)</code>)</p>
  </li>
  <li>
    <p>Decoding to get the target sequence prediction (<code class="language-plaintext highlighter-rouge">ChainCRF.viterbi_decode(...)</code>)</p>
  </li>
</ul>

<p>Below, you’ll find the <code class="language-plaintext highlighter-rouge">ChainCRF</code> class that holds all these methods. The matrix of transition probabilities \(\mathbf{T}\)
is initialized below as <code class="language-plaintext highlighter-rouge">log_transitions</code> (note that \(\mathbf{T}\) are actually the log-transition probabilities because in the CRF equations they are \(\exp(\boldsymbol{\theta}_2f(y_t, y_{t+1})) = \exp\mathbf{T}_{y_t,y_{t+1}}\)), and we hard-code the transition probabilities from any \(y_t\) to the <code class="language-plaintext highlighter-rouge">&lt;ROOT&gt;</code>
tag to be -10000 because this should not be possible (and this becomes 0 in the CRF equation: <code class="language-plaintext highlighter-rouge">exp(log_transitions)</code> gives \(\exp-10000 \approx 0\)).
We do the same for any transition from <code class="language-plaintext highlighter-rouge">&lt;EOS&gt;</code> to any other tag. The class below implements the methods to calculate the NLL loss, and the total forward-pass of the CRF that returns this loss as well as a predicted tag sequence. In the sections below we will implement the necessary methods for our linear-chain CRF, starting with belief propagation.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"> <span class="k">class</span> <span class="nc">ChainCRF</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
      <span class="s">"""
      A linear-chain conditional random field.
      """</span>
      
      <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_tags</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">tag_vocabulary</span><span class="p">:</span> <span class="n">Vocabulary</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ChainCRF</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">tag_vocabulary</span> <span class="o">=</span> <span class="n">tag_vocabulary</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_tags</span> <span class="o">=</span> <span class="n">num_tags</span> <span class="o">+</span> <span class="mi">2</span> <span class="c1"># +2 for &lt;ROOT&gt; and &lt;EOS&gt;
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">root_idx</span> <span class="o">=</span> <span class="n">tag_vocabulary</span><span class="p">.</span><span class="n">size</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">end_idx</span> <span class="o">=</span> <span class="n">tag_vocabulary</span><span class="p">.</span><span class="n">size</span> <span class="o">+</span> <span class="mi">1</span>

        <span class="c1"># Matrix of transition parameters.  Entry (i, j) is the score of
</span>        <span class="c1"># transitioning *from* i *to* j.
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">log_transitions</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_tags</span><span class="p">,</span> 
                                                        <span class="bp">self</span><span class="p">.</span><span class="n">num_tags</span><span class="p">))</span>

        <span class="c1"># Initialize the log transitions with xavier uniform (TODO: refer)
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">xavier_uniform</span><span class="p">()</span>

        <span class="c1"># These two statements enforce the constraint that we never transfer
</span>        <span class="c1"># to the start tag and we never transfer from the stop tag
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">log_transitions</span><span class="p">.</span><span class="n">data</span><span class="p">[:,</span> <span class="bp">self</span><span class="p">.</span><span class="n">root_idx</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">10000.</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">log_transitions</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">end_idx</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">10000.</span>
      
      <span class="k">def</span> <span class="nf">xavier_uniform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="mf">1.</span><span class="p">):</span>
        <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">log_transitions</span><span class="p">)</span>

      <span class="k">def</span> <span class="nf">forward_belief_propagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_features</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> 
                                    <span class="n">input_mask</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nb">NotImplementedError</span><span class="p">()</span>
      
      <span class="k">def</span> <span class="nf">score_sentence</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_features</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span>
                        <span class="n">target_tags</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> 
                        <span class="n">input_mask</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nb">NotImplementedError</span><span class="p">()</span>
          
      <span class="k">def</span> <span class="nf">viterbi_decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_features</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> 
                               <span class="n">input_lengths</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span>
                                                        <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span>
                                                        <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="nb">NotImplementedError</span><span class="p">()</span>
      
      <span class="k">def</span> <span class="nf">negative_log_likelihood</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_features</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> 
                                  <span class="n">target_tags</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> 
                                  <span class="n">input_mask</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="s">"""
        Returns the NLL loss.
        :param input_features: the features for each input sequence
                [batch_size, sequence_length, feature_dimension]
        :param target_tags: the target tags
                [batch_size, sequence_length]
        :param input_mask: the binary mask determining which of 
                the input entries are padding [batch_size, sequence_length]
        """</span>
        <span class="n">partition_function</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">forward_belief_propagation</span><span class="p">(</span>
                                    <span class="n">input_features</span><span class="o">=</span><span class="n">input_features</span><span class="p">,</span> 
                                    <span class="n">input_mask</span><span class="o">=</span><span class="n">input_mask</span><span class="p">)</span>
        <span class="n">log_nominator</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">score_sentence</span><span class="p">(</span>
                                    <span class="n">input_features</span><span class="o">=</span><span class="n">input_features</span><span class="p">,</span>
                                    <span class="n">target_tags</span><span class="o">=</span><span class="n">target_tags</span><span class="p">,</span> 
                                    <span class="n">input_mask</span><span class="o">=</span><span class="n">input_mask</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">partition_function</span> <span class="o">-</span> <span class="n">log_nominator</span>

      <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_features</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> 
                  <span class="n">target_tags</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> 
                  <span class="n">input_mask</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span>
                  <span class="n">input_lengths</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> 
                                                        <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> 
                                                        <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">"""
        The forward-pass of the CRF, which calculates the NLL loss and 
        returns a predicted sequence.
        :param input_features: features for each input sequence
                [batch_size, sequence_length, feature_dimension]
        :param target_tags: the target tags 
                [batch_size, sequence_length]
        :param input_mask: the binary mask determining which of 
                the input entries are padding [batch_size, sequence_length]
        :param input_lengths: the sequence length of each example in the 
                batch of size [batch_size]
        """</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">negative_log_likelihood</span><span class="p">(</span><span class="n">input_features</span><span class="o">=</span><span class="n">input_features</span><span class="p">,</span> 
                                            <span class="n">target_tags</span><span class="o">=</span><span class="n">target_tags</span><span class="p">,</span>
                                            <span class="n">input_mask</span><span class="o">=</span><span class="n">input_mask</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
          <span class="n">score</span><span class="p">,</span> <span class="n">tag_sequence</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">viterbi_decode</span><span class="p">(</span><span class="n">input_features</span><span class="p">,</span>
                                                    <span class="n">input_lengths</span><span class="o">=</span><span class="n">input_lengths</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span> <span class="n">tag_sequence</span>
        </code></pre></figure>

<p>But first, since we implement all these method in batched versions, let’s briefly go over <strong>batching</strong>.</p>

<h1 id="sequence-batching"><span style="color:#C0392B">Sequence Batching</span></h1>

<p>Processing data points in batches has multiple benefits: averaging the gradient over a minibatch in SGD allows playing
with the noise you want while training your model (batch size of 1 gives a maximally noisy gradient, batch size of \(N\) is minimally
noisy gradient, namely gradient descent without the stochasticity), but also: batching examples speeds up training. This motivates
us to implement all the methods for the CRF in batched versions, allowing parallel processing. We can’t parallelize the time-dimension in CRFs unfortunately.</p>

<p>A batch of size 2 would look like this:</p>

<p><img src="/images/batch.png" alt="batch" /></p>

<p>For each batch we will additionally keep track of the lengths in the batch. For the image above a list of lengths would be <code class="language-plaintext highlighter-rouge">input_lengths = [8, 5]</code>.
For example, batched inputs to our encoder will be of size <code class="language-plaintext highlighter-rouge">[batch_size, sequence_length]</code> and outputs of size <code class="language-plaintext highlighter-rouge">[batch_size, sequence_length, hidden_dim*2]</code>.
The input mask for the above batch looks like this:</p>

<p><img src="/images/input_mask.png" alt="input_mask" /></p>

<h1 id="implementation-in-log-space-stable-logsumexp-ing--broadcasting"><span style="color:#C0392B">Implementation in log-space, stable ‘‘logsumexp-ing’’ &amp; broadcasting</span></h1>

<p>Before we can finally get into the interesting implementations, we need to talk about two things. Firstly,
for calculating the partition function we are going to need to sum a bunch of \(exp(\cdot)\)’s, which might explode.
To do this numerically stable, we will use the <em>logsumexp</em>-trick. The log here comes from the fact that we will implement
everything in log-space. Numerical stability doesn’t fare well with
recursive multiplication of small values (i.e., values between 0 and 1) or large values. 
In log-space, multiplications become summations, which have less of a risk of becoming too small or too large.
Recall that the initialization and recursion in the forward-pass of belief propagation are given by the following equations:</p>

\[\begin{aligned}
\alpha(1, y^{\prime}_2) &amp;= \sum_{y^{\prime}_1}\psi(y^{\prime}_1, \mathbf{x}, 1) \cdot  \psi(y^{\prime}_1, y^{\prime}_{2}) \\
\alpha(t, y^{\prime}_{t+1}) &amp;\leftarrow \sum_{y^{\prime}_{t}}\psi(y^{\prime}_t, \mathbf{x}, t) \cdot  \psi(y^{\prime}_t, y^{\prime}_{t+1})\cdot \alpha(t-1, y^{\prime}_t)
\end{aligned}\]

<p>First we convert the alpha initialization to log-space:</p>

\[\begin{aligned}
\alpha(1, y^{\prime}_2) &amp;= \sum_{y^{\prime}_1}\psi(y^{\prime}_1, \mathbf{x}, 1) \cdot  \psi(y^{\prime}_1, y^{\prime}_{2}) \\
\log \alpha(1, y^{\prime}_2) &amp;= \log \sum_{y^{\prime}_1}\psi(y^{\prime}_1, \mathbf{x}, 1) \cdot  \psi(y^{\prime}_1, y^{\prime}_{2}) \\
\end{aligned}\]

<p>Then we convert the recursion equation to log-space, we plug in the CRF factors, and we see why we
get a <em>logsumexp</em>:</p>

\[\begin{aligned}
\log \alpha(t, y^{\prime}_{t+1}) &amp;\leftarrow \log \sum_{y^{\prime}_{t}}\psi(y^{\prime}_t, \mathbf{x}, t) \cdot  \psi(y^{\prime}_t, y^{\prime}_{t+1})\cdot \exp\log\alpha(t-1, y^{\prime}_t) \\
\log \alpha(t, y^{\prime}_{t+1}) &amp;\leftarrow \log \sum_{y^{\prime}_{t}}\exp\left(\boldsymbol{\theta}_1f(y^{\prime}_t, \mathbf{x}, t) + \boldsymbol{\theta}_2f(y^{\prime}_t, y^{\prime}_{t+1})\right) \cdot \exp\log\alpha(t-1, y^{\prime}_t) \\
\log \alpha(t, y^{\prime}_{t+1}) &amp;\leftarrow \underbrace{\log \sum_{y^{\prime}_{t}}\exp}_{\text{logsumexp}}\left(\boldsymbol{\theta}_1f(y^{\prime}_t, \mathbf{x}, t) + \boldsymbol{\theta}_2f(y^{\prime}_t, y^{\prime}_{t+1}) + \log\alpha(t-1, y^{\prime}_t)\right)
\end{aligned}\]

<p>Now what <em>logsumexp</em> does is rewrite this as follows. Let everything inside the \(exp(\cdot)\) above for simplicity be \(q_t\):</p>

\[\begin{aligned}
\log \sum_{y^{\prime}_{t}}\exp(q_t) &amp;= \log \sum_{y^{\prime}_{t}}\exp(q_t - c + c) \\
&amp;= \log \sum_{y^{\prime}_{t}}\exp(q_t - c)\exp(c) \\
&amp;= \log \exp(c)\sum_{y^{\prime}_{t}}\exp(q_t - c) \\
&amp;= c + \log\sum_{y^{\prime}_{t}}\exp(q_t - c) \\
\end{aligned}\]

<p>If we take the constant \(c\) to be the maximum value that \(q_t\) can take in the sum, the summation becomes stable.
See below the code that implements this, adapted from AllenNLP.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">logsumexp</span><span class="p">(</span><span class="n">tensor</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="s">"""
    A numerically stable computation of logsumexp. This is mathematically
    equivalent to `tensor.exp().sum(dim).log()`. 
    This function is typically used for summing log probabilities.
    :param tensor: A tensor of arbitrary size.
    :param dim: The dimension of the tensor to apply the logsumexp to.
    """</span>
    <span class="n">max_score</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
    <span class="n">stable_vec</span> <span class="o">=</span> <span class="n">tensor</span> <span class="o">-</span> <span class="n">max_score</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">max_score</span> <span class="o">+</span> <span class="p">(</span><span class="n">stable_vec</span><span class="p">.</span><span class="n">exp</span><span class="p">().</span><span class="nb">sum</span><span class="p">(</span><span class="n">dim</span><span class="p">)).</span><span class="n">log</span><span class="p">()</span></code></pre></figure>

<p>Secondly, we need to talk about broadcasting. Broadcasting over dimensions is conceptually the same thing as matrix multiplication of a column vector with a row vector,
but then by summing. For example, if we sum <code class="language-plaintext highlighter-rouge">a + b = c</code> where \(a \in \mathbb{R}^{2 \times 1}\) and \(b \in \mathbb{R}^{1 \times 2}\)
the result will be \(c \in \mathbb{R}^{2 \times 2}\):</p>

\[\begin{pmatrix}
1\\ 
2\\ 
\end{pmatrix} + \begin{pmatrix}
3 &amp; 4
\end{pmatrix} = \begin{pmatrix}
4 &amp; 5\\ 
5 &amp; 6
\end{pmatrix}\]

<p>You can use broadcasting if you have a value that you want to add to every index of a vector, like
in the above case 1 is added to 3 and 4 for the top row and 2 to 3 and 4 for the bottom row.</p>

<h1 id="forward-belief-propagation"><span style="color:#C0392B">Forward Belief Propagation</span></h1>

<p>In this section we will implement the forward-pass of belief propagation, which we need to calculate part of the NLL loss (namely \(\log\left(Z(\mathbf{x}^{(i)})\right)\)).</p>

<p>We are implementing the forward recursion in a batched version, meaning that the loop over time goes from
\(t=1\) to \(m-1\) for the largest \(m\) in the batch. Some sequences might already end somewhere earlier in the loop,
which is why we will mask out the recursion for those sequences and retain the old \(\alpha\) variables for them.
Implementing batched forward belief propagation becomes <em>a lot</em> easier if we instead of using the recursion
equation as defined above, use the following recursion:</p>

\[\begin{aligned}
\hat{\alpha}(1, y^{\prime}_2) &amp;= \psi(y^{\prime}_2, \mathbf{x}, 2)\sum_{y^{\prime}_1}\psi(y^{\prime}_1, \mathbf{x}, 1) \cdot  \psi(y^{\prime}_1, y^{\prime}_{2}) \\
\hat{\alpha}(t, y^{\prime}_{t+1}) &amp;\leftarrow \psi(y^{\prime}_{t+1}, \mathbf{x}, t+1)\sum_{y^{\prime}_{t}}\psi(y^{\prime}_t, y^{\prime}_{t+1})\cdot \hat{\alpha}(t-1, y^{\prime}_t)
\end{aligned}\]

<p>Why this is equivalent becomes apparent when we look at the full partition function we’re trying to compute.
We simply move the unary features \(\psi(y_2^{\prime}, \mathbf{x}, 2)\) to the previous recursion.</p>

\[\begin{aligned}
Z(\mathbf{x}) &amp;= \sum_{y^{\prime}_m}\psi(y^{\prime}_m, \mathbf{x}, m)\sum_{y^{\prime}_{m-1}}\psi(y^{\prime}_{m-1}, \mathbf{x}, m-1)\psi(y^{\prime}_{m-1}, y^{\prime}_{m}) \dots \dots \\ 
&amp; \quad \quad \quad \dots \sum_{y^{\prime}_2}\psi(y^{\prime}_2, \mathbf{x}, 2) \cdot  \psi(y^{\prime}_2, y^{\prime}_{3})\underbrace{\sum_{y^{\prime}_1}\psi(y^{\prime}_1, \mathbf{x}, 1) \cdot  \psi(y^{\prime}_1, y^{\prime}_{2})}_{\alpha(1, y^{\prime}_{2})} \\
&amp;= \sum_{y^{\prime}_m}\psi(y^{\prime}_m, \mathbf{x}, m)\sum_{y^{\prime}_{m-1}}\psi(y^{\prime}_{m-1}, \mathbf{x}, m-1)\psi(y^{\prime}_{m-1}, y^{\prime}_{m}) \dots \dots \\ 
&amp; \quad \quad \quad \dots \sum_{y^{\prime}_2} \psi(y^{\prime}_2, y^{\prime}_{3}) \cdot \underbrace{\psi(y^{\prime}_2, \mathbf{x}, 2)\sum_{y^{\prime}_1}\psi(y^{\prime}_1, \mathbf{x}, 1) \cdot  \psi(y^{\prime}_1, y^{\prime}_{2})}_{\hat{\alpha}(1, y^{\prime}_{2})}
\end{aligned}\]

<p>Remember that in the forward recursion of BP we looped until \(m-1\), and then for time \(m\) we don’t have transition probabilities to \(m+1\) anymore, so we separately did the following:</p>

\[Z(\mathbf{x}) = \sum_{y^{\prime}_m}\psi(y^{\prime}_m, \mathbf{x}, m) \cdot\alpha(m-1, y^{\prime}_m)\]

<p>The new recursion results in much easier code because instead of separately keeping track of when each sequence in the batch
ends and writing an if-else statement within the loop over time that calculates the above equations for the sequences that
are ending, we simply already incorporate the unary features of \(y_{t+1}^{\prime}\) in each recursive calculation.
Then, at the end we only need to sum all new alphas, which we can do outside of the loop because the masking
already takes care of keeping the alphas for the ended sequences the same:</p>

\[Z(\mathbf{x}) = \sum_{y^{\prime}_m}\hat{\alpha}(m-1, y^{\prime}_m)\]

<p>The equations we are going to implement now are these recursions in log-space. In the equation below I’ve annotated
every part of the equations with the corresponding variable name their values are a part of in the implementation below.</p>

\[\begin{aligned}
\log\hat{\alpha}(1, y^{\prime}_2) &amp;= \log\psi(y^{\prime}_2, \mathbf{x}, 2) + \log\sum_{y^{\prime}_1}\psi(y^{\prime}_1, \mathbf{x}, 1) \cdot  \psi(y^{\prime}_1, y^{\prime}_{2}) \\
\log\hat{\alpha}(t, y^{\prime}_{t+1}) &amp;\leftarrow \log\left(\psi(y^{\prime}_{t+1}, \mathbf{x}, t+1)\sum_{y^{\prime}_{t}}\psi(y^{\prime}_t, y^{\prime}_{t+1})\cdot \exp\log\hat{\alpha}(t-1, y^{\prime}_t)\right) \\
&amp;\leftarrow \log\left(\sum_{y^{\prime}_{t}}\psi(y^{\prime}_{t+1}, \mathbf{x}, t+1) \cdot \psi(y^{\prime}_t, y^{\prime}_{t+1})\cdot \exp\log\hat{\alpha}(t-1, y^{\prime}_t)\right) \\
&amp;\leftarrow \log\left(\sum_{y^{\prime}_{t}}\exp\left(\underbrace{\boldsymbol{\theta}_1f(y^{\prime}_{t+1}, \mathbf{x}, t+1)}_{\text{unary_features}} + \underbrace{\boldsymbol{\theta}_2 f(y^{\prime}_t, y^{\prime}_{t+1})}_{\text{transition_scores}} + \underbrace{\hat{\alpha}(t-1, y^{\prime}_t)}_{\text{forward_alpha_e}}\right)\right) \\
\end{aligned}\]

<p>We discussed above that \(\boldsymbol{\theta}_1f(y^{\prime}_{t+1}, \mathbf{x}, t+1) = \mathbf{H}_{t+1,y_{t+1}}\) is the t+1-th row and \(y_{t+1}\)-th column of the projected output of our encoder. These values are collected in a vector called <code class="language-plaintext highlighter-rouge">unary_features</code> for all \(y^{\prime}_{t+1}\) (meaning the vector has size \(|S|\)). In the above
 recursion, these values are the same for all \(y_t^{\prime}\) in the sum (meaning we can use broadcasting in the code!). Then \(\boldsymbol{\theta}_2f(y^{\prime}_t, y^{\prime}_{t+1}) = \mathbf{T}_{y_t,y_{t+1}}\)
is the \(y_t\)-th row and \(y_{t+1}\)-th column of the matrix of transition probabilities. The vector <code class="language-plaintext highlighter-rouge">transition_scores</code> in the code holds the probabilities from all \(y_t\)’s to a particular \(y_{t+1}\). These are
the same for each example in the batch, which again asks for broadcasting in the implementation (but I’ll explain that in detail
below the code).</p>

<p>Now let’s take a look at the code that implements all this. Below the code we’ll go through some clarifications.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">forward_belief_propagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_features</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> 
                                     <span class="n">input_mask</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span>
    <span class="s">"""
    Efficient inference with BP of the partition function of the ChainCRF.
    :param input_features: the features for each input sequence
            [batch_size, sequence_length, num_tags] 
    :param input_mask: the binary mask determining which of the input 
            entries are padding [batch_size, sequence_length]
    """</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">num_tags</span> <span class="o">=</span> <span class="n">input_features</span><span class="p">.</span><span class="n">size</span><span class="p">()</span>
    
    <span class="c1"># We don't have input features for the tags &lt;ROOT&gt; and &lt;EOS&gt;, 
</span>    <span class="c1"># so we artifially add those at the tag-dimension. 
</span>    <span class="c1"># See in the class constructor above that the last 
</span>    <span class="c1"># two indices are for the &lt;ROOT&gt; and &lt;EOS&gt; tags.
</span>    <span class="n">input_features</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">(</span>
        <span class="p">[</span><span class="n">input_features</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span> <span class="o">-</span> <span class="mf">10000.</span><span class="p">],</span>
        <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="c1"># Initialize the recursion variables with 
</span>    <span class="c1"># transitions from root token + first unary features.
</span>    <span class="c1"># Note that we don't have unary features for &lt;ROOT&gt; because
</span>    <span class="c1"># the probability that we have &lt;ROOT&gt; at the start is just 1.
</span>    <span class="n">init_alphas</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">log_transitions</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">root_idx</span><span class="p">,</span> <span class="p">:]</span> <span class="o">+</span> <span class="n">input_features</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Set recursion variable.
</span>    <span class="n">forward_alpha</span> <span class="o">=</span> <span class="n">init_alphas</span>

    <span class="c1"># Make time major, we will loop over the time-dimension.
</span>    <span class="n">input_features</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">input_features</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="c1"># input_features: [sequence_length, batch_size, num_tags]
</span>    <span class="n">input_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">input_mask</span><span class="p">.</span><span class="nb">float</span><span class="p">(),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="c1"># input_mask [sequence_length, batch_size]
</span>
    <span class="c1"># Loop over sequence and calculate the recursion alphas.
</span>    <span class="k">for</span> <span class="n">time</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">):</span>

      <span class="c1"># Get unary features for this time step.
</span>      <span class="n">features</span> <span class="o">=</span> <span class="n">input_features</span><span class="p">[</span><span class="n">time</span><span class="p">]</span>

      <span class="c1"># Expand the first dimension so we can broadcast it.
</span>      <span class="c1"># Remember that the unary features are the same for all y_t's in the sum.
</span>      <span class="n">unary_features</span> <span class="o">=</span> <span class="n">features</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_tags</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

      <span class="c1"># Expand the batch dimension so we can broadcast.
</span>      <span class="c1"># The transition scores are the same over the batch dimension.
</span>      <span class="n">transition_scores</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">log_transitions</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

      <span class="c1"># Calculate next tag probabilities.
</span>      <span class="n">forward_alpha_e</span> <span class="o">=</span> <span class="n">forward_alpha</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
      <span class="n">next_forward_alpha</span> <span class="o">=</span> <span class="n">unary_features</span> <span class="o">+</span> <span class="n">transition_scores</span> <span class="o">+</span> <span class="n">forward_alpha_e</span>

      <span class="c1"># Calculate next forward alpha by taking logsumexp over current tag axis, 
</span>      <span class="c1"># mask all instances that ended and keep the old forward alphas
</span>      <span class="c1"># for those instances.
</span>      <span class="n">forward_alpha</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">logsumexp</span><span class="p">(</span><span class="n">next_forward_alpha</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">input_mask</span><span class="p">[</span><span class="n">time</span><span class="p">].</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="o">+</span> <span class="n">forward_alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">input_mask</span><span class="p">[</span><span class="n">time</span><span class="p">]).</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
      <span class="p">)</span>

    <span class="n">final_transitions</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">log_transitions</span><span class="p">[:,</span> <span class="bp">self</span><span class="p">.</span><span class="n">end_idx</span><span class="p">]</span>

    <span class="n">alphas</span> <span class="o">=</span> <span class="n">forward_alpha</span> <span class="o">+</span> <span class="n">final_transitions</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">partition_function</span> <span class="o">=</span> <span class="n">logsumexp</span><span class="p">(</span><span class="n">alphas</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">partition_function</span></code></pre></figure>

<p>The code recurses over the time dimension, from \(1\) to \(m\), and calculates the alphas in log-space.
The first important line is the following:</p>

<p><code class="language-plaintext highlighter-rouge">next_forward_alpha = unary_features + transition_scores + forward_alpha_e</code></p>

<p>This line basically calculates the recursion above without the <em>logsumexp</em>-ing:</p>

\[\underbrace{\boldsymbol{\theta}_1f(y^{\prime}_{t+1}, \mathbf{x}, t+1)}_{\text{unary_features}} + \underbrace{\boldsymbol{\theta}_2 f(y^{\prime}_t, y^{\prime}_{t+1})}_{\text{transition_scores}} + \underbrace{\hat{\alpha}(t-1, y^{\prime}_t)}_{\text{forward_alpha_e}}\]

<p>But then for all possible POS tags \(y^{\prime}_t\) and all next tags \(y^{\prime}_{t+1}\). This is where we use broadcasting. The unary features
 are the same for all \(y^{\prime}_t\), and the forward alphas are the same for all \(y^{\prime}_{t+1}\). Then if we logsumexp over the first dimension:</p>

<p><code class="language-plaintext highlighter-rouge">logsumexp(next_forward_alpha, 1)</code></p>

<p>We get our subsequent alpha recursion variables for all \(y^{\prime}_{t+1}\). The following image shows what happens
in these two lines of code for a single example in a batch graphically:</p>

<p><img src="/images/broadcast_forward.png" alt="broadcast_forward" width="800" class="center" /></p>

<p>The image above has \(\tilde{\alpha}\) which should be \(\hat{\alpha}\) actually but I can’t for the life of me
figure out how to do that in Google drawings <code class="language-plaintext highlighter-rouge">&gt;:(</code>.</p>

<p>Note that we add the transition from each tag to the final tag outside of the loop over time, because these are independent
of time and should happen for every sequence in the batch. Then the whole function returns:</p>

<p><code class="language-plaintext highlighter-rouge">logsumexp(alphas)</code> which is \(Z(\mathbf{x}) = \sum_{y^{\prime}_m}\hat{\alpha}(m-1, y^{\prime}_m)\).</p>

<p>This is possible for all examples in the batch because we make sure within the loop over time to only update the alphas
for the sequences that haven’t ended yet. Whenever the value for a particular time-step of the <code class="language-plaintext highlighter-rouge">input_mask</code> is zero,
the following lines of code make sure that we retain the old alpha for those examples:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">forward_alpha</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">logsumexp</span><span class="p">(</span><span class="n">next_forward_alpha</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">input_mask</span><span class="p">[</span><span class="n">time</span><span class="p">].</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="o">+</span> <span class="n">forward_alpha</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">input_mask</span><span class="p">[</span><span class="n">time</span><span class="p">]).</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="p">)</span></code></pre></figure>

<p>Yay, that’s calculating the partition function! Now let’s look at calculating the nominator of the CRF,
or the function <code class="language-plaintext highlighter-rouge">ChainCRF.score_sentence(...)</code>.</p>

<h1 id="calculating-the-nominator"><span style="color:#C0392B">Calculating the Nominator</span></h1>

<p>To finish calculating the loss, we just need to calculate the (log-)nominator!</p>

\[\sum_{t=1}^m \underbrace{\boldsymbol{\theta}_1 f(y_t^{(i)}, \mathbf{x}^{(i)}, t)}_{\text{unary_features}} + \sum_{t=1}^{m-1} \underbrace{\boldsymbol{\theta}_2 f(y_t^{(i)}, y_{t+1}^{(i)})}_{\text{transition_scores}}\]

<p>Which is simply a sum over time of the unary features and the transition probabilities for a given input sentence \(\mathbf{x}^{(i)}\)
and target tag sequence \(\mathbf{y}^{(i)}\). In the implementation we loop over all the sequences in the batch in parallel,
get the current tag for each example in the batch from the ground-truth sequence <code class="language-plaintext highlighter-rouge">target_tags</code>,
and calculate the current unary scores and transition scores from the current time to the next.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">score_sentence</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_features</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span>
                   <span class="n">target_tags</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> 
                   <span class="n">input_mask</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="s">"""
        Calculates the log-nominator of the CRF for given batch of 
        input features and sequences of target tags.
        :param input_features: the features for each input sequence
            [batch_size, sequence_length, num_tags] 
        :param target_tags: tensor with the ground-truth target tags
            of size [batch_size, sequence_length]
        :param input_mask: the binary mask determining which of the input 
            entries are padding [batch_size, sequence_length]

        Returns: the log-nominator of the CRF [batch_size]
        """</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">num_tags</span> <span class="o">=</span> <span class="n">input_features</span><span class="p">.</span><span class="n">size</span><span class="p">()</span>

        <span class="c1"># Make time major, for the loop over time.
</span>        <span class="n">input_features</span> <span class="o">=</span> <span class="n">input_features</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  
        <span class="c1"># input_features: [sequence_length, batch_size, num_tags]
</span>        <span class="n">input_mask</span> <span class="o">=</span> <span class="n">input_mask</span><span class="p">.</span><span class="nb">float</span><span class="p">().</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  
        <span class="c1"># input_mask: [sequence_length, batch_size]
</span>        <span class="n">target_tags</span> <span class="o">=</span> <span class="n">target_tags</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="c1"># target_tags: [sequence_length, batch_size]
</span>
        <span class="c1"># Get tensor of root tokens and tensor of next tags (first tags).
</span>        <span class="n">root_tags</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="bp">self</span><span class="p">.</span><span class="n">root_idx</span><span class="p">]</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">():</span>
          <span class="n">root_tags</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">next_tags</span> <span class="o">=</span> <span class="n">target_tags</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">squeeze</span><span class="p">()</span>

        <span class="c1"># Initial transition is from root token to first tags.
</span>        <span class="n">initial_transition</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">log_transitions</span><span class="p">[</span><span class="n">root_tags</span><span class="p">,</span> <span class="n">next_tags</span><span class="p">]</span>

        <span class="c1"># Initialize scores.
</span>        <span class="n">scores</span> <span class="o">=</span> <span class="n">initial_transition</span>
        <span class="c1"># scores: [batch_size]
</span>
        <span class="c1"># Loop over time and at each time calculate the score from t to t + 1.
</span>        <span class="k">for</span> <span class="n">time</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">sequence_length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>

            <span class="c1"># Calculate the score for the current time step.
</span>            <span class="n">unary_features</span> <span class="o">=</span> <span class="n">input_features</span><span class="p">[</span><span class="n">time</span><span class="p">]</span>
            <span class="c1"># unary_features: [batch_size, num_tags]
</span>            <span class="n">next_tags</span> <span class="o">=</span> <span class="n">target_tags</span><span class="p">[</span><span class="n">time</span> <span class="o">+</span> <span class="mi">1</span><span class="p">].</span><span class="n">squeeze</span><span class="p">()</span>
            <span class="n">current_tags</span> <span class="o">=</span> <span class="n">target_tags</span><span class="p">[</span><span class="n">time</span><span class="p">].</span><span class="n">squeeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">unary_features</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">gather</span><span class="p">(</span><span class="n">unary_features</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> 
                                          <span class="n">current_tags</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)).</span><span class="n">squeeze</span><span class="p">()</span>
            <span class="c1"># unary_features: [batch_size]
</span>            <span class="n">transition_scores</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">log_transitions</span><span class="p">[</span><span class="n">current_tags</span><span class="p">,</span> <span class="n">next_tags</span><span class="p">]</span>
            <span class="c1"># transition_scores: [batch_size]
</span>
            <span class="c1"># Add scores.
</span>            <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">+</span> <span class="n">transition_scores</span> <span class="o">*</span> <span class="n">input_mask</span><span class="p">[</span><span class="n">time</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> 
                        <span class="o">+</span> <span class="n">unary_features</span> <span class="o">*</span> <span class="n">input_mask</span><span class="p">[</span><span class="n">time</span><span class="p">]</span>
            <span class="c1"># scores: [batch_size]
</span>
        <span class="c1"># Gather the last tag for each example in the batch.
</span>        <span class="n">last_tag_index</span> <span class="o">=</span> <span class="n">input_mask</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nb">long</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="n">last_tags</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">gather</span><span class="p">(</span><span class="n">target_tags</span><span class="p">,</span> 
                                 <span class="mi">0</span><span class="p">,</span> 
                                 <span class="n">last_tag_index</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)).</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1"># Get the transition scores from the last tag to the &lt;EOS&gt; tag.
</span>        <span class="n">end_tags</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="bp">self</span><span class="p">.</span><span class="n">end_idx</span><span class="p">]</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">():</span>
            <span class="n">end_tags</span> <span class="o">=</span> <span class="n">end_tags</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">last_transition</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">log_transitions</span><span class="p">[</span><span class="n">last_tags</span><span class="p">,</span> <span class="n">end_tags</span><span class="p">]</span>

        <span class="c1"># Add the last input if its not masked.
</span>        <span class="n">last_inputs</span> <span class="o">=</span> <span class="n">input_features</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">last_input_score</span> <span class="o">=</span> <span class="n">last_inputs</span><span class="p">.</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">last_tags</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">last_input_score</span> <span class="o">=</span> <span class="n">last_input_score</span><span class="p">.</span><span class="n">squeeze</span><span class="p">()</span>

        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">+</span> <span class="n">last_transition</span> <span class="o">+</span> <span class="n">last_input_score</span> <span class="o">*</span> <span class="n">input_mask</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">scores</span></code></pre></figure>

<p>In the code above we calculate the unary features for the tags at the current time step with the following line:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">unary_features</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">gather</span><span class="p">(</span><span class="n">unary_features</span><span class="p">,</span> 
                              <span class="mi">1</span><span class="p">,</span>
                              <span class="n">current_tags</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)).</span><span class="n">squeeze</span><span class="p">()</span></code></pre></figure>

<p>Here, <code class="language-plaintext highlighter-rouge">current_tags</code> is a vector with the index of the tag at time \(t\) in the sequence for each example in the batch,
and the gather function retrieves this index in the first dimension of the vector <code class="language-plaintext highlighter-rouge">unary_features</code>, which is of size
<code class="language-plaintext highlighter-rouge">[batch_size, num_tags]</code>, meaning this code gathers \(\boldsymbol{\theta}_1 f(y_t^{(i)}, \mathbf{x}^{(i)}, t)\)
for all examples in the batch. Then we add these unary features to the transition features for each current tag to
the next tags at time \(t+1\) (\(\boldsymbol{\theta}_2 f(y_t^{(i)}, y_{t+1}^{(i)})\)), making sure to mask any
unary features for sequences that have ended, and transition scores for sequences that don’t have a tag anymore at time
\(t+1\):</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">+</span> <span class="n">transition_scores</span> <span class="o">*</span> <span class="n">input_mask</span><span class="p">[</span><span class="n">time</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> 
                        <span class="o">+</span> <span class="n">unary_features</span> <span class="o">*</span> <span class="n">input_mask</span><span class="p">[</span><span class="n">time</span><span class="p">]</span></code></pre></figure>

<p>Then outside of the loop over time we still need to gather the transition scores for the last tag of each sequence in the batch
to the <code class="language-plaintext highlighter-rouge">&lt;EOS&gt;</code> token, because we didn’t do this inside the loop. Additionally, we need to add the last unary features for
the longest sequences in the batch. Finally, we return a log-nominator <code class="language-plaintext highlighter-rouge">score</code> for each sequence in the batch.
This concludes the calculation of the NLL! So we can start decoding.</p>

<h1 id="viterbi-decoding"><span style="color:#C0392B">Viterbi Decoding</span></h1>

<p>We’ll finally implement decoding with Viterbi. Recall that in structured prediction we ideally want to use an efficient DP algorithm,
to get rid of the exponential decoding complexity. As mentioned, decoding here is finding the maximum scoring target sequence according to our model. 
This amounts to solving the following equation:</p>

<p>\(\mathbf{y}^{\star} = arg\max_{\mathbf{y} \in \mathcal{Y}} p(\mathbf{y} \mid \mathbf{x})\).</p>

<p>With Viterbi, the time complexity of doing this is \(m \cdot |S|^2\), instead of \(|S|^m\) if done naively. Viterbi works
very similarly to BP, but instead of summing we maximize. The Viterbi implementation is by far the most complex one
of all the CRF methods, especially in the batched version that we’ll use. It’s also the longest function as
we have to implement both the forward recursion to calculate all the likelihoods of the possible sequences,
as well as backtracking by following backpointers to obtain the maximum scoring sequence, so we’ll do it in two steps.
That said, if you went through forward belief propagation and understood that part, this is not much different.</p>

<p>The recursion equations for Viterbi decoding are four equations, one for initializing the recursion variables,
one for the recursion itself, one for intializing the backpointers and one for recursively finding the backpointer.
If you want a detailed explanation of how we get to these equations from the \(arg\max\) above, see <a href="/2021/01/25/crfpt1.html">part one</a> of this
series</p>

\[\begin{aligned}
v(1, y^{\prime}_2) &amp;= \max_{y^{\prime}_1}\psi(y^{\prime}_1, \mathbf{x}, 1) \cdot  \psi(y^{\prime}_1, y^{\prime}_{2}) \\
v(t, y^{\prime}_{t+1}) &amp;\leftarrow \max_{y^{\prime}_{t}}\psi(y^{\prime}_t, \mathbf{x}, t) \cdot  \psi(y^{\prime}_t, y^{\prime}_{t+1})\cdot v(t-1, y^{\prime}_t) \\
\overleftarrow{v}(1, y^{\prime}_2) &amp;= arg\max_{y^{\prime}_1}\psi(y^{\prime}_1, \mathbf{x}, 1) \cdot  \psi(y^{\prime}_1, y^{\prime}_{2}) \\ 
\overleftarrow{v}(t, y^{\prime}_{t+1}) &amp;\leftarrow arg\max_{y^{\prime}_{t}}\psi(y^{\prime}_t, \mathbf{x}, t) \cdot  \psi(y^{\prime}_t, y^{\prime}_{t+1})\cdot v(t-1, y^{\prime}_t) \\
\end{aligned}\]

<p>The animation that graphically depict what goes on in Viterbi is the following:</p>

<p><img src="/images/viterbigif/viterbi.gif" alt="viterbi" /></p>

<p>We loop over the sequence, calculating the recursion for each example in the batch and for each tag \(y_t\),
and at the same time we keep track for each tag which previous tag is the argument that maximizes the sequence up till 
that tag. This is what the first loop over time in the code below does. Then, we loop backwards over time and follow
the backpointers to find the maximum scoring sequence. The code below implements all this,
and below it we’ll go over some more detailed explanations of what’s going, relating the
code to the equations. I also write much more detailed comments than usually in the code below for
clarification.</p>

<p>Now even though in this code we do need to have an <code class="language-plaintext highlighter-rouge">if-else</code>-statement within the loop over time to
keep track of the best last tags (the final column in the above animation). We will still rewrite the recursion
equations like we did above in forward BP to get rid of the need to separately calculate the final recursive calculation
for which we don’t have transition probabilities.
I’m just going to redefine the \(\overleftarrow{v}\) below to avoid notational clutter. Let’s also write everything in logspace while we’re at it,
but now note that since \(exp(\cdot)\) and \(\log(\cdot)\) are monotonically increasing functions and we only care about
the maximizing scores and tags, we can just leave them out of the implementation all together. And finally,
note in the below that we assume \(\psi(y^{\prime}_1, \mathbf{x}, 1)\) of the first tag to be 1
 because it’s just always the <code class="language-plaintext highlighter-rouge">&lt;ROOT&gt;</code> and \(\psi(y^{\prime}_1, y^{\prime}_{2})\) to be the transition probabilities from the root tag to
any other tag. This means the initial \(y^{\prime}_1\) that maximizes the equations is simply <code class="language-plaintext highlighter-rouge">&lt;ROOT&gt;</code>.</p>

\[\begin{aligned}
\hat{v}(1, y^{\prime}_2) &amp;= \log\psi(y^{\prime}_2, \mathbf{x}, 2) + \log\max_{y^{\prime}_1}\psi(y^{\prime}_1, \mathbf{x}, 1) \cdot  \psi(y^{\prime}_1, y^{\prime}_{2}) \\
&amp;\propto \boldsymbol{\theta}_1f(y^{\prime}_{2}, \mathbf{x}, 2) + \underbrace{\max_{y^{\prime}_1}\boldsymbol{\theta}_2 f(y^{\prime}_1, y^{\prime}_{2})}_{\text{init_vit}} \\
\hat{v}(t, y^{\prime}_{t+1}) &amp;\leftarrow \boldsymbol{\theta}_1f(y^{\prime}_{t+1}, \mathbf{x}, t+1) + \log\left(\max_{y^{\prime}_{t}}\exp\left(\boldsymbol{\theta}_2 f(y^{\prime}_t, y^{\prime}_{t+1}) + \hat{v}(t-1, y^{\prime}_t)\right)\right) \\
&amp;\propto \underbrace{\boldsymbol{\theta}_1f(y^{\prime}_{t+1}, \mathbf{x}, t+1)}_{\text{unary_features}} + \max_{y^{\prime}_{t}}\left(\underbrace{\boldsymbol{\theta}_2 f(y^{\prime}_t, y^{\prime}_{t+1})}_{\text{transition_scores}} + \underbrace{\hat{v}(t-1, y^{\prime}_t)}_{\text{forward_vit}}\right) \\
\overleftarrow{v}(t, y^{\prime}_{t+1}) &amp;\leftarrow arg\max_{y^{\prime}_{t}}\underbrace{\boldsymbol{\theta}_2 f(y^{\prime}_t, y^{\prime}_{t+1}) + \hat{v}(t-1, y^{\prime}_t)}_{\text{next_tag_vit}} \\
\end{aligned}\]

<p>I annotated the equations with the variables used in the code again, even though in the equations its all for a single
example and in the code for a batch of examples.
We’re going to do this function in two parts (even though they’re actually a single function), where the first part is
the forward recursion given by the above equations and the second part will be backtracking.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">viterbi_decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_features</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> 
                   <span class="n">input_lengths</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="s">"""
        Find the maximum scoring tag sequence for each example in the batch.
        
        :param input_features: the features for each input sequence
            [batch_size, sequence_length, num_tags] 
        :param input_lengths: lengths of each example in the batch [batch_size]
        
        Returns: tuple of scores and tag sequences per example in the batch.
        """</span>           
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">num_tags</span> <span class="o">=</span> <span class="n">input_features</span><span class="p">.</span><span class="n">size</span><span class="p">()</span>
        
        <span class="c1"># We don't have input features for the tags &lt;ROOT&gt; and &lt;EOS&gt;, 
</span>        <span class="c1"># so we artifially add those at the tag-dimension. 
</span>        <span class="c1"># See in the class constructor above that the last 
</span>        <span class="c1"># two indices are for the &lt;ROOT&gt; and &lt;EOS&gt; tags.
</span>        <span class="n">input_features</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">input_features</span><span class="p">,</span> 
                                   <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">batch_size</span><span class="p">,</span>
                                                <span class="n">sequence_length</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> 
                                                <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span> <span class="o">-</span> <span class="mf">10000.</span><span class="p">],</span>
                                   <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Initialize the viterbi variables in log space
</span>        <span class="c1"># and set the score of the root tag the highest.
</span>        <span class="n">init_vit</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">full</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_tags</span><span class="p">),</span> <span class="o">-</span><span class="mf">10000.</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">init_vit</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="bp">self</span><span class="p">.</span><span class="n">root_idx</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># Initialize tensor to keep track of backpointers.
</span>        <span class="c1"># This tensor will hold the red arrow backpointers
</span>        <span class="c1"># like in the animation above the code.
</span>        <span class="n">backpointers</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_tags</span><span class="p">,</span>
                                   <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">).</span><span class="nb">long</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span>
        <span class="c1"># These lists will hold the best last tags and path scores for
</span>        <span class="c1"># each example in the batch. I.e., when t equals their sequence length.
</span>        <span class="n">best_last_tags</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">best_path_scores</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1"># forward_vit at step t holds the viterbi variables for step t - 1
</span>        <span class="c1"># will be different for each example in batch, but start the same.
</span>        <span class="n">forward_vit</span> <span class="o">=</span> <span class="n">init_vit</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">repeat</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># A counter counting down from number of examples in batch to 0.
</span>        <span class="n">num_examples_left</span> <span class="o">=</span> <span class="n">batch_size</span>

        <span class="c1"># Loop over the sequence for the forward recursion.
</span>        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">):</span>

            <span class="c1"># Find the sequences that are ending at this t.
</span>            <span class="n">ending</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">input_lengths</span> <span class="o">==</span> <span class="n">t</span><span class="p">)</span>
            <span class="n">n_ending</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ending</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">n_ending</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>

                <span class="c1"># Get the viterbi vars of the ending sequences.
</span>                <span class="c1"># Important here is that the sequences are ordered descending in 
</span>                <span class="c1"># length. Meaning the last sequences are ending the first.
</span>                <span class="n">forward_ending</span> <span class="o">=</span> <span class="n">forward_vit</span><span class="p">[</span>
                    <span class="p">(</span><span class="n">num_examples_left</span> <span class="o">-</span> <span class="n">n_ending</span><span class="p">):</span><span class="n">num_examples_left</span><span class="p">]</span>

                <span class="c1"># The terminal var giving the best last tag is 
</span>                <span class="c1"># the viterbi variables + trans. prob. to end token.
</span>                <span class="n">trans_to_end</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">log_transitions</span><span class="p">[:,</span> <span class="bp">self</span><span class="p">.</span><span class="n">end_idx</span><span class="p">]</span>
                <span class="n">terminal_var</span> <span class="o">=</span> <span class="n">forward_ending</span> <span class="o">+</span> <span class="n">trans_to_end</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
                
                <span class="c1"># Get the final best score and tag for these sequences.    
</span>                <span class="n">path_scores</span><span class="p">,</span> <span class="n">best_tag_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">terminal_var</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

                <span class="c1"># First sequence to end is last sequence in batch, so if 
</span>                <span class="c1"># we save them like this and reverse the lists
</span>                <span class="c1"># later on we get the right ordering back.
</span>                <span class="k">for</span> <span class="n">tag</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">best_tag_idx</span><span class="p">)),</span> 
                                      <span class="nb">reversed</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">path_scores</span><span class="p">))):</span>
                    <span class="n">best_last_tags</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">tag</span><span class="p">)</span>
                    <span class="n">best_path_scores</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>

                <span class="c1"># Update counter that tracks how many sequences haven't ended.
</span>                <span class="n">num_examples_left</span> <span class="o">-=</span> <span class="n">n_ending</span>

            <span class="c1"># Calculate scores of next tag
</span>            <span class="n">forward_vit</span> <span class="o">=</span> <span class="n">forward_vit</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_tags</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">transition_scores</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">log_transitions</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">next_tag_vit</span> <span class="o">=</span> <span class="n">forward_vit</span> <span class="o">+</span> <span class="n">transition_scores</span>

            <span class="c1"># Get the best next tags and viterbi vars.
</span>            <span class="n">viterbivars_t</span><span class="p">,</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">next_tag_vit</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">best_tag_ids</span> <span class="o">=</span> <span class="n">idx</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            
            <span class="c1"># Get unary scores at current time step.
</span>            <span class="n">unary_features</span> <span class="o">=</span> <span class="n">input_features</span><span class="p">[:,</span> <span class="n">t</span><span class="p">,</span> <span class="p">:].</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> 
                                                          <span class="bp">self</span><span class="p">.</span><span class="n">num_tags</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                                                          
            <span class="c1"># Add the unary features and assign forward_vit to the set
</span>            <span class="c1"># of viterbi variables we just computed.
</span>            <span class="n">forward_vit</span> <span class="o">=</span> <span class="p">(</span><span class="n">viterbivars_t</span> <span class="o">+</span> <span class="n">unary_features</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)).</span><span class="n">view</span><span class="p">(</span>
                                        <span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

            <span class="c1"># Save the best tags as backpointers.
</span>            <span class="n">backpointers</span><span class="p">[:,</span> <span class="n">t</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">best_tag_ids</span><span class="p">.</span><span class="nb">long</span><span class="p">()</span>

        <span class="c1"># Get final ending sequences and calculate the best last tags
</span>        <span class="n">ending</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">input_lengths</span> <span class="o">==</span> <span class="n">sequence_length</span><span class="p">)</span>
        <span class="n">ending</span> <span class="o">=</span> <span class="n">ending</span><span class="p">.</span><span class="n">cuda</span><span class="p">()</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="n">ending</span>
        <span class="n">n_ending</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">ending</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">n_ending</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>

            <span class="n">forward_ending</span> <span class="o">=</span> <span class="n">forward_vit</span><span class="p">[</span>
                        <span class="p">(</span><span class="n">num_examples_left</span> <span class="o">-</span> <span class="n">n_ending</span><span class="p">):</span><span class="n">num_examples_left</span><span class="p">]</span>

            <span class="c1"># transition to STOP_TAG
</span>            <span class="n">last_transitions</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">log_transitions</span><span class="p">[:,</span> <span class="bp">self</span><span class="p">.</span><span class="n">end_idx</span><span class="p">].</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="n">terminal_var</span> <span class="o">=</span> <span class="n">forward_ending</span> <span class="o">+</span> <span class="n">last_transitions</span>
            <span class="n">path_scores</span><span class="p">,</span> <span class="n">best_tag_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">terminal_var</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

            <span class="k">for</span> <span class="n">tag</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">best_tag_idx</span><span class="p">)),</span>
                                  <span class="nb">reversed</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">path_scores</span><span class="p">))):</span>
                <span class="n">best_last_tags</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">tag</span><span class="p">)</span>
                <span class="n">best_path_scores</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
        
        <span class="c1"># Backtracking (see code below). 
</span>        <span class="p">...</span></code></pre></figure>

<p>Everything in the code above hopefully becomes clear with the equations with annotations given above the code. 
Below I took out a piece of code that’s inside if the <code class="language-plaintext highlighter-rouge">if</code>-statements in the code above. What happens is that we calculate:</p>

<p>\(\hat{y}^{\prime}_m = arg\max_{y^{\prime}_m}\underbrace{\boldsymbol{\theta}_2 f(y^{\prime}_m, y^{\prime}_{m+1})}_{\text{trans_to_end}} + \underbrace{\hat{v}_{m-1}(y^{\prime}_m)}_{\text{forward_ending}}\),</p>

<p>for every example that is ending at that time \(t\), i.e., \(t = m\), and for \(y^{\prime}_{m+1}\) the <code class="language-plaintext highlighter-rouge">&lt;EOS&gt;</code>-tag.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">if</span> <span class="n">n_ending</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>

        <span class="c1"># Get the viterbi vars of the ending sequences.
</span>        <span class="c1"># Important here is that the sequences are ordered descending in 
</span>        <span class="c1"># length. Meaning the last sequences are ending the first.
</span>        <span class="n">forward_ending</span> <span class="o">=</span> <span class="n">forward_vit</span><span class="p">[</span>
            <span class="p">(</span><span class="n">num_examples_left</span> <span class="o">-</span> <span class="n">n_ending</span><span class="p">):</span><span class="n">num_examples_left</span><span class="p">]</span>

        <span class="c1"># The terminal var giving the best last tag is 
</span>        <span class="c1"># the viterbi variables + trans. prob. to end token.
</span>        <span class="n">trans_to_end</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">log_transitions</span><span class="p">[:,</span> <span class="bp">self</span><span class="p">.</span><span class="n">end_idx</span><span class="p">]</span>
        <span class="n">terminal_var</span> <span class="o">=</span> <span class="n">forward_ending</span> <span class="o">+</span> <span class="n">trans_to_end</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        
        <span class="c1"># Get the final best score and tag for these seequences.    
</span>        <span class="n">path_scores</span><span class="p">,</span> <span class="n">best_tag_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">terminal_var</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="c1"># First sequence to end is last sequence in batch, so if 
</span>        <span class="c1"># we save them like this and reverse the lists
</span>        <span class="c1"># later on we get the right ordering back.
</span>        <span class="k">for</span> <span class="n">tag</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">best_tag_idx</span><span class="p">)),</span> 
                              <span class="nb">reversed</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">path_scores</span><span class="p">))):</span>
            <span class="n">best_last_tags</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">tag</span><span class="p">)</span>
            <span class="n">best_path_scores</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>

        <span class="c1"># Update counter that tracks how many sequences haven't ended.
</span>        <span class="n">num_examples_left</span> <span class="o">-=</span> <span class="n">n_ending</span></code></pre></figure>

<p>We do the same calculation outside of the loop over time for all the sequences that have the maximum sequence length
in the batch. Then int the end we have all the backpointers and scores initialized, and we can start the backtracking.
In the lists <code class="language-plaintext highlighter-rouge">best_last_tags</code> we appended the final tags that maximized the ending sequences, starting with the shortest
sequences in the batch. When we make sure to sort all the sequences in the batch in descending order,
we can simply reverse <code class="language-plaintext highlighter-rouge">best_last_tags</code> to get the original ordering back. In the code below, we then put these tags in the initialized
matrix <code class="language-plaintext highlighter-rouge">best_paths</code> that should hold the maximizing sequence for each example. This means <code class="language-plaintext highlighter-rouge">best_paths</code> looks like the image below before we loop backwards over time:</p>

<p><img src="/images/best_paths.png" alt="bestpaths" /></p>

<p>(depending on how long each sequence in the batch is the actual batch might look differently).
Then we loop backwards over time, following the backpointers, and fill the <code class="language-plaintext highlighter-rouge">best_paths</code> for each time in the sequence.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">viterbi_decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_features</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> 
                   <span class="n">input_lengths</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="s">"""
        Find the maximum scoring tag sequence for each example in the batch.
        
        :param input_features: the features for each input sequence
            [batch_size, sequence_length, num_tags] 
        :param target_tags:
        :param input_lengths: lengths of each example in the batch [batch_size]
        
        Returns: tuple of scores and tag sequences per example in the batch.
        """</span>           
        
        <span class="c1"># Forward Recursion (see code above).
</span>        <span class="p">...</span>
        
        <span class="c1"># Reverse the best last tags (and scores) to get the original order.
</span>        <span class="n">best_last_tags</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">best_last_tags</span><span class="p">)))</span>
        <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">()</span>
            <span class="n">best_last_tags</span> <span class="o">=</span> <span class="n">best_last_tags</span><span class="p">.</span><span class="n">cuda</span><span class="p">()</span>
        <span class="n">best_path_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">LongTensor</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="n">best_path_scores</span><span class="p">)))</span>
        <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">():</span>
            <span class="n">best_path_scores</span> <span class="o">=</span> <span class="n">best_path_scores</span><span class="p">.</span><span class="n">cuda</span><span class="p">()</span>

        <span class="c1"># Initialize the best paths for each sequence in the batch by
</span>        <span class="c1"># putting at the correct length for each example
</span>        <span class="c1"># the best last tag found in the above recursion.
</span>        <span class="c1"># This is depicted in the image above.
</span>        <span class="n">best_paths</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span> <span class="o">+</span> <span class="mi">1</span><span class="p">).</span><span class="nb">long</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">():</span>
            <span class="n">best_paths</span> <span class="o">=</span> <span class="n">best_paths</span><span class="p">.</span><span class="n">cuda</span><span class="p">()</span>
        <span class="n">best_paths</span> <span class="o">=</span> <span class="n">best_paths</span><span class="p">.</span><span class="n">index_put_</span><span class="p">(</span>
            <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">backpointers</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))]),</span>
             <span class="n">input_lengths</span><span class="p">),</span>
            <span class="n">best_last_tags</span><span class="p">)</span>

        <span class="c1"># A counter keeping track of number of active sequences.
</span>        <span class="c1"># This increases from 0 until batch_size at the last time step
</span>        <span class="c1"># when even the shortest sequence is active.
</span>        <span class="n">num_active</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="c1"># Loop backwards over time (max. time to 0).
</span>        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">sequence_length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>

            <span class="c1"># If time step equals lengths of some sequences, they are starting.
</span>            <span class="c1"># (starting meaning this time step is their last tag).
</span>            <span class="n">starting</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nonzero</span><span class="p">(</span><span class="n">input_lengths</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">==</span> <span class="n">t</span><span class="p">)</span>
            <span class="n">n_starting</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">starting</span><span class="p">)</span>

            <span class="c1"># If there are sequences starting, grab their best last tags.
</span>            <span class="k">if</span> <span class="n">n_starting</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="c1"># For the longest sequences, initialize best_tag_id.
</span>                <span class="k">if</span> <span class="n">t</span> <span class="o">==</span> <span class="n">sequence_length</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="n">best_tag_id</span> <span class="o">=</span> <span class="n">best_paths</span><span class="p">[</span><span class="n">num_active</span><span class="p">:</span><span class="n">num_active</span> <span class="o">+</span> <span class="n">n_starting</span><span class="p">,</span>
                                             <span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">last_tags</span> <span class="o">=</span> <span class="n">best_paths</span><span class="p">[</span><span class="n">num_active</span><span class="p">:</span><span class="n">num_active</span> <span class="o">+</span> <span class="n">n_starting</span><span class="p">,</span>
                                           <span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
                    <span class="n">best_tag_id</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">((</span><span class="n">best_tag_id</span><span class="p">,</span> 
                                             <span class="n">last_tags</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

                <span class="c1"># Update the number of active sequences.
</span>                <span class="n">num_active</span> <span class="o">+=</span> <span class="n">n_starting</span>

            <span class="c1"># Get relevant backpointers based on sequences that are active.
</span>            <span class="n">active</span> <span class="o">=</span> <span class="n">backpointers</span><span class="p">[:</span><span class="n">num_active</span><span class="p">,</span> <span class="n">t</span><span class="p">]</span>

            <span class="c1"># Follow the backpointers to the best previous tag.
</span>            <span class="n">best_tag_id</span> <span class="o">=</span> <span class="n">best_tag_id</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">num_active</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">best_tag_id</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">gather</span><span class="p">(</span><span class="n">active</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">best_tag_id</span><span class="p">)</span>
            <span class="n">best_paths</span><span class="p">[:</span><span class="n">num_active</span><span class="p">,</span> <span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">best_tag_id</span><span class="p">.</span><span class="n">squeeze</span><span class="p">()</span>

        <span class="c1"># Sanity check that first tag is the &lt;ROOT&gt; token.
</span>        <span class="k">assert</span> <span class="p">(</span><span class="n">best_paths</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">].</span><span class="nb">sum</span><span class="p">().</span><span class="n">item</span><span class="p">()</span> \
                    <span class="o">==</span> <span class="n">best_paths</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">root_idx</span><span class="p">)</span>
        
        <span class="c1"># Return the scores and the paths without the &lt;ROOT&gt; token.
</span>        <span class="k">return</span> <span class="n">best_path_scores</span><span class="p">,</span> <span class="n">best_paths</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span></code></pre></figure>

<p>In the above code, at every time step (for the active sequences) we use the previous best tags, starting with
the best last tags, to find the correct backpointer to follow back. Now <code class="language-plaintext highlighter-rouge">backpointers</code> is a matrix of size <code class="language-plaintext highlighter-rouge">[batch_size, sequence_length, num_tags]</code> (where
the batch is still sorted in descending order) and holds at every time \(t\) the backpointers (i.e., ids of previous tags
that maximize the sequence) for every tag. See this for one example in a batch depicted below:</p>

<p><img src="/images/backpointers.png" alt="bestpaths" /></p>

<p>Now <code class="language-plaintext highlighter-rouge">best_last_tags</code> holds the tag to start backtracking with for every example, which might be a verb like depicted above.
Based on this last tag you just need to follow the path backward to find the correct sequence. Selecting the backpointer
is what happens in the following lines (where <code class="language-plaintext highlighter-rouge">active</code> holds the backpointers for the active sequences):</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Follow the backpointers to the best previous tag.
</span><span class="n">best_tag_id</span> <span class="o">=</span> <span class="n">best_tag_id</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">num_active</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">best_tag_id</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">gather</span><span class="p">(</span><span class="n">active</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">best_tag_id</span><span class="p">)</span>
<span class="n">best_paths</span><span class="p">[:</span><span class="n">num_active</span><span class="p">,</span> <span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">best_tag_id</span><span class="p">.</span><span class="n">squeeze</span><span class="p">()</span></code></pre></figure>

<p>In the end, <code class="language-plaintext highlighter-rouge">best_paths</code> holds the maximizing sequence for every example in the batch, starting at the <code class="language-plaintext highlighter-rouge">&lt;ROOT&gt;</code> tag.
We return the scores and the sequences without the root and we are done!</p>

<p>Go to <a href="/2021/01/25/crfpt3.html">part three</a> to train this model on real POS tagging data!</p>

<h1 id="sources"><span style="color:#2874A6">Sources</span></h1>

<p>Xuezhe Ma and Eduard H. Hovy. 2016. <a href="https://arxiv.org/pdf/1603.01354.pdf" target="_blank"><em>End-to-end
sequence labeling via bi-directional LSTM-CNN-CRF</em></a>. In ACL.</p>

<p>Matt Gardner and Joel Grus and Mark Neumann and Oyvind Tafjord
    and Pradeep Dasigi and Nelson F. Liu and Matthew Peters and
    Michael Schmitz and Luke S. Zettlemoyer. 2017 <a href="https://www.semanticscholar.org/paper/A-Deep-Semantic-Natural-Language-Processing-Gardner-Grus/a5502187140cdd98d76ae711973dbcdaf1fef46d" target="_blank"><em>AllenNLP: A Deep Semantic Natural Language Processing Platform.</em></a></p>

  </div><a class="u-url" href="/2021/01/25/crfpt2.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Ramblings</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Ramblings</li><li><a class="u-email" href="mailto:lauraruis92 at gmail dot com">lauraruis92 at gmail dot com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/LauraRuis"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">LauraRuis</span></a></li><li><a href="https://www.twitter.com/lauraruis"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">lauraruis</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Blog about AI stuff.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
