<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Structured Prediction Pt. 2 - Implementing a linear-chain CRF | Ramblings</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Structured Prediction Pt. 2 - Implementing a linear-chain CRF" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Blog about AI stuff." />
<meta property="og:description" content="Blog about AI stuff." />
<link rel="canonical" href="/2021/01/25/crfpt2.html" />
<meta property="og:url" content="/2021/01/25/crfpt2.html" />
<meta property="og:site_name" content="Ramblings" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-01-25T13:09:17-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Structured Prediction Pt. 2 - Implementing a linear-chain CRF" />
<script type="application/ld+json">
{"headline":"Structured Prediction Pt. 2 - Implementing a linear-chain CRF","dateModified":"2021-01-25T13:09:17-05:00","datePublished":"2021-01-25T13:09:17-05:00","url":"/2021/01/25/crfpt2.html","mainEntityOfPage":{"@type":"WebPage","@id":"/2021/01/25/crfpt2.html"},"description":"Blog about AI stuff.","@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="/feed.xml" title="Ramblings" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Ramblings</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Structured Prediction Pt. 2 - Implementing a linear-chain CRF</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2021-01-25T13:09:17-05:00" itemprop="datePublished">Jan 25, 2021
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<p>In this part of the series of posts on structured prediction with conditional random fields (CRFs) we are going to implement all the ingredients
that were discussed in <a href="/2021/01/25/crfpt1.html">part 1</a>. Recall that we discussed how to model
the dependencies among labels in sequence prediction tasks with a linear-chain CRF. Now, we will put a such a CRF
on top of a neural network feature extractor and use it for
part-of-speech (POS) tagging.</p>

<p><img src="/images/opener_gif/opener.gif" alt="annotated_example" /></p>

<p>To learn a model that can annotate examples like the one above with POS tags, we need two things:</p>

<ol>
  <li>
    <p>A dataset with examples consisting of input sentences annotated with POS tags.
We will choose the <a href="http://universaldependencies.org/" target="_blank">Universal Dependencies</a> dataset (<a href="https://www.aclweb.org/anthology/L14-1067/" target="_blank">Silveira et al., 2014</a>).</p>
  </li>
  <li>
    <p>A feature extractor to extract features from our input data. For this we will choose a bidirectional LSTM, which we will motivate below.</p>
  </li>
</ol>

<p>Then all the things we need to implement are:</p>

<ul>
  <li>
    <p>A <code class="language-plaintext highlighter-rouge">Vocabulary</code> to convert from strings to numerical values that are machine readable</p>
  </li>
  <li>
    <p>A <code class="language-plaintext highlighter-rouge">TaggingDataset</code> to convert all our data to <code class="language-plaintext highlighter-rouge">Tensors</code> that can be processed by <a href="https://pytorch.org/" target="_blank">PyTorch</a></p>
  </li>
  <li>
    <p>An <code class="language-plaintext highlighter-rouge">Encoder</code> model that is our feature extractor (the bidirectional LSTM).</p>
  </li>
  <li>
    <p>A <code class="language-plaintext highlighter-rouge">ChainCRF</code> model that implements all the CRF methods, like belief propagation (BF) and Viterbi decoding.</p>
  </li>
  <li>
    <p>A <code class="language-plaintext highlighter-rouge">train()</code> loop to train our CRF and feature-extractor end-to-end on data.</p>
  </li>
  <li>
    <p>A <code class="language-plaintext highlighter-rouge">test()</code> loop to test a trained model on new data.</p>
  </li>
</ul>

<p>The final model we get by walking through this post is depicted in the image below.</p>

<p><img src="/images/bilstmcrf.png" alt="bilstmcrf" width="600" class="center" /></p>

<p>To train this model end-to-end we will use the negative log-likelihood (NLL) loss function, which is simply the negated log-likelihood
that was given in part 1 of this series. Given some example input-output pairs \((\mathbf{x}^{(i)}, \mathbf{y}^{(i)})_{i=1}^N\),
the NLL of the entire dataset is:</p>

\[\begin{aligned}
\text{NLL} = -\log \mathcal{L}(\boldsymbol{\theta}) &amp;= - \sum_{i=1}^{N} \log p(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)}) \\
&amp;= \sum_{i=1}^{N}\log\left(Z(\mathbf{x}^{(i)})\right) - \sum_{i=1}^{N}\left(\sum_{t=1}^m \boldsymbol{\theta}_1 f(y_t^{(i)}, \mathbf{x}^{(i)}, t) + \sum_{t=1}^{m-1} \boldsymbol{\theta}_2 f(y_t^{(i)}, y_{t+1}^{(i)})\right) \\
\end{aligned}\]

<p>Instead of maximizing the log-likelihood of our data, we will minimize the negative log-likelihood, which is equivalent.
We can use stochastic gradient descent (SGD) with automatic differentiation in <a href="https://pytorch.org/" target="_blank">PyTorch</a>, meaning we only need the forward-part of the BP algorithm.
Recall that the forward recursion allows calculation of the partition function (\(Z(\mathbf{x})\)), which we need for the NLL. The backward recursion allows calculating the
marginals (which are needed for the gradients). PyTorch takes care of the latter calculation for us.</p>

<p>Let’s start with feature extraction and defining \(\boldsymbol{\theta}_1\), \(\boldsymbol{\theta}_2\), \(f(y_t, \mathbf{x}, t)\), and \(f(y_t, y_{t+1})\).</p>

<h2 id="preliminaries"><span style="color:#C0392B">Preliminaries</span></h2>

<p>Ok I lied, let’s start with some preliminaries, if you want to run the code used in this post yourself, make sure to install <strong>PyTorch &gt;= 1.7.0</strong>,
<strong>Python 3.6</strong>, and <strong>TorchNLP &gt;= 0.5.0</strong>.</p>

<h2 id="feature-extraction"><span style="color:#C0392B">Feature Extraction</span></h2>

<p>There are some desiderata for our features. The function \(f(y_t, \mathbf{x}, t)\) signifies that we want each tag in the output sequence \(y_t\) to be informed about (i.e., depend on) the entire input sequence \(\mathbf{x}\),
but also on the current word at time \(t\).
Furthermore, \(f(y_t, y_{t+1})\) tells us that we want each next output tag \(y_{t+1}\) to depend on the previous tag \(y_{t}\).
Parametrizing the former part, we take \(\boldsymbol{\theta}_1f(y_t, \mathbf{x}, t)\) to be the output of a bidirectional LSTM projected down to the right dimension with a linear layer:</p>

\[\begin{aligned}
\mathbf{\bar{H}} &amp;= \text{biLSTM}(\mathbf{x}) \\
\mathbf{H} &amp;= \mathbf{\bar{H}}\mathbf{W} + \mathbf{b}
\end{aligned}\]

<p>In the above, \(\mathbf{x} \in \mathbb{R}^{m}\) is our input sequence, \(\mathbf{\bar{H}} \in \mathbb{R}^{m \times 2d_h}\) the hidden vectors for each input word \(x_t\) stacked into a matrix, with \(d_h\) the hidden dimension of the LSTM (doubled because
a bidirectional LSTM is essentially two LSTMs processing the input sequence from left-to-right and right-to-left).
\(\mathbf{W} \in \mathbb{R}^{2d_h \times |S|}\) a matrix of parameters that projects the output to the right dimension, namely \(|S|\) values for each input word \(x_t\). The t-th row of \(\mathbf{H} \in \mathbb{R}^{m \times |S|}\)
(let’s define that by \(\mathbf{H}_{t*}\)) then holds the features for the t-th word (\(x_t\)) in the input sequence. 
These values reflect \(\boldsymbol{\theta}_1f(y_t, \mathbf{x}_t, t)\) for each possible \(y_t\), since they depend on the entire input sequence \(\mathbf{x}\), but are specific to the current word at time \(t\).</p>

\[\boldsymbol{\theta}_1f(y_t, \mathbf{x}_t, t) = \mathbf{H}_{t,y_t}\]

<p>Using PyTorch, we can code this up in a few lines. As is common in computational models of language, we will assign each
input token a particular index and use dense word embeddings that represent each word in our input vocabulary. We reserve
index 0 for the special token <code class="language-plaintext highlighter-rouge">&lt;PAD&gt;</code>, which we need later when we will batch our examples, grouping together examples
of different length \(m\).</p>

<p>Strictly speaking btw, our bi-LSTM does not take \(\mathbf{x}\) as input but \(\mathbf{E}_{x_t*}\), where
\(\mathbf{E}\) is the matrix of embedding parameters of size \(|S| \times d_e\). So if \(x_t = 3\) (index 3 in our vocabulary, which might
be mapping to <em>book</em> for example), \(\mathbf{E}_{x_t*}\) takes out the corresponding embedding of size \(d_e\).</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">Encoder</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="s">"""
  A simple encoder model to encode sentences. Bi-LSTM over word embeddings.
  """</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocabulary_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                     <span class="n">hidden_dimension</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">padding_idx</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Encoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
    <span class="c1"># The word embeddings.
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">num_embeddings</span><span class="o">=</span><span class="n">vocabulary_size</span><span class="p">,</span> 
                                  <span class="n">embedding_dim</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">,</span>
                                  <span class="n">padding_idx</span><span class="o">=</span><span class="n">padding_idx</span><span class="p">)</span>
    
    <span class="c1"># The bi-LSTM.
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">bi_lstm</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">,</span> 
                           <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_dimension</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> 
                           <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
  
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sentence</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="s">"""
    :param sentence: input sequence of size [batch_size, sequence_length]
    Returns: tensor of size [batch_size, sequence_length, hidden_size * 2] 
    the hidden states of the biLSTM for each time step.
    """</span>
    <span class="n">embedded</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
    <span class="c1"># embedded: [batch_size, sequence_length, embedding_dimension]
</span>    
    <span class="n">output</span><span class="p">,</span> <span class="p">(</span><span class="n">hidden</span><span class="p">,</span> <span class="n">cell</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">bi_lstm</span><span class="p">(</span><span class="n">embedded</span><span class="p">)</span>
    <span class="c1"># output: [batch_size, sequence_length, hidden_size * 2]
</span>    <span class="c1"># hidden: [batch_size, hidden_size * 2]
</span>    <span class="c1"># cell: [batch_size, hidden_size * 2]
</span>    <span class="k">return</span> <span class="n">output</span></code></pre></figure>

<p>That’s all we need to do to extract features from out input sequences! Later in this post we will define \(\boldsymbol{\theta}_2\) 
and \(f(y_t, y_{t+1})\), which are part of the actual CRF. First, we’ll set up our <code class="language-plaintext highlighter-rouge">Tagger</code>-module which takes the encoder,
the CRF (to implement later), and outputs the negative log-likelihood and a predicted tag sequence.</p>

<h2 id="the-tagger"><span style="color:#C0392B">The Tagger</span></h2>

<p>Below we can find the <code class="language-plaintext highlighter-rouge">Tagger</code> module. This is basically the class that implements the model as depicted in the image
above. The most interesting part is still missing (namely the <code class="language-plaintext highlighter-rouge">ChainCRF</code>-module, but also the <code class="language-plaintext highlighter-rouge">Vocabulary</code>-module), but bear with me, we’ll get to those. The
 forward pass of the <code class="language-plaintext highlighter-rouge">Tagger</code> takes an input sequence, a target sequence, and an input mask (we’ll get to what that is
 when we discuss batching), puts the input sequence through the encoder and the CRF, and outputs the NLL <code class="language-plaintext highlighter-rouge">loss</code>, the <code class="language-plaintext highlighter-rouge">score</code> (
 which is basically the nominator of \(p(\mathbf{y} \mid \mathbf{x})\)), and the <code class="language-plaintext highlighter-rouge">tag_sequence</code> obtained by decoding with viterbi.
If you don’t have a feeling of what the parameters <code class="language-plaintext highlighter-rouge">input_mask</code> and <code class="language-plaintext highlighter-rouge">input_lengths</code> in the module should be, don’t worry,
that will be discussed in the section ‘sequence batching’ below.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">Tagger</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="s">"""
  A POS tagger.
  """</span>
  
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_vocabulary</span><span class="p">:</span> <span class="n">Vocabulary</span><span class="p">,</span> 
               <span class="n">target_vocabulary</span><span class="p">:</span> <span class="n">Vocabulary</span><span class="p">,</span>
               <span class="n">embedding_dimension</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">hidden_dimension</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Tagger</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
    <span class="c1"># The Encoder to extract features from the input sequence.
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">Encoder</span><span class="p">(</span><span class="n">vocabulary_size</span><span class="o">=</span><span class="n">input_vocabulary</span><span class="p">.</span><span class="n">size</span><span class="p">,</span> 
                           <span class="n">embedding_dim</span><span class="o">=</span><span class="n">embedding_dimension</span><span class="p">,</span> 
                           <span class="n">hidden_dimension</span><span class="o">=</span><span class="n">hidden_dimension</span><span class="p">,</span> 
                           <span class="n">padding_idx</span><span class="o">=</span><span class="n">input_vocabulary</span><span class="p">.</span><span class="n">pad_idx</span><span class="p">)</span>
    
    <span class="c1"># The linear projection (with parameters W and b).  
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">encoder_to_tags</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_dimension</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> 
                                     <span class="n">target_vocabulary</span><span class="p">.</span><span class="n">size</span><span class="p">)</span>
    
    <span class="c1"># The linear-chain CRF.
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">tagger</span> <span class="o">=</span> <span class="n">ChainCRF</span><span class="p">(</span><span class="n">num_tags</span><span class="o">=</span><span class="n">target_vocabulary</span><span class="p">.</span><span class="n">size</span><span class="p">,</span> 
                           <span class="n">tag_vocabulary</span><span class="o">=</span><span class="n">target_vocabulary</span><span class="p">)</span>
  
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_sequence</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> 
              <span class="n">target_sequence</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> 
              <span class="n">input_mask</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="s">"""
    :param input_sequence: input sequence of size [batch_size, sequence_length]
    :param target_sequence: POS tags target, [batch_size, sequence_length]
    :param input_mask: padding-mask, [batch_size, sequence_length]
    :param input_lengths: lengths of each example in the batch [batch_size]
    Returns: ...
    """</span>
    <span class="c1"># input_sequence: [batch_size, sequence_length, input_vocabulary_size]
</span>    <span class="n">lstm_features</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">input_sequence</span><span class="p">)</span>
    <span class="c1"># lstm_features: [batch_size, sequence_length, hidden_dimension*2]
</span>    
    <span class="n">crf_features</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">encoder_to_tags</span><span class="p">(</span><span class="n">lstm_features</span><span class="p">)</span>
    <span class="c1"># crf_features: [batch_size, sequence_length, target_vocabulary_size]
</span>    
    <span class="n">loss</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span> <span class="n">tag_sequence</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">tagger</span><span class="p">(</span><span class="n">input_features</span><span class="o">=</span><span class="n">crf_features</span><span class="p">,</span>
                                            <span class="n">target_tags</span><span class="o">=</span><span class="n">target_sequence</span><span class="p">,</span>
                                            <span class="n">input_mask</span><span class="o">=</span><span class="n">input_mask</span><span class="p">,</span>
                                            <span class="n">input_lengths</span><span class="o">=</span><span class="n">input_lengths</span><span class="p">)</span>
    <span class="c1"># loss, score: scalars
</span>    <span class="c1"># tag_sequence: [batch_size, sequence_length]
</span>    <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span> <span class="n">tag_sequence</span></code></pre></figure>

<p>OK, now we can finally get to the definition of \(\boldsymbol{\theta}_2\) and \(f(y_t, y_{t+1})\), and the implementation of the linear-chain CRF!</p>

<h2 id="implementing-a-linear-chain-crf"><span style="color:#C0392B">Implementing a Linear-Chain CRF</span></h2>

<p>We want \(f(y_t, y_{t+1})\) to represent the likelihood of some tag \(y_{t+1}\) following \(y_t\) in the sequence, which
can be interpreted as transition ‘probabilities’ from one tag to another. The parameters \(\boldsymbol{\theta}_2\) are shared
over time (meaning they are the same for each \(t \in \{1, \dots, m-1\}\)), and thus we can simply define a matrix of transition ‘probabilities’ from each tag to another tag.
We define \(\boldsymbol{\theta}_2f(y_t, y_{t+1}) = \mathbf{T}_{y_t,y_{t+1}}\), meaning the t-th row and (t+1)-th column of a matrix \(\mathbf{T}\).
This matrix \(\mathbf{T}\) will be of size \((|S| + 2) \times (|S| + 2)\). The 2 extra tags are the <code class="language-plaintext highlighter-rouge">&lt;ROOT&gt;</code> and the <code class="language-plaintext highlighter-rouge">&lt;EOS&gt;</code> tag.
We need some tag to start of the sequence and to end the sequence, because we want to take into account the probability of a particular tag being the
first tag of a sequence, and the probability of a tag being the last tag. For the former we will use <code class="language-plaintext highlighter-rouge">&lt;ROOT&gt;</code>, and a for the latter we’ll use <code class="language-plaintext highlighter-rouge">&lt;EOS&gt;</code>.</p>

<p>In this part we will implement:</p>

<ul>
  <li>
    <p>The forward-pass of belief propagation (<code class="language-plaintext highlighter-rouge">ChainCRF.forward_belief_propagation(...)</code>), calculating the partition function (i.e., the denominator of \(p(\mathbf{y} \mid \mathbf{x})\)).</p>
  </li>
  <li>
    <p>Calculating the nominator of \(p(\mathbf{y} \mid \mathbf{x})\) (<code class="language-plaintext highlighter-rouge">ChainCRF.score_sentence(...)</code>)</p>
  </li>
  <li>
    <p>Decoding to get the target sequence prediction (<code class="language-plaintext highlighter-rouge">ChainCRF.viterbi_decode(...)</code>)</p>
  </li>
</ul>

<p>Below, you’ll find the <code class="language-plaintext highlighter-rouge">ChainCRF</code> class that holds all these methods. The matrix of transition probabilities \(\mathbf{T}\)
is initialized below as <code class="language-plaintext highlighter-rouge">log_transitions</code> (in log-space, so the matrix <code class="language-plaintext highlighter-rouge">log_transitions</code> corresponds to \(\log\mathbf{T}\)), and we hard-code the transition probabilities from any \(y_t\) to the <code class="language-plaintext highlighter-rouge">&lt;ROOT&gt;</code>
tag to be -10000 because this should not be possible (remember that to go from log-probabilities to probabilities we do <code class="language-plaintext highlighter-rouge">exp(log_transitions)</code> and \(\exp-10000 \approx 0\)).
We do the same for any transition from <code class="language-plaintext highlighter-rouge">&lt;EOS&gt;</code> to any other tag. In the sections below we will implement the necessary methods for our linear-chain CRF, starting with belief propagation.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"> <span class="k">class</span> <span class="nc">ChainCRF</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
      <span class="s">"""
      A linear-chain conditional random field.
      """</span>
      
      <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_tags</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">tag_vocabulary</span><span class="p">:</span> <span class="n">Vocabulary</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ChainCRF</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">tag_vocabulary</span> <span class="o">=</span> <span class="n">tag_vocabulary</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_tags</span> <span class="o">=</span> <span class="n">num_tags</span> <span class="o">+</span> <span class="mi">2</span> <span class="c1"># +2 for &lt;ROOT&gt; and &lt;EOS&gt;
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">root_idx</span> <span class="o">=</span> <span class="n">tag_vocabulary</span><span class="p">.</span><span class="n">size</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">end_idx</span> <span class="o">=</span> <span class="n">tag_vocabulary</span><span class="p">.</span><span class="n">size</span> <span class="o">+</span> <span class="mi">1</span>

        <span class="c1"># Matrix of transition parameters.  Entry (i, j) is the score of
</span>        <span class="c1"># transitioning *from* i *to* j.
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">log_transitions</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_tags</span><span class="p">,</span> 
                                                        <span class="bp">self</span><span class="p">.</span><span class="n">num_tags</span><span class="p">))</span>

        <span class="c1"># Initialize the log transitions with xavier uniform (TODO: refer)
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">xavier_uniform</span><span class="p">()</span>

        <span class="c1"># These two statements enforce the constraint that we never transfer
</span>        <span class="c1"># to the start tag and we never transfer from the stop tag
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">log_transitions</span><span class="p">.</span><span class="n">data</span><span class="p">[:,</span> <span class="bp">self</span><span class="p">.</span><span class="n">root_idx</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">10000.</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">log_transitions</span><span class="p">.</span><span class="n">data</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">end_idx</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">10000.</span>
      
      <span class="k">def</span> <span class="nf">xavier_uniform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="mf">1.</span><span class="p">):</span>
        <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">log_transitions</span><span class="p">)</span>

      <span class="k">def</span> <span class="nf">forward_belief_propagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_features</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> 
                                    <span class="n">input_mask</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nb">NotImplementedError</span><span class="p">()</span>
      
      <span class="k">def</span> <span class="nf">score_sentence</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_features</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span>
                        <span class="n">target_tags</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> 
                        <span class="n">input_mask</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nb">NotImplementedError</span><span class="p">()</span>
          
      <span class="k">def</span> <span class="nf">viterbi_decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_features</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> 
                               <span class="n">input_lengths</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="k">raise</span> <span class="nb">NotImplementedError</span><span class="p">()</span>
      
      <span class="k">def</span> <span class="nf">negative_log_likelihood</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_features</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> 
                                  <span class="n">target_tags</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> 
                                  <span class="n">input_mask</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="s">"""
        Returns the NLL loss.
        :param input_features: the features for each input sequence
                [batch_size, sequence_length, feature_dimension]
        :param target_tags: the target tags
                [batch_size, sequence_length]
        :param input_mask: the binary mask determining which of 
                the input entries are padding [batch_size, sequence_length]
        """</span>
        <span class="n">partition_function</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">forward_belief_propagation</span><span class="p">(</span>
                                    <span class="n">input_features</span><span class="o">=</span><span class="n">input_features</span><span class="p">,</span> 
                                    <span class="n">input_mask</span><span class="o">=</span><span class="n">input_mask</span><span class="p">)</span>
        <span class="n">nominator</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">score_sentence</span><span class="p">(</span>
                                    <span class="n">input_features</span><span class="o">=</span><span class="n">input_features</span><span class="p">,</span>
                                    <span class="n">target_tags</span><span class="o">=</span><span class="n">target_tags</span><span class="p">,</span> 
                                    <span class="n">input_mask</span><span class="o">=</span><span class="n">input_mask</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">partition_function</span> <span class="o">-</span> <span class="n">nominator</span>

      <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_features</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> 
                  <span class="n">target_tags</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> 
                  <span class="n">input_mask</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span>
                  <span class="n">input_lengths</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> 
                                                        <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> 
                                                        <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">]:</span>
        <span class="s">"""
        The forward-pass of the CRF, which calculates the NLL loss and 
        returns a predicted sequence.
        :param input_features: features for each input sequence
                [batch_size, sequence_length, feature_dimension]
        :param target_tags: the target tags 
                [batch_size, sequence_length]
        :param input_mask: the binary mask determining which of 
                the input entries are padding [batch_size, sequence_length]
        """</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">negative_log_likelihood</span><span class="p">(</span><span class="n">input_features</span><span class="o">=</span><span class="n">input_features</span><span class="p">,</span> 
                                            <span class="n">target_tags</span><span class="o">=</span><span class="n">target_tags</span><span class="p">,</span>
                                            <span class="n">input_mask</span><span class="o">=</span><span class="n">input_mask</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
          <span class="n">score</span><span class="p">,</span> <span class="n">tag_sequence</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">viterbi_decode</span><span class="p">(</span><span class="n">input_features</span><span class="p">,</span>
                                                    <span class="n">input_lengths</span><span class="o">=</span><span class="n">input_lengths</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span> <span class="n">tag_sequence</span>
        </code></pre></figure>

<p>But first, since we implement all these method in batched versions, let’s briefly go over <strong>batching</strong>.</p>

<h1 id="sequence-batching"><span style="color:#C0392B">Sequence Batching</span></h1>

<p>Processing data points in batches has multiple benefits: averaging the gradient over a minibatch in SGD allows playing
with the noise you want while training your model (batch size of 1 gives a maximally noisy gradient, batch size of \(N\) is minimally
noisy gradient, namely gradient descent without the stochasticity), but also: batching examples speeds up training. This motivates
us to implement all the methods for the CRF in batched versions, allowing parallel processing. We can’t parallelize the time-dimension in CRFs unfortunately.</p>

<p>A batch of size 2 would look like this:</p>

<p><img src="/images/batch.png" alt="batch" /></p>

<p>For each batch we will additionally keep track of the lengths in the batch. For the image above a list of lengths would be <code class="language-plaintext highlighter-rouge">input_lengths = [8, 5]</code>.
For example, batched inputs to our encoder will be of size <code class="language-plaintext highlighter-rouge">[batch_size, sequence_length]</code> and outputs of size <code class="language-plaintext highlighter-rouge">[batch_size, sequence_length, hidden_dim*2]</code>.
The input mask for the above batch looks like this:</p>

<p><img src="/images/input_mask.png" alt="input_mask" /></p>

<h1 id="forward-belief-propagation"><span style="color:#C0392B">Forward Belief Propagation</span></h1>

<p>In this section we will implement the forward-pass of belief propagation, which we need to calculate part of the NLL loss (namely \(\log\left(Z(\mathbf{x}^{(i)})\right)\)). We will do this in log-space, because numerical stability doesn’t fare well with
recursive multiplication of probabilities (i.e., values between 0 and 1). In log-space, multiplications become summations, which have less of a risk of becoming too small.</p>

<p>Recall that the initialization and recursion in the forward-pass of belief propagation are given by the following equations:</p>

\[\begin{aligned}
\alpha(1, y^{\prime}_2) &amp;= \sum_{y^{\prime}_1}\psi(y^{\prime}_1, \mathbf{x}, 1) \cdot  \psi(y^{\prime}_1, y^{\prime}_{2}) \\
\alpha(t, y^{\prime}_{t+1}) &amp;\leftarrow \sum_{y^{\prime}_{t}}\psi(y^{\prime}_t, \mathbf{x}, t) \cdot  \psi(y^{\prime}_t, y^{\prime}_{t+1})\cdot \alpha(t-1, y^{\prime}_t)
\end{aligned}\]

<p>We discussed above that \(\psi(y^{\prime}_t, \mathbf{x}, t) = \mathbf{H}_{t,y_t}\) (the t-th row and \(y_t\)-th column of the projected output of our encoder) and that \(\psi(y^{\prime}_t, y^{\prime}_{t+1}) = \mathbf{T}_{y_t,y_{t+1}}\)
(the \(y_t\)-th row and \(y_{t+1}\)-th columnd of the matrix of transition probabilities). So in the code <code class="language-plaintext highlighter-rouge">input_features</code> is a batch with \(\mathbf{H} \in \mathbb{R}^{m \times |S|}\) for each data point. 
The <code class="language-plaintext highlighter-rouge">log_transitions</code>, or \(\log\mathbf{T}_{y_t,y_{t+1}}\), on the other hand, are the same for each example in the batch.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">forward_belief_propagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_features</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">,</span> 
                                     <span class="n">input_mask</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">)</span>
    <span class="s">"""
    Efficient inference with BP of the partition function of the ChainCRF.
    :param input_features: the features for each input sequence
            [batch_size, sequence_length, num_tags] 
    :param input_mask: the binary mask determining which of the input 
            entries are padding [batch_size, sequence_length]
    """</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">num_tags</span> <span class="o">=</span> <span class="n">input_features</span><span class="p">.</span><span class="n">size</span><span class="p">()</span>
    
    <span class="c1"># We don't have input features for the tags &lt;ROOT&gt; and &lt;EOS&gt;, so artifially
</span>    <span class="c1"># construct those.
</span>    <span class="n">input_features</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">(</span>
        <span class="p">[</span><span class="n">input_features</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span> <span class="o">-</span> <span class="mf">10000.</span><span class="p">],</span>
        <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="c1"># Initialize the recursion variables with 
</span>    <span class="c1"># transitions from root token + first emission probabilities.
</span>    <span class="n">init_alphas</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">log_transitions</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">root_idx</span><span class="p">,</span> <span class="p">:]</span> <span class="o">+</span> <span class="n">input_features</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>

    <span class="c1"># Set recursion variable.
</span>    <span class="n">forward_var</span> <span class="o">=</span> <span class="n">init_alphas</span>

    <span class="c1"># Make time major.
</span>    <span class="n">input_features</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">input_features</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="c1"># input_features: [sequence_length, batch_size, num_tags]
</span>    <span class="n">input_mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">input_mask</span><span class="p">.</span><span class="nb">float</span><span class="p">(),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="c1"># input_mask [sequence_length, batch_size]
</span>
    <span class="c1"># Loop over sequence and calculate the transition probability for the next 
</span>    <span class="c1"># tag at each step (from t - 1 to t)
</span>    <span class="c1"># current tag at t - 1, next tag at t
</span>    <span class="c1"># emission probabilities: (example, next tag)
</span>    <span class="c1"># transition probabilities: (current tag, next tag)
</span>    <span class="c1"># forward var: (instance, current tag)
</span>    <span class="c1"># next tag var: (instance, current tag, next tag)
</span>    <span class="k">for</span> <span class="n">time</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_time</span><span class="p">):</span>

      <span class="c1"># Get emission scores for this time step.
</span>      <span class="n">features</span> <span class="o">=</span> <span class="n">input_features</span><span class="p">[</span><span class="n">time</span><span class="p">]</span>

      <span class="c1"># Broadcast emission probabilities.
</span>      <span class="n">emit_scores</span> <span class="o">=</span> <span class="n">features</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">num_tags</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

      <span class="c1"># Calculate transition probabilities (broadcast over example axis, same for all examples in batch).
</span>      <span class="n">transition_scores</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">log_transitions</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

      <span class="c1"># Calculate next tag probabilities.
</span>      <span class="n">next_tag_var</span> <span class="o">=</span> <span class="n">forward_var</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="n">emit_scores</span> <span class="o">+</span> <span class="n">transition_scores</span>
      <span class="k">print</span><span class="p">(</span><span class="n">next_tag_var</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

      <span class="c1"># Calculate next forward var by taking logsumexp over next tag axis, mask all instances that ended
</span>      <span class="c1"># and keep old forward var for instances those.
</span>      <span class="n">forward_var</span> <span class="o">=</span> <span class="p">(</span><span class="n">logsumexp</span><span class="p">(</span><span class="n">next_tag_var</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">input_mask</span><span class="p">[</span><span class="n">time</span><span class="p">].</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span>
                      <span class="n">forward_var</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">input_mask</span><span class="p">[</span><span class="n">time</span><span class="p">]).</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

    <span class="n">final_transitions</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">log_transitions</span><span class="p">[:,</span> <span class="bp">self</span><span class="p">.</span><span class="n">end_idx</span><span class="p">]</span>

    <span class="n">alphas</span> <span class="o">=</span> <span class="n">forward_var</span> <span class="o">+</span> <span class="n">final_transitions</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">partition_function</span> <span class="o">=</span> <span class="n">logsumexp</span><span class="p">(</span><span class="n">alphas</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">partition_function</span></code></pre></figure>

<h1 id="sources"><span style="color:#2874A6">Sources</span></h1>

<p>Natalia Silveira and Timothy Dozat and Marie-Catherine de Marneffe and Samuel Bowman and
    Miriam Connor and John Bauer and Christopher D. Manning (2014).
    <a href="https://www.aclweb.org/anthology/L14-1067/" target="_blank"><em>A Gold Standard Dependency Corpus for English</em></a></p>

<p>Xuezhe Ma and Eduard H. Hovy. 2016. <a href="https://arxiv.org/pdf/1603.01354.pdf" target="_blank"><em>End-to-end
sequence labeling via bi-directional LSTM-CNNsCRF</em></a>. In ACL.</p>

  </div><a class="u-url" href="/2021/01/25/crfpt2.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Ramblings</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Ramblings</li><li><a class="u-email" href="mailto:lauraruis92 at gmail dot com">lauraruis92 at gmail dot com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/LauraRuis"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">LauraRuis</span></a></li><li><a href="https://www.twitter.com/lauraruis"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">lauraruis</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Blog about AI stuff.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
