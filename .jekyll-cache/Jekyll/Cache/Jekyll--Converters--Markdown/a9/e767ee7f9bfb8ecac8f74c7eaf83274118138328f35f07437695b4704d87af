I"<~<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<p><img src="/images/figure1.png" alt="figure1" /></p>

<p>This post is an attempt at shedding some light on the above image, taken from 
<a href="https://arxiv.org/pdf/2110.09485.pdf" target="_blank">Balestriero, Pesenti, and LeCun, 2021.</a> The goal is to reproduce it, and, in the process of doing
that, getting a better understanding of what this whole interpolation/extrapolation debate 
(<a href="https://twitter.com/ylecun/status/1409940043951742981" target="_blank">here</a> and
<a href="https://twitter.com/GaryMarcus/status/1411401507610796032" target="_blank">here</a> and
<a href="https://twitter.com/fchollet/status/1450524400227287040" target="_blank">here</a>) is about. I mean, I have to be honest.
The whole debate is not going to be any clearer after reading this post. David Hume describes quite nicely what is probably 
going on in this discussion in his <em>“an enquiry concerning human understanding”</em>:</p>

<blockquote>
  <p>It might reasonably be expected, in questions, which have been canvassed and disputed with great eagerness, since the 
first origin of science and philosophy, that the meaning of all terms, at least, should have been agreed upon among the
disputants; and our enquiries, in the course of two thousand years, have been able to pass from words to the true and 
real subject of the controversy. For how easy may it seem to give exact definitions of the terms employed in reasoning,
and make these definitions, not the mere sounds of words, the object of future scrutiny and examination? But if we consider
the matter more narrowly, we shall be apt to draw a quite opposite conclusion. From this circumstance alone, that a 
controversy has been long kept on foot, and remains still undecided, we may presume, that there is some ambiguity in
the expression, and that the disputants affix different ideas to the terms employed in the controversy.</p>
</blockquote>

<p>Basically; it seems like a lot of arguments between people are a result of not properly defining what we are talking about (and the
last author on this paper <a href="https://twitter.com/ylecun/status/1450809828268548101" target="_blank">probably agrees</a>). 
Didn’t this quote make me look really smart just now? Well think again, I never read Hume, and the 
reason I’m writing this blogpost is because I didn’t understand the learning in high dimensions
paper at all on first reading. With this post I hope to get a better understanding of it, and share that understanding.</p>

<p>After this post, we will hopefully know more about the following terms:</p>

<ul>
  <li>The curse of dimensionality</li>
  <li>Convex hull</li>
  <li>Ambient dimension</li>
  <li>Intrinsic dimension / Data manifold dimension</li>
  <li>Interpolation / Extrapolation</li>
</ul>

<h2 id="the-key-idea"><span style="color:#C0392B">The Key Idea</span></h2>
<p>The key idea in this paper is that we shouldn’t be using interpolation and extrapolation in the way they are defined in
 this paper as indicators
of generalization performance, because models are almost surely extrapolating. It doesn’t matter what the intrinsic dimension
of the data manifold is, which may be much lower than the dimensionality of our data representation. The authors hope that
the paper opens the door to better suited geometrical definitions of interpolation and extrapolation that align
with generalization performances in the context of high dimensional data. So let’s dive in!</p>

<h1 id="when-are-we-interpolating"><span style="color:#C0392B">When are we interpolating?</span></h1>
<blockquote>
  <p><strong><em>In this section:</em></strong>  Interpolation, Extrapolation, Convex Hull, Convex Combination, Affine Combination, Curse of Dimensionality</p>
</blockquote>

<p>The definition of interpolation in the paper is the following:</p>

<p><strong>Definition 1</strong>. Interpolation occurs for a sample \(\mathbf{x}\) whenever this sample belongs to the convex hull of a 
set of samples \(\mathbf{X} \triangleq \{\mathbf{x}_1, \dots, \mathbf{x}_N\}\), if not, extrapolation occurs.</p>

<p>So what is the <em>convex hull</em> of a set of samples?</p>

<p>A vector \(\mathbf{x}\) lies within the convex hull of a set of samples
\(\mathbf{x}_1, \dots, \mathbf{x}_N\) if we can write it as a convex combination of the samples:</p>

\[\mathbf{x} = \lambda_1 \mathbf{x}_1 + \dots + \lambda_N \mathbf{x}_N\]

<p>Subject to the constraints that the \(\lambda_i\)’s are nonnegative and sum to one: \(\lambda_i \geq 0\) and \(\sum_i \lambda_i = 1\).
Let’s have a look at what that means in a dimension we can still visualize.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Make up some samples that happen to form a nice square in R2.
</span><span class="n">hull_samples</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                         <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                         <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
                         
<span class="c1"># Here's a point that is a convex combination of the hull samples ..
</span><span class="n">point_in_hull</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">]])</span>

<span class="c1"># .. and here's one that isn't.
</span><span class="n">point_outside_hull</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">2.5</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>

<span class="c1"># Calculate the convex hull with scipy.spatial.ConvexHull
</span><span class="n">hull</span> <span class="o">=</span> <span class="n">ConvexHull</span><span class="p">(</span><span class="n">hull_samples</span><span class="p">)</span>

<span class="c1"># Let's plot it.
</span><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">hull_samples</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">hull_samples</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s">'o'</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s">'navy'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">point_in_hull</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">point_in_hull</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s">'o'</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s">'lightgreen'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">point_outside_hull</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">point_outside_hull</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s">'o'</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s">'tomato'</span><span class="p">)</span>
<span class="k">for</span> <span class="n">simplex</span> <span class="ow">in</span> <span class="n">hull</span><span class="p">.</span><span class="n">simplices</span><span class="p">:</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">hull_samples</span><span class="p">[</span><span class="n">simplex</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">hull_samples</span><span class="p">[</span><span class="n">simplex</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="s">'k'</span><span class="p">)</span></code></pre></figure>

<p><img src="/images/2d_convexhull.png" alt="2d_convexhull" width="400" class="center" /></p>

<p>Everything within (or on) this square is a convex combination of the convex hull of the four samples, everything outside
it but still in \(\mathbb{R}^2\) is an <em>affine combination</em> of the samples, relaxing the constraints on the \(\lambda_i\)’s. Note that we can easily
calculate the probability of lying within the convex hull of samples here if we assume that all values will lie between zero
and three. Let’s say we sample points uniformly between zero and three for both dimensions, then the probability of a new 
sample to lie within the convex hull from this subspace is the area of the convex hull divided by the total area:</p>

\[p(\mathbf{x} \in \text{Hull}(\mathbf{X})) = \frac{1}{9}\]

<p>In general, for a convex hull with area \(c\) in \(\mathbb{R}^2\) and data points between \([x_0, x_1]\) the probability of 
lying in the convex hull is \(\frac{c}{(x_1 - x_0)^2}\).</p>

<p>Now let’s see what happens if we move to three dimensions. We’ll stretch out the square from above into a cube and
calculate the probability of lying within this cube.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Make the cube of 3D hull points (8 points, each is a corner of the cube).
</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>

<span class="c1"># Set the same limits as the 2D example.
</span><span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_zlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="c1"># A function to get a convex combination of a set of points.
</span><span class="k">def</span> <span class="nf">convex_combination</span><span class="p">(</span><span class="n">points</span><span class="p">):</span>
  <span class="n">lambdas</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">points</span><span class="p">))</span>
  <span class="n">lambdas_n</span> <span class="o">=</span> <span class="n">lambdas</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">lambdas</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">points</span><span class="p">.</span><span class="n">transpose</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]),</span> <span class="n">lambdas_n</span><span class="p">)</span>

<span class="c1"># A function to get an affine combination of a set of points 
# that is *not* a convex combination.
</span><span class="k">def</span> <span class="nf">affine_combination</span><span class="p">(</span><span class="n">points</span><span class="p">):</span>
  <span class="n">lambdas</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">points</span><span class="p">))</span>
  <span class="n">lambdas_n</span> <span class="o">=</span> <span class="n">lambdas</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">lambdas</span><span class="p">)</span>
  <span class="n">offset</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.02</span><span class="p">),</span>
                             <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.02</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)])</span>
  <span class="n">lambdas_n</span> <span class="o">=</span> <span class="n">lambdas_n</span> <span class="o">+</span> <span class="n">offset</span>
  <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">points</span><span class="p">.</span><span class="n">transpose</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]),</span> <span class="n">lambdas_n</span><span class="p">)</span>

<span class="c1"># Plot some convex combinations of the hull data points.
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
  <span class="n">num_points</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
  <span class="n">point_indices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="n">num_points</span><span class="p">)</span>
  <span class="n">points</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">X</span><span class="p">[</span><span class="n">point_indices</span><span class="p">],</span> 
                     <span class="n">Y</span><span class="p">[</span><span class="n">point_indices</span><span class="p">],</span> 
                     <span class="n">Z</span><span class="p">[</span><span class="n">point_indices</span><span class="p">]]).</span><span class="n">transpose</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
  <span class="n">new_point</span> <span class="o">=</span> <span class="n">convex_combination</span><span class="p">(</span><span class="n">points</span><span class="p">)</span>
  <span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">*</span><span class="n">new_point</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s">'lightgreen'</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mf">2.5</span><span class="p">)</span>

<span class="c1"># Plot some affine combinations of the hull data points.
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
  <span class="n">num_points</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
  <span class="n">point_indices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="n">num_points</span><span class="p">)</span>
  <span class="n">points</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">X</span><span class="p">[</span><span class="n">point_indices</span><span class="p">],</span> 
                     <span class="n">Y</span><span class="p">[</span><span class="n">point_indices</span><span class="p">],</span> 
                     <span class="n">Z</span><span class="p">[</span><span class="n">point_indices</span><span class="p">]]).</span><span class="n">transpose</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
  <span class="n">new_point</span> <span class="o">=</span> <span class="n">affine_combination</span><span class="p">(</span><span class="n">points</span><span class="p">)</span>
  <span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">*</span><span class="n">new_point</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s">'tomato'</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mf">12.5</span><span class="p">)</span>

<span class="c1"># Plot the box around the convex hull.
</span><span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="s">'o'</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s">'navy'</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mf">7.5</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
  <span class="n">idx</span> <span class="o">=</span> <span class="n">i</span>
  <span class="n">idx_right</span> <span class="o">=</span> <span class="p">(</span><span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">4</span>
  <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">([</span><span class="n">X</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">idx_right</span><span class="p">]],</span>
          <span class="p">[</span><span class="n">Y</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">Y</span><span class="p">[</span><span class="n">idx_right</span><span class="p">]],</span>
          <span class="p">[</span><span class="n">Z</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">Z</span><span class="p">[</span><span class="n">idx_right</span><span class="p">]],</span> <span class="n">color</span> <span class="o">=</span> <span class="s">'k'</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
  <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">([</span><span class="n">X</span><span class="p">[</span><span class="n">idx</span><span class="o">+</span><span class="mi">4</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">idx_right</span><span class="o">+</span><span class="mi">4</span><span class="p">]],</span>
          <span class="p">[</span><span class="n">Y</span><span class="p">[</span><span class="n">idx</span><span class="o">+</span><span class="mi">4</span><span class="p">],</span> <span class="n">Y</span><span class="p">[</span><span class="n">idx_right</span><span class="o">+</span><span class="mi">4</span><span class="p">]],</span>
          <span class="p">[</span><span class="n">Z</span><span class="p">[</span><span class="n">idx</span><span class="o">+</span><span class="mi">4</span><span class="p">],</span> <span class="n">Z</span><span class="p">[</span><span class="n">idx_right</span><span class="o">+</span><span class="mi">4</span><span class="p">]],</span> <span class="n">color</span> <span class="o">=</span> <span class="s">'k'</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
  <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">([</span><span class="n">X</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">idx_right</span><span class="o">+</span><span class="mi">3</span><span class="p">]],</span>
          <span class="p">[</span><span class="n">Y</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">Y</span><span class="p">[</span><span class="n">idx_right</span><span class="o">+</span><span class="mi">3</span><span class="p">]],</span>
          <span class="p">[</span><span class="n">Z</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">Z</span><span class="p">[</span><span class="n">idx_right</span><span class="o">+</span><span class="mi">3</span><span class="p">]],</span> <span class="n">color</span> <span class="o">=</span> <span class="s">'k'</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span></code></pre></figure>

<p><img src="/images/3d_convexhull.png" alt="3d_convexhull" width="400" class="center" /></p>

<p>This cube has a volume of one, and the probability of lying within this cube has become:</p>

\[p(\mathbf{x} \in \text{Hull}(\mathbf{X})) = \frac{1}{27}\]

<p>In general, for a convex hull with volume \(c\) in \(\mathbb{R}^3\) and data points between \([x_0, x_1]\) the probability of
lying in the convex hull is \(\frac{c}{(x_1 - x_0)^3}\). This gives part of the intuition as to why the probability of 
lying within the convex hull quickly goes down as the dimensionality of the problem goes up; exponentially quickly.</p>

<p>This intuition is formalized in the paper by a theorem describing the limiting behaviour of the probability of lying in
 the convex hull for new (i.i.d.) samples from a Gaussian.</p>

<p><strong>Theorem 1</strong> (Baranay and Furedi, 1988). Given a \(d\)-dimensional dataset 
\(\mathbf{X} \triangleq \{\mathbf{x}_1, \dots, \mathbf{x}_N\}\) with i.i.d. samples \(\mathbf{x}_n \sim \mathcal{N}(0, \mathbb{I}_d)\),
\(\forall n\), the probability that a new sample \(\mathbf{x} \sim \mathcal{N}(0, \mathbb{I}_d)\) is in interpolation regime
(recall Def. 1) has the following limiting behavior</p>

\[\lim_{d \rightarrow \infty} p(\mathbf{x} \in \text{Hull}(\mathbf{X})) = \begin{cases} 
      1 &amp; \iff N &gt; d^{-1}2^{d/2} \\
      0 &amp; \iff N &lt; d^{-1}2^{d/2}
   \end{cases}\]

<p>This theorem says that, to keep the possibility of lying in the convex hull tend to one for increasing dimensions,
we need to exponentially increase the number of datapoints \(N\). See below this boundary of minimal \(N\) needed
 plotted (\(N = d^{-1}2^{d/2}\)).</p>

<p><img src="/images/evolution_N.png" alt="evolution_N" width="400" class="center" /></p>

<p>In this section we learned what the convex hull of datapoints is, and what it means to be interpolating or extrapolating w.r.t. the
convex hull of samples. We got some intuition about the curse of dimensionality, and why it is pretty unlikely to be in interpolation regime
for new data points in real-world datasets. We are ready to start reproducing the first plot.</p>

<h2 id="reproducing-the-first-plot"><span style="color:#C0392B">Reproducing the First Plot</span></h2>
<blockquote>
  <p><strong><em>In this section:</em></strong>  Ambient dimension</p>
</blockquote>

<p><img src="/images/first_plot.png" alt="firstplot" width="400" class="center" /></p>

<p>This image is a stack of six plots. Each plot shows the estimated
probability that a new sample from a Gaussian with dimension \(d\) will lie in the convex hull of \(N\) other samples of this
Gaussian. For example,
for the bottom plot the estimated probability that a new sample \(\mathbf{x} \sim \mathcal{N}(0, \mathbb{I}_2)\) lies in
the convex hull of the \(N = 10\) samples \(\mathbf{x}_1, \dots, \mathbf{x}_{10} \sim \mathcal{N}(0, \mathbb{I}_2)\) is roughly 50%.
For a gaussian with dimensionality 7 we already need more than \(10^{2.4} \approx 250\) examples to have roughly 50% chance of a new sample
being in the convex hull of those 250 examples. The black line through the six plots is the line that denotes how \(N\)
should change to keep the probability of a new sample being in the convex hull of 50% for increasing dimension \(d\). This
 shows that the necessary number of datapoints \(N\) increases exponentially with \(d\), because it’s a straight line through a logarithmic scale.</p>

<p>This dimension of the Gaussian \(d\) is called the <em>ambient dimension</em>. The ambient dimension of the data is the
number of dimensions we use to represent it. If we take the well-known MNIST dataset as an example, the ambient dimension
that is often used here is \(d = 28 \times 28 = 784\).</p>

<p><strong>How to get the estimate of being in the convex hull?</strong> <br />
We want to reproduce the above image, so how do we estimate the probability of being in the convex hull for a new sample? 
The authors of the paper used a Monte-Carlo estimate of the probability. For each of the six plots above, we can sample \(N\) points from a Gaussian with
dimension \(d\) to form a convex hull of samples: \(\text{Hull}(\mathbf{X}) = \{\mathbf{x}_1, \dots, \mathbf{x}_{N}\}\).
 Then, we sample a new point \(\mathbf{x} \sim \mathcal{N}(0, \mathbb{I}_d)\)
and determine whether it lies in the convex hull. We repeat this whole thing 500.000 times, and this gives 500.000 binary
decisions whether the sample was inside the hull or not. The average of this gives the probability that a new sample lies
in the convex hull of another set of samples from a Gaussian with dimension \(d\), or 1 point on the plot. To get the
 entire plot we need to do this for \(N\) between 1 and \(10^3 = 1000\) for every \(d\). You can imagine that this
 will take a while, so we will cheat the estimat slightly and only sample the convex hull once for every of the 500.000 trials
 we do per \(N\). Additionally, we only take \(10\) different values for \(N\) for each \(d\).</p>

<p>Below, we’ll implement everything to do this. We are going to need a few functions:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">sample_from_gaussian</code>: a function to sample from a multivariate Gaussian</li>
  <li><code class="language-plaintext highlighter-rouge">is_in_convex_hull_batch</code>: returns vector of true’s and falses that specify whether each sample in a batch of new vectors fall inside the convex hull</li>
  <li><code class="language-plaintext highlighter-rouge">probability_in_convex_hull_batch</code>: returns the estimated probability that a new sample lies in the convex hull.</li>
</ul>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">sample_from_gaussian</span><span class="p">(</span><span class="n">num_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">num_dimensions</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                         <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">:</span>
  <span class="s">"""
  Sample num_samples from a multivariate Gaussian with `num_dimensions`
  independent dimensions.

  Returns: [num_samples, num_dimensions] gaussian samples.
  """</span>
  <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_dimensions</span><span class="p">)</span> <span class="o">+</span> <span class="n">mu</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">identity</span><span class="p">(</span><span class="n">num_dimensions</span><span class="p">)</span> <span class="o">*</span> <span class="n">sigma</span>
  <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">)</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">samples</span> <span class="o">=</span> <span class="n">sample_from_gaussian</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="c1"># Plot the 3 independent dimensions of the samples.
</span><span class="n">plt</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">samples</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'g'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">samples</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'b'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">samples</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'r'</span><span class="p">)</span></code></pre></figure>

<p><img src="/images/gauss_samples.png" alt="gaussamples" width="400" class="center" /></p>

<p><strong>Probability of lying in the convex hull</strong> <br />
Recall that a vector \(\mathbf{x}\) lies within the convex hull spanned by samples \(\mathbf{x}_1, \dots, \mathbf{x}_N \in \mathbb{R}^d\) 
if we can write it as a convex combination of the samples \(\mathbf{x} = \lambda_1\mathbf{x}_1 + \dots + \lambda_N\mathbf{x}_N\)
 subject to \(\lambda_i \geq 0\) and \(\sum_i \lambda_i = 1\).</p>

<p>Now to find out whether a new point \(\mathbf{x}\) is in the convex hull we just need to find out whether it is a convex 
combination of the hull samples. This can be framed as a quadratic programming problem, and that’s what we will do for the
second plot below, but for the first plot we can use a very simple batched implementation using <a href="https://scipy.org/" target="_blank">SciPy</a>.</p>

<p>The QP problem is necessary for the second plot since the method used below does not work for degenerate convex hull 
spaces, which will be the case for the second plot (lower intrinsic dimension than ambient dimension).</p>

<p>Below we use a batched implementation of finding whether points lie in the convex hull by relying on <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.Delaunay.html">scipy.Delaunay</a>.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">is_in_convex_hull_batch</span><span class="p">(</span><span class="n">new_samples</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">,</span> 
                            <span class="n">hull_samples</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">:</span>
  <span class="s">"""
  Returns a vector of size new_samples.shape[0] (the number of new samples),
  with a boolean indicating whether or not the sample lies in the convex hull.
  """</span>
  <span class="k">assert</span> <span class="n">new_samples</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">hull_samples</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>  \
  <span class="s">"Dimensions of new sample and convex hull samples should be the same, "</span>
  <span class="s">"but are %d and %d"</span> <span class="o">%</span> <span class="p">(</span><span class="n">new_samples</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">samples</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">hull_samples</span><span class="p">,</span> <span class="n">Delaunay</span><span class="p">):</span>
    <span class="n">hull</span> <span class="o">=</span> <span class="n">Delaunay</span><span class="p">(</span><span class="n">hull_samples</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">hull</span><span class="p">.</span><span class="n">find_simplex</span><span class="p">(</span><span class="n">new_samples</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">0</span>

<span class="k">def</span> <span class="nf">probability_in_convex_hull_batch</span><span class="p">(</span><span class="n">new_samples</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">,</span> 
                                     <span class="n">hull_samples</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
  <span class="s">"""
  The first dimension is the number of new samples, the second dimension
  is the dimensionality of the vector.
  """</span>
  <span class="n">in_convex_hull</span> <span class="o">=</span> <span class="n">is_in_convex_hull_batch</span><span class="p">(</span><span class="n">new_samples</span><span class="p">,</span> <span class="n">hull_samples</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">in_convex_hull</span><span class="p">)</span> <span class="o">/</span> <span class="n">new_samples</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></code></pre></figure>

<p>Let’s see if it works for the convex hull we visualized above in \(\mathbb{R}^2\). In addition to the point inside and the point outside the hull
 above, let’s add two more points outside the hull to get a probability of being inside the hull of \(\frac{1}{4}\).</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">point_in_hull</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.3</span><span class="p">,</span> <span class="mf">1.8</span><span class="p">])</span>
<span class="n">point_outside_hull</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">point_outside_hull_2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">point_outside_hull_3</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">points</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">point_in_hull</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
                         <span class="n">np</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">point_outside_hull</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
                         <span class="n">np</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">point_outside_hull_2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
                         <span class="n">np</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">point_outside_hull_3</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Probability inside hull (batch method): "</span><span class="p">,</span> 
      <span class="n">probability_in_convex_hull_batch</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">hull_samples</span><span class="p">))</span></code></pre></figure>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">&gt;</span><span class="o">&gt;</span> Probability inside hull <span class="o">(</span>batch method<span class="o">)</span>: 0.25
</code></pre></div></div>

<p>Now we are ready to reproduce the first plot. We sample \(N\) points for an ambient dimension \(d\) from a multivariate 
Gaussian \(\mathcal{N}(\mathbf{0}, \mathbb{I}_d)\). These points form the convex hull. Then we sample 500000 new points (<code class="language-plaintext highlighter-rouge">num_trials</code>) 
from the same Gaussian, and see whether they fall within the hull, or outside, to estimate the probability of being inside the convex hull.</p>

<p>We do this for different $N$ per dimensions \(d \in \{2, \dots, 7\}\). The N below (in <code class="language-plaintext highlighter-rouge">num_convex_hull_samples_per_dim</code>) 
are chosen to roughly be the same as in the plot (I eyeballed it).</p>

<p><strong>NB</strong>: note that the code below runs reasonably quick for dimensions 2 to 6, but takes a few minutes for dimensions = 7.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># The different ambient dimensions that we will consider.
</span><span class="n">ambient_dimensions</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="c1"># The different values for N that we will consider.
</span><span class="n">num_dataset_sizes</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">num_convex_hull_samples_per_dim</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">logspace</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.8</span><span class="p">,</span> <span class="n">num_dataset_sizes</span><span class="p">),</span>
                                   <span class="n">np</span><span class="p">.</span><span class="n">logspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">num_dataset_sizes</span><span class="p">),</span>
                                   <span class="n">np</span><span class="p">.</span><span class="n">logspace</span><span class="p">(</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">2.1</span><span class="p">,</span> <span class="n">num_dataset_sizes</span><span class="p">),</span>
                                   <span class="n">np</span><span class="p">.</span><span class="n">logspace</span><span class="p">(</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.2</span><span class="p">,</span> <span class="n">num_dataset_sizes</span><span class="p">),</span>
                                   <span class="n">np</span><span class="p">.</span><span class="n">logspace</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="n">num_dataset_sizes</span><span class="p">),</span>
                                   <span class="n">np</span><span class="p">.</span><span class="n">logspace</span><span class="p">(</span><span class="mf">2.1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">num_dataset_sizes</span><span class="p">)]</span>
<span class="n">num_trials</span> <span class="o">=</span> <span class="mi">500000</span>

<span class="k">def</span> <span class="nf">get_convex_hull_probability_gaussian</span><span class="p">(</span><span class="n">num_trials</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dataset_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> 
                                         <span class="n">ambient_dimension</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
  <span class="s">"""Samples `dataset_size` samples from a Gaussian of dimension 
  `ambient_dimension`, and then calculates for `num_trials` new samples
  whether they lie on the convex hull or not. Returns the estimated
  probability that a new sample lies in the convex hull."""</span>
  <span class="n">convex_hull_samples</span> <span class="o">=</span>  <span class="n">sample_from_gaussian</span><span class="p">(</span><span class="n">num_samples</span><span class="o">=</span><span class="n">dataset_size</span><span class="p">,</span> 
                                              <span class="n">num_dimensions</span><span class="o">=</span><span class="n">ambient_dimension</span><span class="p">)</span>
  <span class="n">new_samples</span> <span class="o">=</span> <span class="n">sample_from_gaussian</span><span class="p">(</span><span class="n">num_samples</span><span class="o">=</span><span class="n">num_trials</span><span class="p">,</span> 
                                     <span class="n">num_dimensions</span><span class="o">=</span><span class="n">ambient_dimension</span><span class="p">)</span>
  <span class="n">p_new_samples_in_hull</span> <span class="o">=</span> <span class="n">probability_in_convex_hull_batch</span><span class="p">(</span>
      <span class="n">new_vectors</span><span class="o">=</span><span class="n">new_samples</span><span class="p">,</span> <span class="n">hull_samples</span><span class="o">=</span><span class="n">convex_hull_samples</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">p_new_samples_in_hull</span>

<span class="c1"># For each ambient dimension d, get the probability that a new sample lies in
# the convex hull per value for N.
</span><span class="n">all_probabilities</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">ambient_dimensions</span><span class="p">),</span>
                              <span class="n">num_dataset_sizes</span><span class="p">])</span>
<span class="k">for</span> <span class="n">dimension_idx</span><span class="p">,</span> <span class="n">dimension</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ambient_dimensions</span><span class="p">):</span>
  <span class="k">print</span><span class="p">(</span><span class="s">"Working on %d-D"</span> <span class="o">%</span> <span class="n">dimension</span><span class="p">)</span>
  <span class="n">num_convex_hull_samples</span> <span class="o">=</span> <span class="n">num_convex_hull_samples_per_dim</span><span class="p">[</span><span class="n">dimension_idx</span><span class="p">]</span>
  <span class="k">for</span> <span class="n">size_idx</span><span class="p">,</span> <span class="n">dataset_size</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">num_convex_hull_samples</span><span class="p">):</span>
    <span class="n">probability_in_hull</span> <span class="o">=</span> <span class="n">get_convex_hull_probability_gaussian</span><span class="p">(</span>
        <span class="n">num_trials</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">dataset_size</span><span class="p">),</span> <span class="n">dimension</span><span class="p">)</span>
    <span class="n">all_probabilities</span><span class="p">[</span><span class="n">dimension_idx</span><span class="p">,</span> <span class="n">size_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">probability_in_hull</span></code></pre></figure>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">&gt;</span><span class="o">&gt;</span> Working on 2-D
<span class="gp">&gt;</span><span class="o">&gt;</span> Working on 3-D
<span class="gp">&gt;</span><span class="o">&gt;</span> Working on 4-D
<span class="gp">&gt;</span><span class="o">&gt;</span> Working on 5-D
<span class="gp">&gt;</span><span class="o">&gt;</span> Working on 6-D
<span class="gp">&gt;</span><span class="o">&gt;</span> Working on 7-D
</code></pre></div></div>

<details onclose="">
<summary>Below the function plot_probabilities() to plot this (very uninteresting).</summary>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">plot_probabilities</span><span class="p">(</span><span class="n">x_space_per_dimension</span><span class="p">,</span> <span class="n">dimensions</span><span class="p">,</span> <span class="n">probabilities</span><span class="p">):</span>
  <span class="s">"""
  x_space_per_dimension: [len(dimensions)] an np.logspace per dimension
  dimensions: the different dimensions to be plotted on the Y-axis
  probabilities: [num_dimensions, num_dataset_sizes] convex hull probabilities
  """</span>
  <span class="n">num_dimensions</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dimensions</span><span class="p">)</span>
  <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>

  <span class="c1"># All stacked plots equally high.
</span>  <span class="n">gridspecs</span> <span class="o">=</span> <span class="n">gridspec</span><span class="p">.</span><span class="n">GridSpec</span><span class="p">(</span><span class="n">num_dimensions</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> 
                                <span class="n">height_ratios</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_dimensions</span><span class="p">)</span> 

  <span class="c1"># Loop over the grids and plot the probabilities for a dimension in each.
</span>  <span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s">"b"</span><span class="p">,</span> <span class="s">"orange"</span><span class="p">,</span> <span class="s">"g"</span><span class="p">,</span> <span class="s">"r"</span><span class="p">,</span> <span class="s">"purple"</span><span class="p">,</span> <span class="s">"brown"</span><span class="p">]</span>
  <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">gridspecs</span><span class="p">))):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x_space_per_dimension</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">ax</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_xscale</span><span class="p">(</span><span class="s">"log"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>  <span class="c1"># Hardcode the limits for each subplots here.
</span>    <span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">probabilities</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Only add the Y tick for p=1 at the topmost subplot.
</span>    <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">dimensions</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">dimensions</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
      <span class="n">ax</span><span class="p">.</span><span class="n">set_yticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="n">minor</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">ax</span><span class="p">.</span><span class="n">set_yticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">minor</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    
    <span class="c1"># Only add the X ticks for the bottom subplot.
</span>    <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
      <span class="n">ax</span><span class="p">.</span><span class="n">set_xticks</span><span class="p">([])</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'log(N)'</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">12</span><span class="p">)</span>
      <span class="n">ax</span><span class="p">.</span><span class="n">xaxis</span><span class="p">.</span><span class="n">set_label_coords</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.655</span><span class="p">)</span>

    <span class="c1"># Some annotations
</span>    <span class="n">plt</span><span class="p">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'black'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'--'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">text</span><span class="p">(</span><span class="mi">10</span><span class="o">**</span><span class="mi">3</span><span class="o">+</span><span class="mi">500</span><span class="p">,</span><span class="mf">0.5</span><span class="p">,</span><span class="s">'d=%d'</span> <span class="o">%</span> <span class="n">dimensions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="p">,</span><span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

  <span class="c1"># remove vertical gap between subplots
</span>  <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"p(x in Hull)"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="p">.</span><span class="mi">0</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span></code></pre></figure>

</details>

<p><img src="/images/left_plot.png" alt="leftplot" width="800" class="center" /></p>

<p>Jippie<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote">1</a></sup>! There it is! So beautiful. Very reproduce. Much similar.</p>

<h2 id="reproducing-the-second-plot"><span style="color:#C0392B">Reproducing the Second Plot</span></h2>
<blockquote>
  <p><strong><em>In this section:</em></strong>  Intrinsic dimension, Data Manifold, Quadratic Programming</p>
</blockquote>

<p><img src="/images/second_plot.png" alt="secondplot" width="300" class="center" /></p>

<p>The second plot in Figure 1 looks incredibly similar to the first one. The difference lies in the underlying data manifold
that is sampled from, i.e. the <em>intrinsic dimension</em> will be different. For the first plot, we sampled from a \(d\)-dimensional Gaussian. 
The intrinsic dimension of this Gaussian was the same as the dimension we used to represent it: \(d = \bar{d}\).
 For this plot, we will sample from a \(1\)-dimensional nonlinear continuous manifold (\(\bar{d} = 1\)), but represent that data in a higher
 dimension (\(d = \{2, \dots, 7\}\)). The point of the plot is to show that even for data with a 1-dimensional manifold, if the ambient dimension
 goes up, you will still need exponentially more data to stay in interpolation regime with a high probability.</p>

<p>So how are we going to get data from a nonlinear continuous 1-dimensional data manifold and represent it in a higher dimension \(d\)?
We are going to take the following steps.</p>

<ul>
  <li>Sample \(d\) independent Gaussian points.</li>
  <li>Interpolate between these points with a spline to get a 1D data manifold.</li>
  <li>Sample from a uniform distribution between 0 and 1 and put these through the spline to get samples.</li>
</ul>

<p>These samples are now taken from a spline \(f: \mathbb{R} \rightarrow \mathbb{R}^{d}\), meaning the ambient dimension will be \(d\),
 whereas the intrinsic dimension is 1.</p>

<p><strong>Why would this work?</strong></p>

<p>This works because the chances of sampling a point from a Gaussian that’s overlapping with a previous point is zero,
so with every point that we sample we increase the dimensionality the spline moves through. Let’s implement it below.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">construct_one_dimensional_manifold</span><span class="p">(</span><span class="n">ambient_dimension</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
  <span class="s">"""
  ambient_dimension: what dimension should the data be represented in

  Returns: The spline function f: R -&gt; R^ambient_dimension.
  """</span>
  <span class="c1"># We take dimensions+2 for the dimension of the Gaussian because otherwise 
</span>  <span class="c1"># the scipy interp1d function doesn't work.
</span>  <span class="n">gaussian_points</span> <span class="o">=</span> <span class="n">sample_from_gaussian</span><span class="p">(</span><span class="n">num_samples</span><span class="o">=</span><span class="n">ambient_dimension</span><span class="p">,</span> 
                                         <span class="n">num_dimensions</span><span class="o">=</span><span class="n">ambient_dimension</span><span class="o">+</span><span class="mi">2</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="n">ambient_dimension</span><span class="o">+</span><span class="mi">2</span><span class="p">,</span> <span class="n">endpoint</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
  <span class="n">f</span> <span class="o">=</span> <span class="n">interp1d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">gaussian_points</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s">'cubic'</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">f</span>

<span class="k">def</span> <span class="nf">sample_from_one_dimensional_manifold</span><span class="p">(</span><span class="n">manifold_func</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">manifold_func</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">num_samples</span><span class="p">))</span></code></pre></figure>

<p>Now we can sample from 1-dimensional manifolds and represent it in any ambient dimension. We can’t visualize dimensions
above 3, but we can show what the manifold looks like in 2D. Below let’s sample from the manifold, and represent it in
2 and 10 dimensions.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">spline_2d</span> <span class="o">=</span> <span class="n">construct_one_dimensional_manifold</span><span class="p">(</span><span class="n">ambient_dimension</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">spline_10d</span> <span class="o">=</span> <span class="n">construct_one_dimensional_manifold</span><span class="p">(</span><span class="n">ambient_dimension</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="c1"># Plot some samples from this manifold for d=2 and d=10.
</span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">endpoint</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">spline_2d</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:],</span> <span class="n">spline_2d</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">1</span><span class="p">,</span> <span class="p">:])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">spline_10d</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:],</span> <span class="n">spline_10d</span><span class="p">(</span><span class="n">x</span><span class="p">)[</span><span class="mi">1</span><span class="p">,</span> <span class="p">:])</span></code></pre></figure>

<p><img src="/images/1d_manifold.png" alt="1d_manifold" width="800" class="center" /></p>

<p>In the above image one can see that the spline represented in 10 dimensions on the right is much more wobbly than the one
that has ambient dimension 2; both have intrinsic dimension 1.</p>

<h1 id="does-it-lie-in-the-convex-hull-a-qp-problem"><span style="color:#C0392B">Does it lie in the convex hull? A QP problem</span></h1>

<p>Unfortunately, the batched method for finding out whether a new point lies inside the convex hull doesn’t work anymore. 
This is because the fact that our samples all come from a 1D manifold causes geometric degeneracy. From the SciPy docs:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">QhullError 
Raised when Qhull encounters an error condition, such as geometrical degeneracy 
when options to resolve are not enabled.
</span></code></pre></div></div>

<p>Instead, we will frame it as a quadratic programming problem and solve it with a QP solver that can handle degeneracy.</p>

<p>We want to find the coefficients such that \(\boldsymbol{\lambda}_1 \mathbf{x}_1 + \dots + \boldsymbol{\lambda}_N \mathbf{x}_N = \mathbf{x}\). 
The constraints on the \(\boldsymbol{\lambda}\)’s also need to be taken care of. If we can find coefficients that satisfy these constraints, 
the new point \(\mathbf{x}\) lies in the convex hull.</p>

<p>Recall the following definitions:</p>
<ul>
  <li>\(N\) the number of convex hull samples</li>
  <li>\(d\) the ambient dimension</li>
  <li>\(\boldsymbol{\lambda} \in \mathbb{R}^{N}\) the convex combination coefficients</li>
  <li>the convex hull samples \(\mathbf{X} \in \mathbb{R}^{N \times d}\)</li>
  <li>the new sample \(\mathbf{x} \in \mathbb{R}^d\)</li>
</ul>

<p>Our quadratic program problem is the following:</p>

\[\begin{align}
\text{min}_{\boldsymbol{\lambda}} &amp;\left(\mathbf{X}^T\boldsymbol{\lambda} - \mathbf{x}\right)^T \left(\mathbf{X}^T\boldsymbol{\lambda} - \mathbf{x}\right) \\
s.t. &amp; \sum_i \lambda_i = 1 \\
&amp; \lambda_i \geq 0 
\end{align}\]

<p>If we solve this QP problem, and \(\left(\mathbf{X}^T\boldsymbol{\lambda} - \mathbf{x}\right)^T \left(\mathbf{X}^T\boldsymbol{\lambda} - \mathbf{x}\right) = 0\),
that means we have \(\mathbf{x} = \mathbf{X}^T\boldsymbol{\lambda}\), and our new sample is a convex combination of the
samples, i.e. lies in the convex hull.</p>

<p>We can rewrite the objective to the <a href="https://scaron.info/blog/quadratic-programming-in-python.html">standard form</a> as follows:</p>

\[\begin{align}
\text{min}_{\boldsymbol{\lambda}} &amp;\frac{1}{2}\boldsymbol{\lambda}^T(\mathbf{X}\mathbf{X}^T)\boldsymbol{\lambda} + (-\mathbf{X}\mathbf{y})^T\boldsymbol{\lambda}
\end{align}\]

<p>This follows from the fact that multiplying by a constant doesn’t change the objective, and neither does a constant addition.
For the QP library that we are going to use, we need to rewrite the inequality constraints into the form \(G\boldsymbol{\lambda} \leq h\).
To this end we define the matrix \(G\) as follows:</p>

\[G = \begin{pmatrix}
-1 &amp; 0 &amp; 0 &amp; 0 \\ 
0 &amp; -1 &amp; 0 &amp; 0\\ 
0 &amp; 0 &amp; -1 &amp; 0\\ 
0 &amp; 0 &amp; 0 &amp; -1
\end{pmatrix}\]

<p>and the vector \(h\):</p>

\[h = \begin{pmatrix}
0\\ 
0\\ 
0\\ 0
\end{pmatrix}\]

<p>The equality constraint needs to be written into the form \(A\lambda = b\):</p>

\[A = \begin{pmatrix}
1 &amp; 1 &amp; 1 &amp; 1 \\ 
\end{pmatrix}\]

<p>and the vector \(b\):</p>

\[b = \begin{pmatrix}
1
\end{pmatrix}\]

<p>This ensures that the sum of the \(\lambda_i\) equals 1 and all entries are above non-negative.</p>

<p>Below we implement this, and we assume a new vector is in the complex hull if the QP problem has a ‘loss’ smaller than 1e-5 
(meaning \(\left(\mathbf{X}^T\boldsymbol{\lambda} - \mathbf{x}\right)^T \left(\mathbf{X}^T\boldsymbol{\lambda} - \mathbf{x}\right) \leq 1e-5\)).</p>

<p>Note the definitions of the following variables in the code below to keep it the same as in <a href="https://scaron.info/blog/quadratic-programming-in-python.html">the example</a> that I used.</p>

\[\mathbf{P} := \mathbf{X}\mathbf{X}^T\]

\[\mathbf{q} := -\mathbf{X}\mathbf{y}\]

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># You can ignore this function; it's a wrapper around the QP solver.
</span><span class="k">def</span> <span class="nf">cvxopt_solve_qp</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">G</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">A</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
  <span class="s">"Copied from https://scaron.info/blog/quadratic-programming-in-python.html"</span>
  <span class="n">P</span> <span class="o">=</span> <span class="p">.</span><span class="mi">5</span> <span class="o">*</span> <span class="p">(</span><span class="n">P</span> <span class="o">+</span> <span class="n">P</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>  <span class="c1"># make sure P is symmetric
</span>  <span class="n">args</span> <span class="o">=</span> <span class="p">[</span><span class="n">cvxopt</span><span class="p">.</span><span class="n">matrix</span><span class="p">(</span><span class="n">P</span><span class="p">),</span> <span class="n">cvxopt</span><span class="p">.</span><span class="n">matrix</span><span class="p">(</span><span class="n">q</span><span class="p">)]</span>
  <span class="k">if</span> <span class="n">G</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
      <span class="n">args</span><span class="p">.</span><span class="n">extend</span><span class="p">([</span><span class="n">cvxopt</span><span class="p">.</span><span class="n">matrix</span><span class="p">(</span><span class="n">G</span><span class="p">),</span> <span class="n">cvxopt</span><span class="p">.</span><span class="n">matrix</span><span class="p">(</span><span class="n">h</span><span class="p">)])</span>
      <span class="k">if</span> <span class="n">A</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
          <span class="n">args</span><span class="p">.</span><span class="n">extend</span><span class="p">([</span><span class="n">cvxopt</span><span class="p">.</span><span class="n">matrix</span><span class="p">(</span><span class="n">A</span><span class="p">),</span> <span class="n">cvxopt</span><span class="p">.</span><span class="n">matrix</span><span class="p">(</span><span class="n">b</span><span class="p">)])</span>
  <span class="n">sol</span> <span class="o">=</span> <span class="n">cvxopt</span><span class="p">.</span><span class="n">solvers</span><span class="p">.</span><span class="n">qp</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
  <span class="k">if</span> <span class="s">'optimal'</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">sol</span><span class="p">[</span><span class="s">'status'</span><span class="p">]:</span>  <span class="c1"># Failed
</span>      <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">10</span><span class="p">]</span> <span class="o">*</span> <span class="n">P</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]).</span><span class="n">reshape</span><span class="p">((</span><span class="n">P</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],))</span>
  <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">sol</span><span class="p">[</span><span class="s">'x'</span><span class="p">]).</span><span class="n">reshape</span><span class="p">((</span><span class="n">P</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],))</span>
  
<span class="k">def</span> <span class="nf">is_in_convex_hull_qp</span><span class="p">(</span><span class="n">new_sample</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">,</span> <span class="n">hull_samples</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
  <span class="s">"""
  :param new_sample: [num_dimensions]
  :param hull_samples: [num_hull_samples, num_dimensions]

  Calcs for `new_sample` whether it lies in the convex hull of `hull_samples`.
  """</span>
  <span class="n">num_hull_samples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">hull_samples</span><span class="p">)</span>
  <span class="n">num_dimensions</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">new_sample</span><span class="p">)</span>

  <span class="c1"># Define the QP problem.
</span>  <span class="n">P</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">hull_samples</span><span class="p">,</span> <span class="n">hull_samples</span><span class="p">.</span><span class="n">T</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">double</span><span class="p">)</span>
  <span class="n">q</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">hull_samples</span><span class="p">,</span> <span class="n">new_vector</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">double</span><span class="p">)</span>

  <span class="n">G</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="n">identity</span><span class="p">(</span><span class="n">num_hull_samples</span><span class="p">)</span>
  <span class="n">h</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_hull_samples</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">double</span><span class="p">)</span>

  <span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_hull_samples</span><span class="p">]).</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">double</span><span class="p">)</span>
  <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">]).</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">double</span><span class="p">)</span>

  <span class="n">result</span> <span class="o">=</span> <span class="n">cvxopt_solve_qp</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">G</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">l_i</span><span class="p">,</span> <span class="n">sign</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">new_sample</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="n">hull_samples</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">sign</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">l_i</span><span class="p">)).</span><span class="n">T</span><span class="p">,</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="n">l_i</span><span class="p">)))</span>

  <span class="n">l</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">l</span> <span class="o">&lt;</span> <span class="mf">1e-5</span><span class="p">:</span>
    <span class="k">return</span> <span class="bp">True</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="k">return</span> <span class="bp">False</span>

<span class="k">def</span> <span class="nf">probability_in_convex_hull_qp</span><span class="p">(</span><span class="n">new_samples</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">,</span> 
                                  <span class="n">hull_samples</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
  <span class="s">"""
  The first dimension is the number of new samples, the second dimension
  is the dimensionality of the sample. Returns the estimated probability
  that a new sample lies in the convex hull of `hull_samples`.
  """</span>
  <span class="n">in_convex_hull</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">new_samples</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]])</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">new_samples</span><span class="p">)):</span>
    <span class="n">new_sample</span> <span class="o">=</span> <span class="n">new_samples</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">vec_in_convex_hull</span> <span class="o">=</span> <span class="n">is_in_convex_hull_qp</span><span class="p">(</span><span class="n">new_sample</span><span class="p">,</span> <span class="n">hull_samples</span><span class="p">)</span>
    <span class="n">in_convex_hull</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">vec_in_convex_hull</span>
  <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">in_convex_hull</span><span class="p">)</span> <span class="o">/</span> <span class="n">new_samples</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></code></pre></figure>

<p>Let’s test it again for the example points and convex hull that we also used above (probability should be \(\frac{1}{4}\)).</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">print</span><span class="p">(</span><span class="s">"Probability inside hull (quadratic programming method): "</span><span class="p">,</span> 
      <span class="n">probability_in_convex_hull_qp</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">hull_samples</span><span class="p">))</span></code></pre></figure>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">&gt;</span><span class="o">&gt;</span> Probability inside hull <span class="o">(</span>quadratic programming method<span class="o">)</span>: 0.25
</code></pre></div></div>

<p>We’re ready to reproduce the figure. We take the following steps to do so:</p>

<ol>
  <li>Sample a spline for a dimension \(d = \{2, \dots, 7\}\).</li>
  <li>Get \(N\) convex hull samples from this same spline.</li>
  <li>Get a new sample from the spline</li>
  <li>See whether it’s in the convex hull of the \(N\) samples by solving the quadratic programming problem.</li>
  <li>Repeat <code class="language-plaintext highlighter-rouge">num_trials</code> times.</li>
</ol>

<p><strong>CAVEAT</strong>: The code below is <em>incredibly</em> slow, because of the new method of finding whether a point is in the convex hull used; 
framing it as a QP problem. The paper does 500000 trials, but below we do just 100, to be a bit faster (runs in a few minutes).
 It still works, but obviously the estimate is much worse.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">ambient_dimensions</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">num_dataset_sizes</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">num_convex_hull_samples_per_d</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">logspace</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.8</span><span class="p">,</span> <span class="n">num_dataset_sizes</span><span class="p">),</span>
                                 <span class="n">np</span><span class="p">.</span><span class="n">logspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">num_dataset_sizes</span><span class="p">),</span>
                                 <span class="n">np</span><span class="p">.</span><span class="n">logspace</span><span class="p">(</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">2.1</span><span class="p">,</span> <span class="n">num_dataset_sizes</span><span class="p">),</span>
                                 <span class="n">np</span><span class="p">.</span><span class="n">logspace</span><span class="p">(</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.2</span><span class="p">,</span> <span class="n">num_dataset_sizes</span><span class="p">),</span>
                                 <span class="n">np</span><span class="p">.</span><span class="n">logspace</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="n">num_dataset_sizes</span><span class="p">),</span>
                                 <span class="n">np</span><span class="p">.</span><span class="n">logspace</span><span class="p">(</span><span class="mf">2.1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">num_dataset_sizes</span><span class="p">)]</span>

<span class="n">num_trials</span> <span class="o">=</span> <span class="mi">100</span>

<span class="k">def</span> <span class="nf">get_convex_hull_probability</span><span class="p">(</span><span class="n">manifold_spline</span><span class="p">,</span> <span class="n">num_trials</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> 
                                <span class="n">dataset_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> 
                                <span class="n">ambient_dimension</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
  <span class="c1"># Get the samples to form the convex hull.
</span>  <span class="n">hull_samples</span> <span class="o">=</span> <span class="n">manifold_spline</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                                   <span class="n">size</span><span class="o">=</span><span class="n">dataset_size</span><span class="p">))</span>

  <span class="c1"># Get new samples from the manifold.                          
</span>  <span class="n">new_samples</span> <span class="o">=</span> <span class="n">manifold_spline</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                                  <span class="n">size</span><span class="o">=</span><span class="n">num_trials</span><span class="p">))</span>

  <span class="c1"># Estimate the probability that a new sample lies in the convex hull.
</span>  <span class="n">p_new_samples_in_hull</span> <span class="o">=</span> <span class="n">probability_in_convex_hull_qp</span><span class="p">(</span>
      <span class="n">new_samples</span><span class="o">=</span><span class="n">new_samples</span><span class="p">.</span><span class="n">transpose</span><span class="p">(),</span>
      <span class="n">hull_samples</span><span class="o">=</span><span class="n">hull_samples</span><span class="p">.</span><span class="n">transpose</span><span class="p">())</span>
  <span class="k">return</span> <span class="n">p_new_samples_in_hull</span>

<span class="c1"># Loop over all ambient dimensions.
</span><span class="n">all_probabilities_2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">ambient_dimensions</span><span class="p">),</span>
                                <span class="n">num_dataset_sizes</span><span class="p">])</span>
<span class="k">for</span> <span class="n">dimension_idx</span><span class="p">,</span> <span class="n">dimension</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ambient_dimensions</span><span class="p">):</span>
  <span class="k">print</span><span class="p">(</span><span class="s">"Working on %d-D"</span> <span class="o">%</span> <span class="n">dimension</span><span class="p">)</span>
  <span class="n">dataset_sizes</span> <span class="o">=</span> <span class="n">num_convex_hull_samples_per_d</span><span class="p">[</span><span class="n">dimension_idx</span><span class="p">]</span>

  <span class="c1"># Sample a new spline for this ambient dimension.
</span>  <span class="n">spline</span> <span class="o">=</span> <span class="n">construct_one_dimensional_manifold</span><span class="p">(</span><span class="n">ambient_dimension</span><span class="o">=</span><span class="n">dimension</span><span class="p">)</span>

  <span class="c1"># Loop over the different dataset sizes.
</span>  <span class="k">for</span> <span class="n">size_idx</span><span class="p">,</span> <span class="n">dataset_size</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataset_sizes</span><span class="p">):</span>

    <span class="c1"># Estimate the probability
</span>    <span class="n">probability_in_hull</span> <span class="o">=</span> <span class="n">get_convex_hull_probability</span><span class="p">(</span><span class="n">spline</span><span class="p">,</span> <span class="n">num_trials</span><span class="p">,</span>
                                                      <span class="nb">int</span><span class="p">(</span><span class="n">dataset_size</span><span class="p">),</span>
                                                      <span class="n">dimension</span><span class="p">)</span>
    <span class="n">all_probabilities_2</span><span class="p">[</span><span class="n">dimension_idx</span><span class="p">,</span> <span class="n">size_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">probability_in_hull</span></code></pre></figure>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">&gt;</span><span class="o">&gt;</span> Working on 2-D
<span class="gp">&gt;</span><span class="o">&gt;</span> Working on 3-D
<span class="gp">&gt;</span><span class="o">&gt;</span> Working on 4-D
<span class="gp">&gt;</span><span class="o">&gt;</span> Working on 5-D
<span class="gp">&gt;</span><span class="o">&gt;</span> Working on 6-D
<span class="gp">&gt;</span><span class="o">&gt;</span> Working on 7-D
</code></pre></div></div>

<p><img src="/images/middle_plot.png" alt="middleplot" width="800" class="center" /></p>

<p>We can see that even though we cheated by only sampling the data points once per ambient dimension, and
only doing 100 Monte-Carlo trials instead of 500000, we get away with it!</p>

<h2 id="reproducing-the-final-plot"><span style="color:#C0392B">Reproducing the Final Plot</span></h2>
<blockquote>
  <p><strong><em>In this section:</em></strong>  Convex Hull Dimension</p>
</blockquote>

<p><img src="/images/third_plot.png" alt="thirdplot" width="300" class="center" /></p>

<p>The final plot requires the introduction of some new terminology; it’s the plot that answers the question: 
“so how can we control the probability of interpolating, i.e. of a new sample lying in the convex hull”? 
The answer is (somewhat unsurprisingly now); by keeping the <em>dimensions of the convex
hull</em> the same. The convex hull dimension, also referred to as the dimension of the lowest dimensional affine
subspace including the entire data manifold in the paper, is the dimension of the convex hull of the data points.
Recall the discussion of what the convex hull is above? Below on the left we take two of those datapoints, that happen
to lie on a line in \(\mathbb{R}^3\), so the ambient dimension is 3, but the convex hull dimension is 1; it’s a line.
If we take 4 points that lie on a plane, like in the middle below, we have ambient dimension of 3 and convex hull
dimension of 2. Finally, if we take the full set of 8 data points, we get a convex hull dimension of 3, which equals
 the ambient dimension. This convex hull dimension is denoted by \(d^*\) in the paper.</p>

<p><img src="/images/convexhull_ds.png" alt="convexhull_ds" width="800" class="center" /></p>

<p>Now what we do to reproduce the final plot, is show that if we keep the covex hull dimension \(d^*\) similar,
but increase the ambient dimension, it doesn’t have the effect it had before anymore! We don’t need more datapoints \(N\)
to keep the probability of a new sample lying in the convex hull at 50% for a higher ambient dimension.</p>

<h1 id="disclaimers"><span style="color:#C0392B">Disclaimers</span></h1>

<p>…</p>

<h1 id="acknowledgements"><span style="color:#2874A6">Acknowledgements</span></h1>

<p>If you want to be a computer scientist like me and still be able to understand concepts from statistics,
take the following steps:</p>

<ol>
  <li>Meet a cute boy at a houseparty</li>
  <li>Find out he’s doing a PhD in maths</li>
  <li>Motivate said cute boy / maths PhD to become your boyfriend</li>
  <li>Quarantine together during a global pandemic</li>
  <li>Pester him with questions about convex hulls</li>
  <li>???</li>
  <li>Profit</li>
</ol>

<h1 id="sources"><span style="color:#2874A6">Sources</span></h1>

<p>Randall Balestriero and Jerome Pesenti and Yann LeCun (2021).
    <a href="https://arxiv.org/pdf/2110.09485.pdf" target="_blank"><em>Learning in High Dimension Always Amounts to Extrapolation</em></a></p>

<p>Barany, I. and Furedi, Z. (1988). 
<a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.726.8687&amp;rep=rep1&amp;type=pdf" target="_blank"><em>On the shape of the convex hull of random points.</em></a></p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Jippie means “Yippie” in Dutch. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>
:ET