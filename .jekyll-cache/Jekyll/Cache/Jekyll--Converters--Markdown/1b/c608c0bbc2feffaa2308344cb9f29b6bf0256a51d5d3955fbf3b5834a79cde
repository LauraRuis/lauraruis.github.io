I"ÛÎ<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<p><img src="/images/figure1.png" alt="figure1" /></p>

<p>This post is an attempt at shining some light on the above image, taken from 
<a href="https://arxiv.org/pdf/2110.09485.pdf" target="_blank">Balestriero, Pesenti, and LeCun, 2021.</a> The goal is to reproduce it, and, in the process of doing
that, getting a better understanding of what this whole interpolation/extrapolation debate 
(<a href="https://twitter.com/ylecun/status/1409940043951742981" target="_blank">here</a> and
<a href="https://twitter.com/GaryMarcus/status/1411401507610796032" target="_blank">here</a> and
<a href="https://twitter.com/fchollet/status/1450524400227287040" target="_blank">here</a>) is about. I mean, I have to be honest.
The whole debate is not going to be any clearer after reading this post. David Hume describes quite nicely what is probably 
going on in this discussion in his <em>‚Äúan enquiry concerning human understanding‚Äù</em>:</p>

<blockquote>
  <p>It might reasonably be expected, in questions, which have been canvassed and disputed with great eagerness, since the 
first origin of science and philosophy, that the meaning of all terms, at least, should have been agreed upon among the
disputants; and our enquiries, in the course of two thousand years, have been able to pass from words to the true and 
real subject of the controversy. For how easy may it seem to give exact definitions of the terms employed in reasoning,
and make these definitions, not the mere sounds of words, the object of future scrutiny and examination? But if we consider
the matter more narrowly, we shall be apt to draw a quite opposite conclusion. From this circumstance alone, that a 
controversy has been long kept on foot, and remains still undecided, we may presume, that there is some ambiguity in
the expression, and that the disputants affix different ideas to the terms employed in the controversy.</p>
</blockquote>

<p>Basically; it seems like a lot of arguments between people are a result of not properly defining what we are talking about (and the
last author on this paper <a href="https://twitter.com/ylecun/status/1450809828268548101" target="_blank">probably agrees</a>). 
Didn‚Äôt this quote make me look really smart just now? Well think again, I never read Hume, and the 
reason I‚Äôm writing this blogpost is because I didn‚Äôt understand the learning in high dimensions
paper at all on first reading. With this post I hope to get a better understanding of it, and share that understanding.</p>

<p>After this post, we will hopefully know more about the following terms:</p>

<ul>
  <li>The curse of dimensionality</li>
  <li>Convex hull</li>
  <li>Ambient dimension</li>
  <li>Intrinsic dimension / Data manifold dimension</li>
  <li>Interpolation / Extrapolation</li>
</ul>

<h2 id="the-key-idea"><span style="color:#C0392B">The Key Idea</span></h2>
<p>The key idea in this paper is that we shouldn‚Äôt be using interpolation and extrapolation in the way they are defined in
 this paper as indicators
of generalization performance, because models are almost surely extrapolating. It doesn‚Äôt matter what the intrinsic dimension
of the data manifold is, which may be much lower than the dimensionality of our data representation. The authors hope that
the paper opens the door to better suited geometrical definitions of interpolation and extrapolation that align
with generalization performances in the context of high dimensional data. So let‚Äôs dive in!</p>

<h1 id="when-are-we-interpolating"><span style="color:#C0392B">When are we interpolating?</span></h1>
<blockquote>
  <p><strong><em>In this section:</em></strong>  Interpolation, Extrapolation, Convex Hull, Convex Combination, Affine Combination, Curse of Dimensionality</p>
</blockquote>

<p>The definition of interpolation in the paper is the following:</p>

<p><strong>Definition 1</strong>. Interpolation occurs for a sample \(\mathbf{x}\) whenever this sample belongs to the convex hull of a 
set of samples \(\mathbf{X} \triangleq \{\mathbf{x}_1, \dots, \mathbf{x}_N\}\), if not, extrapolation occurs.</p>

<p>So what is the <em>convex hull</em> of a set of samples?</p>

<p>A vector \(\mathbf{x}\) lies within the convex hull of a set of samples
\(\mathbf{x}_1, \dots, \mathbf{x}_N\) if we can write it as a convex combination of the samples:</p>

\[\mathbf{x} = \lambda_1 \mathbf{x}_1 + \dots + \lambda_N \mathbf{x}_N\]

<p>Subject to the constraints that the \(\lambda_i\)‚Äôs are nonnegative and sum to one: \(\lambda_i \geq 0\) and \(\sum_i \lambda_i = 1\).
Let‚Äôs have a look at what that means in a dimension we can still visualize.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Make up some samples that happen to form a nice square in R2.
</span><span class="n">hull_samples</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                         <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
                         <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
                         <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
                         
<span class="c1"># Here's a point that is a convex combination of the hull samples ..
</span><span class="n">point_in_hull</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">]])</span>

<span class="c1"># .. and here's one that isn't.
</span><span class="n">point_outside_hull</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">2.5</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>

<span class="c1"># Calculate the convex hull with scipy.spatial.ConvexHull
</span><span class="n">hull</span> <span class="o">=</span> <span class="n">ConvexHull</span><span class="p">(</span><span class="n">hull_samples</span><span class="p">)</span>

<span class="c1"># Let's plot it.
</span><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">hull_samples</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">hull_samples</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s">'o'</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s">'navy'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">point_in_hull</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">point_in_hull</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s">'o'</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s">'lightgreen'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">point_outside_hull</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">point_outside_hull</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="s">'o'</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s">'tomato'</span><span class="p">)</span>
<span class="k">for</span> <span class="n">simplex</span> <span class="ow">in</span> <span class="n">hull</span><span class="p">.</span><span class="n">simplices</span><span class="p">:</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">hull_samples</span><span class="p">[</span><span class="n">simplex</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">hull_samples</span><span class="p">[</span><span class="n">simplex</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="s">'k'</span><span class="p">)</span></code></pre></figure>

<p><img src="/images/2d_convexhull.png" alt="2d_convexhull" width="400" class="center" /></p>

<p>Everything within (or on) this square is a convex combination of the convex hull of the four samples, everything outside
it but still in \(\mathbb{R}^2\) is an <em>affine combination</em> of the samples, relaxing the constraints on the \(\lambda_i\)‚Äôs. Note that we can easily
calculate the probability of lying within the convex hull of samples here if we assume that all values will lie between zero
and three. Let‚Äôs say we sample points uniformly between zero and three for both dimensions, then the probability of a new 
sample to lie within the convex hull from this subspace is the area of the convex hull divided by the total area:</p>

\[p(\mathbf{x} \in \text{Hull}(\mathbf{X})) = \frac{1}{9}\]

<p>In general, for a convex hull with area \(c\) in \(\mathbb{R}^2\) and data points between \([x_0, x_1]\) the probability of 
lying in the convex hull is \(\frac{c}{(x_1 - x_0)^2}\).</p>

<p>Now let‚Äôs see what happens if we move to three dimensions. We‚Äôll stretch out the square from above into a cube and
calculate the probability of lying within this cube.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Make the cube of 3D hull points (8 points, each is a corner of the cube).
</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>

<span class="c1"># Set the same limits as the 2D example.
</span><span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">ax</span><span class="p">.</span><span class="n">set_zlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="c1"># A function to get a convex combination of a set of points.
</span><span class="k">def</span> <span class="nf">convex_combination</span><span class="p">(</span><span class="n">points</span><span class="p">):</span>
  <span class="n">lambdas</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">points</span><span class="p">))</span>
  <span class="n">lambdas_n</span> <span class="o">=</span> <span class="n">lambdas</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">lambdas</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">points</span><span class="p">.</span><span class="n">transpose</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]),</span> <span class="n">lambdas_n</span><span class="p">)</span>

<span class="c1"># A function to get an affine combination of a set of points 
# that is *not* a convex combination.
</span><span class="k">def</span> <span class="nf">affine_combination</span><span class="p">(</span><span class="n">points</span><span class="p">):</span>
  <span class="n">lambdas</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">points</span><span class="p">))</span>
  <span class="n">lambdas_n</span> <span class="o">=</span> <span class="n">lambdas</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">lambdas</span><span class="p">)</span>
  <span class="n">offset</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.02</span><span class="p">),</span>
                             <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">(</span><span class="mf">0.02</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)])</span>
  <span class="n">lambdas_n</span> <span class="o">=</span> <span class="n">lambdas_n</span> <span class="o">+</span> <span class="n">offset</span>
  <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">points</span><span class="p">.</span><span class="n">transpose</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]),</span> <span class="n">lambdas_n</span><span class="p">)</span>

<span class="c1"># Plot some convex combinations of the hull data points.
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
  <span class="n">num_points</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
  <span class="n">point_indices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="n">num_points</span><span class="p">)</span>
  <span class="n">points</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">X</span><span class="p">[</span><span class="n">point_indices</span><span class="p">],</span> 
                     <span class="n">Y</span><span class="p">[</span><span class="n">point_indices</span><span class="p">],</span> 
                     <span class="n">Z</span><span class="p">[</span><span class="n">point_indices</span><span class="p">]]).</span><span class="n">transpose</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
  <span class="n">new_point</span> <span class="o">=</span> <span class="n">convex_combination</span><span class="p">(</span><span class="n">points</span><span class="p">)</span>
  <span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">*</span><span class="n">new_point</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s">'lightgreen'</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mf">2.5</span><span class="p">)</span>

<span class="c1"># Plot some affine combinations of the hull data points.
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
  <span class="n">num_points</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
  <span class="n">point_indices</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="n">num_points</span><span class="p">)</span>
  <span class="n">points</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">X</span><span class="p">[</span><span class="n">point_indices</span><span class="p">],</span> 
                     <span class="n">Y</span><span class="p">[</span><span class="n">point_indices</span><span class="p">],</span> 
                     <span class="n">Z</span><span class="p">[</span><span class="n">point_indices</span><span class="p">]]).</span><span class="n">transpose</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
  <span class="n">new_point</span> <span class="o">=</span> <span class="n">affine_combination</span><span class="p">(</span><span class="n">points</span><span class="p">)</span>
  <span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="o">*</span><span class="n">new_point</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s">'tomato'</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mf">12.5</span><span class="p">)</span>

<span class="c1"># Plot the box around the convex hull.
</span><span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="s">'o'</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s">'navy'</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mf">7.5</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
  <span class="n">idx</span> <span class="o">=</span> <span class="n">i</span>
  <span class="n">idx_right</span> <span class="o">=</span> <span class="p">(</span><span class="n">idx</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">4</span>
  <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">([</span><span class="n">X</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">idx_right</span><span class="p">]],</span>
          <span class="p">[</span><span class="n">Y</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">Y</span><span class="p">[</span><span class="n">idx_right</span><span class="p">]],</span>
          <span class="p">[</span><span class="n">Z</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">Z</span><span class="p">[</span><span class="n">idx_right</span><span class="p">]],</span> <span class="n">color</span> <span class="o">=</span> <span class="s">'k'</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
  <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">([</span><span class="n">X</span><span class="p">[</span><span class="n">idx</span><span class="o">+</span><span class="mi">4</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">idx_right</span><span class="o">+</span><span class="mi">4</span><span class="p">]],</span>
          <span class="p">[</span><span class="n">Y</span><span class="p">[</span><span class="n">idx</span><span class="o">+</span><span class="mi">4</span><span class="p">],</span> <span class="n">Y</span><span class="p">[</span><span class="n">idx_right</span><span class="o">+</span><span class="mi">4</span><span class="p">]],</span>
          <span class="p">[</span><span class="n">Z</span><span class="p">[</span><span class="n">idx</span><span class="o">+</span><span class="mi">4</span><span class="p">],</span> <span class="n">Z</span><span class="p">[</span><span class="n">idx_right</span><span class="o">+</span><span class="mi">4</span><span class="p">]],</span> <span class="n">color</span> <span class="o">=</span> <span class="s">'k'</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
  <span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">([</span><span class="n">X</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">idx_right</span><span class="o">+</span><span class="mi">3</span><span class="p">]],</span>
          <span class="p">[</span><span class="n">Y</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">Y</span><span class="p">[</span><span class="n">idx_right</span><span class="o">+</span><span class="mi">3</span><span class="p">]],</span>
          <span class="p">[</span><span class="n">Z</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="n">Z</span><span class="p">[</span><span class="n">idx_right</span><span class="o">+</span><span class="mi">3</span><span class="p">]],</span> <span class="n">color</span> <span class="o">=</span> <span class="s">'k'</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span></code></pre></figure>

<p><img src="/images/3d_convexhull.png" alt="3d_convexhull" width="400" class="center" /></p>

<p>This cube has a volume of one, and the probability of lying within this cube has become:</p>

\[p(\mathbf{x} \in \text{Hull}(\mathbf{X})) = \frac{1}{27}\]

<p>In general, for a convex hull with volume \(c\) in \(\mathbb{R}^3\) and data points between \([x_0, x_1]\) the probability of
lying in the convex hull is \(\frac{c}{(x_1 - x_0)^3}\). This gives part of the intuition as to why the probability of 
lying within the convex hull quickly goes down as the dimensionality of the problem goes up; exponentially quickly.</p>

<p>This intuition is formalized in the paper by a theorem describing the limiting behaviour of the probability of lying in
 the convex hull for new (i.i.d.) samples from a Gaussian.</p>

<p><strong>Theorem 1</strong> (Baranay and Furedi, 1988). Given a \(d\)-dimensional dataset 
\(\mathbf{X} \triangleq \{\mathbf{x}_1, \dots, \mathbf{x}_N\}\) with i.i.d. samples \(\mathbf{x}_n \sim \mathcal{N}(0, \mathbb{I}_d)\),
\(\forall n\), the probability that a new sample \(\mathbf{x} \sim \mathcal{N}(0, \mathbb{I}_d)\) is in interpolation regime
(recall Def. 1) has the following limiting behavior</p>

\[\lim_{d \rightarrow \infty} p(\mathbf{x} \in \text{Hull}(\mathbf{X})) = \begin{cases} 
      1 &amp; \iff N &gt; d^{-1}2^{d/2} \\
      0 &amp; \iff N &lt; d^{-1}2^{d/2}
   \end{cases}\]

<p>This theorem says that, to keep the possibility of lying in the convex hull tend to one for increasing dimensions,
we need to exponentially increase the number of datapoints \(N\). See below this boundary of minimal \(N\) needed
 plotted (\(N = d^{-1}2^{d/2}\)).</p>

<p><img src="/images/evolution_N.png" alt="evolution_N" width="400" class="center" /></p>

<p>In this section we learned what the convex hull of datapoints is, and what it means to be interpolating or extrapolating w.r.t. the
convex hull of samples. We got some intuition about the curse of dimensionality, and why it is pretty unlikely to be in interpolation regime
for new data points in real-world datasets. We are ready to start reproducing the first plot.</p>

<h2 id="reproducing-the-first-plot"><span style="color:#C0392B">Reproducing the First Plot</span></h2>
<blockquote>
  <p><strong><em>In this section:</em></strong>  Ambient dimension, Intrinsic Dimension, Convex Hull Dimension</p>
</blockquote>

<p><img src="/images/first_plot.png" alt="firstplot" width="400" class="center" /></p>

<p>This image is a stack of six plots. Each plot shows the estimated
probability that a new sample from a Gaussian with dimension \(d\) will lie in the convex hull of \(N\) other samples of this
Gaussian. For example,
for the bottom plot the estimated probability that a new sample \(\mathbf{x} \sim \mathcal{N}(0, \mathbb{I}_2)\) lies in
the convex hull of the \(N = 10\) samples \(\mathbf{x}_1, \dots, \mathbf{x}_{10} \sim \mathcal{N}(0, \mathbb{I}_2)\) is roughly 50%.
For a gaussian with dimensionality 7 we already need more than \(10^{2.4} \approx 250\) examples to have roughly 50% chance of a new sample
being in the convex hull of those 250 examples. The black line through the six plots is the line that denotes how \(N\)
should change to keep the probability of a new sample being in the convex hull of 50% for increasing dimension \(d\). This
 shows that the necessary number of datapoints \(N\) increases exponentially with \(d\), because it‚Äôs a straight line through a logarithmic scale.</p>

<p>This dimension of the Gaussian \(d\) is called the <em>ambient dimension</em>. The ambient dimension of the data is the
number of dimensions we use to represent it. If we take the well-known MNIST dataset as an example, the ambient dimension
that is often used here is \(d = 28 \times 28 = 784\).</p>

<p><strong>How to get the estimate of being in the convex hull?</strong> <br />
We want to reproduce the above image, so how do we estimate the probability of being in the convex hull for a new sample? 
The authors of the paper used a Monte-Carlo estimate of the probability. For each of the six plots above, we can sample \(N\) points from a Gaussian with
dimension \(d\) to form a convex hull of samples: \(\text{Hull}(\mathbf{X}) = \{\mathbf{x}_1, \dots, \mathbf{x}_{N}\}\).
 Then, we sample a new point \(\mathbf{x} \sim \mathcal{N}(0, \mathbb{I}_d)\)
and determine whether it lies in the convex hull. We repeat this whole thing 500.000 times, and this gives 500.000 binary
decisions whether the sample was inside the hull or not. The average of this gives the probability that a new sample lies
in the convex hull of another set of samples from a Gaussian with dimension \(d\), or 1 point on the plot. To get the
 entire plot we need to do this for \(N\) between 1 and \(10^3 = 1000\) for every \(d\). You can imagine that this
 will take a while, so we will cheat the estimat slightly and only sample the convex hull once for every of the 500.000 trials
 we do per \(N\). Additionally, we only take \(10\) different values for \(N\) for each \(d\).</p>

<p>Below, we‚Äôll implement everything to do this. We are going to need a few functions:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">sample_from_gaussian</code>: a function to sample from a multivariate Gaussian</li>
  <li><code class="language-plaintext highlighter-rouge">is_in_convex_hull_batch</code>: returns vector of true‚Äôs and falses that specify whether each sample in a batch of new vectors fall inside the convex hull</li>
  <li><code class="language-plaintext highlighter-rouge">probability_in_convex_hull_batch</code>: returns the estimated probability that a new sample lies in the convex hull.</li>
</ul>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">sample_from_gaussian</span><span class="p">(</span><span class="n">num_samples</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">num_dimensions</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                         <span class="n">sigma</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">:</span>
  <span class="s">"""
  Sample num_samples from a multivariate Gaussian with `num_dimensions`
  independent dimensions.

  Returns: [num_samples, num_dimensions] gaussian samples.
  """</span>
  <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_dimensions</span><span class="p">)</span> <span class="o">+</span> <span class="n">mu</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">identity</span><span class="p">(</span><span class="n">num_dimensions</span><span class="p">)</span> <span class="o">*</span> <span class="n">sigma</span>
  <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">)</span></code></pre></figure>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">samples</span> <span class="o">=</span> <span class="n">sample_from_gaussian</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

<span class="c1"># Plot the 3 independent dimensions of the samples.
</span><span class="n">plt</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">samples</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'g'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">samples</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'b'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">samples</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'r'</span><span class="p">)</span></code></pre></figure>

<p><img src="/images/gauss_samples.png" alt="gaussamples" width="400" class="center" /></p>

<p><strong>Probability of lying in the convex hull</strong> <br />
Recall that a vector \(\mathbf{x}\) lies within the convex hull spanned by samples \(\mathbf{x}_1, \dots, \mathbf{x}_N \in \mathbb{R}^d\) 
if we can write it as a convex combination of the samples \(\mathbf{x} = \lambda_1\mathbf{x}_1 + \dots + \lambda_N\mathbf{x}_N\)
 subject to \(\lambda_i \geq 0\) and \(\sum_i \lambda_i = 1\).</p>

<p>Now to find out whether a new point \(\mathbf{x}\) is in the convex hull we just need to find out whether it is a convex 
combination of the hull samples. This can be framed as a quadratic programming problem, and that‚Äôs what we will do for the
second plot below, but for the first plot we can use a very simple batched implementation using <a href="https://scipy.org/" target="_blank">SciPy</a>.</p>

<p>The QP problem is necessary for the second plot since the method used below does not work for degenerate convex hull 
spaces, which will be the case for the second plot (lower intrinsic dimension than ambient dimension).</p>

<p>Below we use a batched implementation of finding whether points lie in the convex hull by relying on <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.Delaunay.html">scipy.Delaunay</a>.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">is_in_convex_hull_batch</span><span class="p">(</span><span class="n">new_samples</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">,</span> 
                            <span class="n">hull_samples</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">:</span>
  <span class="s">"""
  Returns a vector of size new_samples.shape[0] (the number of new samples),
  with a boolean indicating whether or not the sample lies in the convex hull.
  """</span>
  <span class="k">assert</span> <span class="n">new_samples</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="n">hull_samples</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>  \
  <span class="s">"Dimensions of new sample and convex hull samples should be the same, "</span>
  <span class="s">"but are %d and %d"</span> <span class="o">%</span> <span class="p">(</span><span class="n">new_samples</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">samples</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
  <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">hull_samples</span><span class="p">,</span> <span class="n">Delaunay</span><span class="p">):</span>
    <span class="n">hull</span> <span class="o">=</span> <span class="n">Delaunay</span><span class="p">(</span><span class="n">hull_samples</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">hull</span><span class="p">.</span><span class="n">find_simplex</span><span class="p">(</span><span class="n">new_samples</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">0</span>

<span class="k">def</span> <span class="nf">probability_in_convex_hull_batch</span><span class="p">(</span><span class="n">new_samples</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">,</span> 
                                     <span class="n">hull_samples</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
  <span class="s">"""
  The first dimension is the number of new samples, the second dimension
  is the dimensionality of the vector.
  """</span>
  <span class="n">in_convex_hull</span> <span class="o">=</span> <span class="n">is_in_convex_hull_batch</span><span class="p">(</span><span class="n">new_samples</span><span class="p">,</span> <span class="n">hull_samples</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">in_convex_hull</span><span class="p">)</span> <span class="o">/</span> <span class="n">new_samples</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></code></pre></figure>

<p>Let‚Äôs see if it works for the convex hull we visualized above in \(\mathbb{R}^2\). In addition to the point inside and the point outside the hull
 above, let‚Äôs add two more points outside the hull to get a probability of being inside the hull of \(\frac{1}{4}\).</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">point_in_hull</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.3</span><span class="p">,</span> <span class="mf">1.8</span><span class="p">])</span>
<span class="n">point_outside_hull</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">point_outside_hull_2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">point_outside_hull_3</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">points</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">point_in_hull</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
                         <span class="n">np</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">point_outside_hull</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
                         <span class="n">np</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">point_outside_hull_2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
                         <span class="n">np</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">point_outside_hull_3</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Probability inside hull (batch method): "</span><span class="p">,</span> 
      <span class="n">probability_in_convex_hull_batch</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">hull_samples</span><span class="p">))</span></code></pre></figure>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">&gt;</span><span class="o">&gt;</span> Probability inside hull <span class="o">(</span>batch method<span class="o">)</span>: 0.25
</code></pre></div></div>

<p>Now we are ready to reproduce the first plot. We sample \(N\) points for an ambient dimension \(d\) from a multivariate 
Gaussian \(\mathcal{N}(\mathbf{0}, \mathbb{I}_d)\). These points form the convex hull. Then we sample 500000 new points (<code class="language-plaintext highlighter-rouge">num_trials</code>) 
from the same Gaussian, and see whether they fall within the hull, or outside, to estimate the probability of being inside the convex hull.</p>

<p>We do this for different $N$ per dimensions \(d \in \{2, \dots, 7\}\). The N below (in <code class="language-plaintext highlighter-rouge">num_convex_hull_samples_per_dim</code>) 
are chosen to roughly be the same as in the plot (I eyeballed it).</p>

<p><strong>NB</strong>: note that the code below runs reasonably quick for dimensions 2 to 6, but takes a few minutes for dimensions = 7.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># The different ambient dimensions that we will consider.
</span><span class="n">ambient_dimensions</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>

<span class="c1"># The different values for N that we will consider.
</span><span class="n">num_dataset_sizes</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">num_convex_hull_samples_per_dim</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">logspace</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.8</span><span class="p">,</span> <span class="n">num_dataset_sizes</span><span class="p">),</span>
                                   <span class="n">np</span><span class="p">.</span><span class="n">logspace</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">num_dataset_sizes</span><span class="p">),</span>
                                   <span class="n">np</span><span class="p">.</span><span class="n">logspace</span><span class="p">(</span><span class="mf">1.2</span><span class="p">,</span> <span class="mf">2.1</span><span class="p">,</span> <span class="n">num_dataset_sizes</span><span class="p">),</span>
                                   <span class="n">np</span><span class="p">.</span><span class="n">logspace</span><span class="p">(</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">2.2</span><span class="p">,</span> <span class="n">num_dataset_sizes</span><span class="p">),</span>
                                   <span class="n">np</span><span class="p">.</span><span class="n">logspace</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="n">num_dataset_sizes</span><span class="p">),</span>
                                   <span class="n">np</span><span class="p">.</span><span class="n">logspace</span><span class="p">(</span><span class="mf">2.1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">num_dataset_sizes</span><span class="p">)]</span>
<span class="n">num_trials</span> <span class="o">=</span> <span class="mi">500000</span>

<span class="k">def</span> <span class="nf">get_convex_hull_probability_gaussian</span><span class="p">(</span><span class="n">num_trials</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dataset_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> 
                                         <span class="n">ambient_dimension</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
  <span class="s">"""Samples `dataset_size` samples from a Gaussian of dimension 
  `ambient_dimension`, and then calculates for `num_trials` new samples
  whether they lie on the convex hull or not. Returns the estimated
  probability that a new sample lies in the convex hull."""</span>
  <span class="n">convex_hull_samples</span> <span class="o">=</span>  <span class="n">sample_from_gaussian</span><span class="p">(</span><span class="n">num_samples</span><span class="o">=</span><span class="n">dataset_size</span><span class="p">,</span> 
                                              <span class="n">num_dimensions</span><span class="o">=</span><span class="n">ambient_dimension</span><span class="p">)</span>
  <span class="n">new_samples</span> <span class="o">=</span> <span class="n">sample_from_gaussian</span><span class="p">(</span><span class="n">num_samples</span><span class="o">=</span><span class="n">num_trials</span><span class="p">,</span> 
                                     <span class="n">num_dimensions</span><span class="o">=</span><span class="n">ambient_dimension</span><span class="p">)</span>
  <span class="n">p_new_samples_in_hull</span> <span class="o">=</span> <span class="n">probability_in_convex_hull_batch</span><span class="p">(</span>
      <span class="n">new_vectors</span><span class="o">=</span><span class="n">new_samples</span><span class="p">,</span> <span class="n">hull_samples</span><span class="o">=</span><span class="n">convex_hull_samples</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">p_new_samples_in_hull</span>

<span class="c1"># For each ambient dimension d, get the probability that a new sample lies in
# the convex hull per value for N.
</span><span class="n">all_probabilities</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">ambient_dimensions</span><span class="p">),</span>
                              <span class="n">num_dataset_sizes</span><span class="p">])</span>
<span class="k">for</span> <span class="n">dimension_idx</span><span class="p">,</span> <span class="n">dimension</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ambient_dimensions</span><span class="p">):</span>
  <span class="k">print</span><span class="p">(</span><span class="s">"Working on %d-D"</span> <span class="o">%</span> <span class="n">dimension</span><span class="p">)</span>
  <span class="n">num_convex_hull_samples</span> <span class="o">=</span> <span class="n">num_convex_hull_samples_per_dim</span><span class="p">[</span><span class="n">dimension_idx</span><span class="p">]</span>
  <span class="k">for</span> <span class="n">size_idx</span><span class="p">,</span> <span class="n">dataset_size</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">num_convex_hull_samples</span><span class="p">):</span>
    <span class="n">probability_in_hull</span> <span class="o">=</span> <span class="n">get_convex_hull_probability_gaussian</span><span class="p">(</span>
        <span class="n">num_trials</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">dataset_size</span><span class="p">),</span> <span class="n">dimension</span><span class="p">)</span>
    <span class="n">all_probabilities</span><span class="p">[</span><span class="n">dimension_idx</span><span class="p">,</span> <span class="n">size_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">probability_in_hull</span></code></pre></figure>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">&gt;</span><span class="o">&gt;</span> Working on 2-D
<span class="gp">&gt;</span><span class="o">&gt;</span> Working on 3-D
<span class="gp">&gt;</span><span class="o">&gt;</span> Working on 4-D
<span class="gp">&gt;</span><span class="o">&gt;</span> Working on 5-D
<span class="gp">&gt;</span><span class="o">&gt;</span> Working on 6-D
<span class="gp">&gt;</span><span class="o">&gt;</span> Working on 7-D
</code></pre></div></div>

<details open="">
<summary>Below some code to plot this (very uninteresting).</summary>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">plot_probabilities</span><span class="p">(</span><span class="n">x_space_per_dimension</span><span class="p">,</span> <span class="n">dimensions</span><span class="p">,</span> <span class="n">probabilities</span><span class="p">):</span>
  <span class="s">"""
  x_space_per_dimension: [len(dimensions)] an np.logspace per dimension
  dimensions: the different dimensions to be plotted on the Y-axis
  probabilities: [num_dimensions, num_dataset_sizes] convex hull probabilities
  """</span>
  <span class="n">num_dimensions</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dimensions</span><span class="p">)</span>
  <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>

  <span class="c1"># All stacked plots equally high.
</span>  <span class="n">gridspecs</span> <span class="o">=</span> <span class="n">gridspec</span><span class="p">.</span><span class="n">GridSpec</span><span class="p">(</span><span class="n">num_dimensions</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> 
                                <span class="n">height_ratios</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_dimensions</span><span class="p">)</span> 

  <span class="c1"># Loop over the grids and plot the probabilities for a dimension in each.
</span>  <span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s">"b"</span><span class="p">,</span> <span class="s">"orange"</span><span class="p">,</span> <span class="s">"g"</span><span class="p">,</span> <span class="s">"r"</span><span class="p">,</span> <span class="s">"purple"</span><span class="p">,</span> <span class="s">"brown"</span><span class="p">]</span>
  <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">reversed</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">gridspecs</span><span class="p">))):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x_space_per_dimension</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplot</span><span class="p">(</span><span class="n">ax</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_xscale</span><span class="p">(</span><span class="s">"log"</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span>  <span class="c1"># Hardcode the limits for each subplots here.
</span>    <span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">probabilities</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">ax</span><span class="p">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Only add the Y tick for p=1 at the topmost subplot.
</span>    <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">dimensions</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">dimensions</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
      <span class="n">ax</span><span class="p">.</span><span class="n">set_yticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="n">minor</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">ax</span><span class="p">.</span><span class="n">set_yticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">minor</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    
    <span class="c1"># Only add the X ticks for the bottom subplot.
</span>    <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
      <span class="n">ax</span><span class="p">.</span><span class="n">set_xticks</span><span class="p">([])</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'log(N)'</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">12</span><span class="p">)</span>
      <span class="n">ax</span><span class="p">.</span><span class="n">xaxis</span><span class="p">.</span><span class="n">set_label_coords</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.655</span><span class="p">)</span>

    <span class="c1"># Some annotations
</span>    <span class="n">plt</span><span class="p">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'black'</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s">'--'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">text</span><span class="p">(</span><span class="mi">10</span><span class="o">**</span><span class="mi">3</span><span class="o">+</span><span class="mi">500</span><span class="p">,</span><span class="mf">0.5</span><span class="p">,</span><span class="s">'d=%d'</span> <span class="o">%</span> <span class="n">dimensions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="p">,</span><span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

  <span class="c1"># remove vertical gap between subplots
</span>  <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"p(x in Hull)"</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">hspace</span><span class="o">=</span><span class="p">.</span><span class="mi">0</span><span class="p">)</span>
  <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span></code></pre></figure>

</details>

<h1 id="disclaimers"><span style="color:#C0392B">Disclaimers</span></h1>

<p>‚Ä¶</p>

<h1 id="sources"><span style="color:#2874A6">Sources</span></h1>

<p>Randall Balestriero and Jerome Pesenti and Yann LeCun (2021).
    <a href="https://arxiv.org/pdf/2110.09485.pdf" target="_blank"><em>Learning in High Dimension Always Amounts to Extrapolation</em></a></p>
:ET