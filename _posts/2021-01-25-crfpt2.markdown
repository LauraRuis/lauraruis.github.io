---
layout: post
title:  "Structured Prediction Pt. 2 - Implementing a linear-chain CRF"
date:   2021-01-25 13:09:17 -0500
usemathjax: true
---
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

In this part of the series of posts on structured prediction with conditional random fields (CRFs) we are going to implement all the ingredients
that were discussed in [part 1]({% post_url 2021-01-25-crfpt1 %}). Recall that we discussed how to model
the dependencies among labels in sequence prediction tasks with a linear-chain CRF. Now, we will put a such a CRF
on top of a neural network feature extractor and use it for
part-of-speech (POS) tagging. 

Everything below is inspired by <a href="https://arxiv.org/abs/1603.01354" target="_blank">this paper</a> by Ma & Hovy,
and the implementation has lots of parts that come from
<a href="https://github.com/allenai/allennlp/blob/main/allennlp/modules/conditional_random_field.py" target="_blank">the AllenNLP implementation</a>,
so if it's simply a good implementation you're looking for, take a look at that one. If you'd like to understand how it works from scratch, keep on reading. Bear with me here,
I discuss everything rather in detail and try not to skip over anything, from batching and broadcasting to changing the forward-recursion of BP for a cleaner implementation and Viterbi,
so if you rather want a succint blogpost you might want to choose one of the great other options that are also out there! Alternatively, there are some sections you can skip
if they're clear, like batching and broadcasting.

![annotated_example](/images/opener_gif/opener.gif)

To learn a model that can annotate examples like the one above with POS tags, we need two things:

1. A dataset with examples consisting of input sentences annotated with POS tags.
We will choose the <a href="http://universaldependencies.org/" target="_blank">Universal Dependencies</a> dataset (<a href="https://www.aclweb.org/anthology/L14-1067/" target="_blank">Silveira et al., 2014</a>).

2. A feature extractor to extract features from our input data. For this we will choose a bidirectional LSTM, which we will motivate below. 

Then all the things we need to implement are:

- A `Vocabulary` to convert from strings to numerical values that are machine readable

- A `TaggingDataset` to convert all our data to `Tensors` that can be processed by <a href="https://pytorch.org/" target="_blank">PyTorch</a>

- An `Encoder` model that is our feature extractor (the bidirectional LSTM).

- A `ChainCRF` model that implements all the CRF methods, like belief propagation (BF) and Viterbi decoding.

- A `train()` loop to train our CRF and feature-extractor end-to-end on data.

- A `test()` loop to test a trained model on new data.

The final model we get by walking through this post is depicted in the image below.

<img src="/images/bilstmcrf.png" alt="bilstmcrf" width="600" class="center"/>

To train this model end-to-end we will use the negative log-likelihood (NLL) loss function, which is simply the negated log-likelihood
that was given in part 1 of this series. Given some example input-output pairs $$(\mathbf{x}^{(i)}, \mathbf{y}^{(i)})_{i=1}^N$$,
the NLL of the entire dataset is:

$$
\begin{aligned}
\text{NLL} = -\log \mathcal{L}(\boldsymbol{\theta}) &= - \sum_{i=1}^{N} \log p(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)}) \\
&= \sum_{i=1}^{N}\log\left(Z(\mathbf{x}^{(i)})\right) - \sum_{i=1}^{N}\left(\sum_{t=1}^m \boldsymbol{\theta}_1 f(y_t^{(i)}, \mathbf{x}^{(i)}, t) + \sum_{t=1}^{m-1} \boldsymbol{\theta}_2 f(y_t^{(i)}, y_{t+1}^{(i)})\right) \\
\end{aligned}
$$

Instead of maximizing the log-likelihood of our data, we will minimize the negative log-likelihood, which is equivalent.
We can use stochastic gradient descent (SGD) with automatic differentiation in <a href="https://pytorch.org/" target="_blank">PyTorch</a>, meaning we only need the forward-part of the BP algorithm.
Recall that the forward recursion allows calculation of the partition function ($$Z(\mathbf{x})$$), which we need for the NLL. The backward recursion allows calculating the
marginals (which are needed for the gradients). PyTorch takes care of the latter calculation for us.

Let's start with feature extraction and defining $$\boldsymbol{\theta}_1$$, $$\boldsymbol{\theta}_2$$, $$f(y_t, \mathbf{x}, t)$$, and $$f(y_t, y_{t+1})$$.

## <span style="color:#C0392B">Preliminaries</span>

Ok I lied, let's start with some preliminaries, if you want to run the code used in this post yourself, make sure to install **PyTorch >= 1.7.0**,
**Python 3.6**, and **TorchNLP >= 0.5.0**. 

## <span style="color:#C0392B">Feature Extraction</span>

There are some desiderata for our features. The function $$f(y_t, \mathbf{x}, t)$$ signifies that we want each tag in the output sequence $$y_t$$ to be informed about (i.e., depend on) the entire input sequence $$\mathbf{x}$$,
but also on the current word at time $$t$$.
Furthermore, $$f(y_t, y_{t+1})$$ tells us that we want each next output tag $$y_{t+1}$$ to depend on the previous tag $$y_{t}$$.
Parametrizing the former part, we take $$\boldsymbol{\theta}_1f(y_t, \mathbf{x}, t)$$ to be the output of a bidirectional LSTM projected down to the right dimension with a linear layer:

$$
\begin{aligned}
\mathbf{\bar{H}} &= \text{biLSTM}(\mathbf{x}) \\
\mathbf{H} &= \mathbf{\bar{H}}\mathbf{W} + \mathbf{b}
\end{aligned}
$$

In the above, $$\mathbf{x} \in \mathbb{R}^{m}$$ is our input sequence, $$\mathbf{\bar{H}} \in \mathbb{R}^{m \times 2d_h}$$ the hidden vectors for each input word $$x_t$$ stacked into a matrix, with $$d_h$$ the hidden dimension of the LSTM (doubled because
a bidirectional LSTM is essentially two LSTMs processing the input sequence from left-to-right and right-to-left).
$$\mathbf{W} \in \mathbb{R}^{2d_h \times |S|}$$ a matrix of parameters that projects the output to the right dimension, namely $$|S|$$ values for each input word $$x_t$$. The t-th row of $$\mathbf{H} \in \mathbb{R}^{m \times |S|}$$
(let's define that by $$\mathbf{H}_{t*}$$) then holds the features for the t-th word ($$x_t$$) in the input sequence. 
These values reflect $$\boldsymbol{\theta}_1f(y_t, \mathbf{x}_t, t)$$ for each possible $$y_t$$, since they depend on the entire input sequence $$\mathbf{x}$$, but are specific to the current word at time $$t$$.

$$\boldsymbol{\theta}_1f(y_t, \mathbf{x}_t, t) = \mathbf{H}_{t,y_t}$$

Using PyTorch, we can code this up in a few lines. As is common in computational models of language, we will assign each
input token a particular index and use dense word embeddings that represent each word in our input vocabulary. We reserve
index 0 for the special token `<PAD>`, which we need later when we will batch our examples, grouping together examples
of different length $$m$$. 

Strictly speaking btw, our bi-LSTM does not take $$\mathbf{x}$$ as input but $$\mathbf{E}_{x_t*}$$, where
$$\mathbf{E}$$ is the matrix of embedding parameters of size $$|S| \times d_e$$. So if $$x_t = 3$$ (index 3 in our vocabulary, which might
be mapping to *book* for example), $$\mathbf{E}_{x_t*}$$ takes out the corresponding embedding of size $$d_e$$.

{% highlight python %}
class Encoder(nn.Module):
  """
  A simple encoder model to encode sentences. Bi-LSTM over word embeddings.
  """
  def __init__(self, vocabulary_size: int, embedding_dim: int,
                     hidden_dimension: int, padding_idx: int):
    super(Encoder, self).__init__()
    # The word embeddings.
    self.embedding = nn.Embedding(num_embeddings=vocabulary_size, 
                                  embedding_dim=embedding_dim,
                                  padding_idx=padding_idx)
    
    # The bi-LSTM.
    self.bi_lstm = nn.LSTM(input_size=embedding_dim, 
                           hidden_size=hidden_dimension, num_layers=1, 
                           bias=True, bidirectional=True)
  
  def forward(self, sentence: torch.Tensor) -> torch.Tensor:
    """
    :param sentence: input sequence of size [batch_size, sequence_length]
    Returns: tensor of size [batch_size, sequence_length, hidden_size * 2] 
    the hidden states of the biLSTM for each time step.
    """
    embedded = self.embedding(sentence)
    # embedded: [batch_size, sequence_length, embedding_dimension]
    
    output, (hidden, cell) = self.bi_lstm(embedded)
    # output: [batch_size, sequence_length, hidden_size * 2]
    # hidden: [batch_size, hidden_size * 2]
    # cell: [batch_size, hidden_size * 2]
    return output
{% endhighlight %}

That's all we need to do to extract features from out input sequences! Later in this post we will define $$\boldsymbol{\theta}_2$$ 
and $$f(y_t, y_{t+1})$$, which are part of the actual CRF. First, we'll set up our `Tagger`-module which takes the encoder,
the CRF (to implement later), and outputs the negative log-likelihood and a predicted tag sequence.

## <span style="color:#C0392B">The Tagger</span>

Below we can find the `Tagger` module. This is basically the class that implements the model as depicted in the image
above. The most interesting part is still missing (namely the `ChainCRF`-module, but also the `Vocabulary`-module), but bear with me, we'll get to those. The
 forward pass of the `Tagger` takes an input sequence, a target sequence, and an input mask (we'll get to what that is
 when we discuss batching), puts the input sequence through the encoder and the CRF, and outputs the NLL `loss`, the `score` (
 which is basically the nominator of $$p(\mathbf{y} \mid \mathbf{x})$$), and the `tag_sequence` obtained by decoding with viterbi.
If you don't have a feeling of what the parameters `input_mask` and `input_lengths` in the module should be, don't worry,
that will be discussed in the section 'sequence batching' below.

{% highlight python %}
class Tagger(nn.Module):
  """
  A POS tagger.
  """
  
  def __init__(self, input_vocabulary: Vocabulary, 
               target_vocabulary: Vocabulary,
               embedding_dimension: int, hidden_dimension: int):
    super(Tagger, self).__init__()
    # The Encoder to extract features from the input sequence.
    self.encoder = Encoder(vocabulary_size=input_vocabulary.size, 
                           embedding_dim=embedding_dimension, 
                           hidden_dimension=hidden_dimension, 
                           padding_idx=input_vocabulary.pad_idx)
    
    # The linear projection (with parameters W and b).  
    self.encoder_to_tags = nn.Linear(hidden_dimension * 2, 
                                     target_vocabulary.size)
    
    # The linear-chain CRF.
    self.tagger = ChainCRF(num_tags=target_vocabulary.size, 
                           tag_vocabulary=target_vocabulary)
  
  def forward(self, input_sequence: torch.Tensor, 
              target_sequence: torch.Tensor, 
              input_mask: torch.Tensor, input_lengths: torch.Tensor):
    """
    :param input_sequence: input sequence of size [batch_size, sequence_length]
    :param target_sequence: POS tags target, [batch_size, sequence_length]
    :param input_mask: padding-mask, [batch_size, sequence_length]
    :param input_lengths: lengths of each example in the batch [batch_size]
    Returns: ...
    """
    # input_sequence: [batch_size, sequence_length, input_vocabulary_size]
    lstm_features = self.encoder(input_sequence)
    # lstm_features: [batch_size, sequence_length, hidden_dimension*2]
    
    crf_features = self.encoder_to_tags(lstm_features)
    # crf_features: [batch_size, sequence_length, target_vocabulary_size]
    
    loss, score, tag_sequence = self.tagger(input_features=crf_features,
                                            target_tags=target_sequence,
                                            input_mask=input_mask,
                                            input_lengths=input_lengths)
    # loss, score: scalars
    # tag_sequence: [batch_size, sequence_length]
    return loss, score, tag_sequence
{% endhighlight %}

OK, now we can finally get to the definition of $$\boldsymbol{\theta}_2$$ and $$f(y_t, y_{t+1})$$, and the implementation of the linear-chain CRF!

## <span style="color:#C0392B">Implementing a Linear-Chain CRF</span>

We want $$f(y_t, y_{t+1})$$ to represent the likelihood of some tag $$y_{t+1}$$ following $$y_t$$ in the sequence, which
can be interpreted as transition 'probabilities' from one tag to another. The parameters $$\boldsymbol{\theta}_2$$ are shared
over time (meaning they are the same for each $$t \in \{1, \dots, m-1\}$$), and thus we can simply define a matrix of transition 'probabilities' from each tag to another tag.
We define $$\boldsymbol{\theta}_2f(y_t, y_{t+1}) = \mathbf{T}_{y_t,y_{t+1}}$$, meaning the t-th row and (t+1)-th column of a matrix $$\mathbf{T}$$.
This matrix $$\mathbf{T}$$ will be of size $$(|S| + 2) \times (|S| + 2)$$. The 2 extra tags are the `<ROOT>` and the `<EOS>` tag.
We need some tag to start of the sequence and to end the sequence, because we want to take into account the probability of a particular tag being the
first tag of a sequence, and the probability of a tag being the last tag. For the former we will use `<ROOT>`, and a for the latter we'll use `<EOS>`.

In this part we will implement:

- The forward-pass of belief propagation (`ChainCRF.forward_belief_propagation(...)`), calculating the partition function (i.e., the denominator of $$p(\mathbf{y} \mid \mathbf{x})$$).

- Calculating the nominator of $$p(\mathbf{y} \mid \mathbf{x})$$ (`ChainCRF.score_sentence(...)`)

- Decoding to get the target sequence prediction (`ChainCRF.viterbi_decode(...)`)

Below, you'll find the `ChainCRF` class that holds all these methods. The matrix of transition probabilities $$\mathbf{T}$$
is initialized below as `log_transitions` (note that $$\mathbf{T}$$ are actually the log-transition probabilities because in the CRF equations they are $$\exp(\boldsymbol{\theta}_2f(y_t, y_{t+1})) = \exp\mathbf{T}_{y_t,y_{t+1}}$$), and we hard-code the transition probabilities from any $$y_t$$ to the `<ROOT>`
tag to be -10000 because this should not be possible (and this becomes 0 in the CRF equation: `exp(log_transitions)` gives $$\exp-10000 \approx 0$$).
We do the same for any transition from `<EOS>` to any other tag. The class below implements the methods to calculate the NLL loss, and the total forward-pass of the CRF that returns this loss as well as a predicted tag sequence. In the sections below we will implement the necessary methods for our linear-chain CRF, starting with belief propagation.
{% highlight python %}
 class ChainCRF(nn.Module):
      """
      A linear-chain conditional random field.
      """
      
      def __init__(self, num_tags: int, tag_vocabulary: Vocabulary):
        super(ChainCRF, self).__init__()

        self.tag_vocabulary = tag_vocabulary
        self.num_tags = num_tags + 2 # +2 for <ROOT> and <EOS>
        self.root_idx = tag_vocabulary.size
        self.end_idx = tag_vocabulary.size + 1

        # Matrix of transition parameters.  Entry (i, j) is the score of
        # transitioning *from* i *to* j.
        self.log_transitions = nn.Parameter(torch.randn(self.num_tags, 
                                                        self.num_tags))

        # Initialize the log transitions with xavier uniform (TODO: refer)
        self.xavier_uniform()

        # These two statements enforce the constraint that we never transfer
        # to the start tag and we never transfer from the stop tag
        self.log_transitions.data[:, self.root_idx] = -10000.

        self.log_transitions.data[self.end_idx, :] = -10000.
      
      def xavier_uniform(self, gain=1.):
        torch.nn.init.xavier_uniform_(self.log_transitions)

      def forward_belief_propagation(self, input_features: torch.Tensor, 
                                    input_mask: torch.Tensor) -> torch.Tensor:
        raise NotImplementedError()
      
      def score_sentence(self, input_features: torch.Tensor,
                        target_tags: torch.Tensor, 
                        input_mask: torch.Tensor) -> torch.Tensor:
        raise NotImplementedError()
          
      def viterbi_decode(self, input_features: torch.Tensor, 
                               input_lengths: torch.Tensor) -> Tuple[
                                                        torch.Tensor,
                                                        torch.Tensor]:
        raise NotImplementedError()
      
      def negative_log_likelihood(self, input_features: torch.Tensor, 
                                  target_tags: torch.Tensor, 
                                  input_mask: torch.Tensor) -> torch.Tensor:
        """
        Returns the NLL loss.
        :param input_features: the features for each input sequence
                [batch_size, sequence_length, feature_dimension]
        :param target_tags: the target tags
                [batch_size, sequence_length]
        :param input_mask: the binary mask determining which of 
                the input entries are padding [batch_size, sequence_length]
        """
        partition_function = self.forward_belief_propagation(
                                    input_features=input_features, 
                                    input_mask=input_mask)
        nominator = self.score_sentence(
                                    input_features=input_features,
                                    target_tags=target_tags, 
                                    input_mask=input_mask)
        return partition_function - nominator

      def forward(self, input_features: torch.Tensor, 
                  target_tags: torch.Tensor, 
                  input_mask: torch.Tensor,
                  input_lengths: torch.Tensor) -> Tuple[torch.Tensor, 
                                                        torch.Tensor, 
                                                        torch.Tensor]:
        """
        The forward-pass of the CRF, which calculates the NLL loss and 
        returns a predicted sequence.
        :param input_features: features for each input sequence
                [batch_size, sequence_length, feature_dimension]
        :param target_tags: the target tags 
                [batch_size, sequence_length]
        :param input_mask: the binary mask determining which of 
                the input entries are padding [batch_size, sequence_length]
        """
        loss = self.negative_log_likelihood(input_features=input_features, 
                                            target_tags=target_tags,
                                            input_mask=input_mask)
        with torch.no_grad():
          score, tag_sequence = self.viterbi_decode(input_features,
                                                    input_lengths=input_lengths)
        return loss, score, tag_sequence
        
{% endhighlight %}


But first, since we implement all these method in batched versions, let's briefly go over **batching**.

# <span style="color:#C0392B">Sequence Batching</span>

Processing data points in batches has multiple benefits: averaging the gradient over a minibatch in SGD allows playing
with the noise you want while training your model (batch size of 1 gives a maximally noisy gradient, batch size of $$N$$ is minimally
noisy gradient, namely gradient descent without the stochasticity), but also: batching examples speeds up training. This motivates
us to implement all the methods for the CRF in batched versions, allowing parallel processing. We can't parallelize the time-dimension in CRFs unfortunately.

A batch of size 2 would look like this:

![batch](/images/batch.png)

For each batch we will additionally keep track of the lengths in the batch. For the image above a list of lengths would be `input_lengths = [8, 5]`.
For example, batched inputs to our encoder will be of size `[batch_size, sequence_length]` and outputs of size `[batch_size, sequence_length, hidden_dim*2]`.
The input mask for the above batch looks like this:

![input_mask](/images/input_mask.png)

# <span style="color:#C0392B">Implementation in log-space, stable ''logsumexp-ing'' & broadcasting</span>

Before we can finally get into the interesting implementations, we need to talk about two things. Firstly,
for calculating the partition function we are going to need to sum a bunch of $$exp(\cdot)$$'s, which might explode.
To do this numerically stable, we will use the *logsumexp*-trick. The log here comes from the fact that we will implement
everything in log-space. Numerical stability doesn't fare well with
recursive multiplication of small values (i.e., values between 0 and 1) or large values. 
In log-space, multiplications become summations, which have less of a risk of becoming too small or too large.
Recall that the initialization and recursion in the forward-pass of belief propagation are given by the following equations:

$$
\begin{aligned}
\alpha(1, y^{\prime}_2) &= \sum_{y^{\prime}_1}\psi(y^{\prime}_1, \mathbf{x}, 1) \cdot  \psi(y^{\prime}_1, y^{\prime}_{2}) \\
\alpha(t, y^{\prime}_{t+1}) &\leftarrow \sum_{y^{\prime}_{t}}\psi(y^{\prime}_t, \mathbf{x}, t) \cdot  \psi(y^{\prime}_t, y^{\prime}_{t+1})\cdot \alpha(t-1, y^{\prime}_t)
\end{aligned}
$$

First we convert the alpha initialization to log-space:

$$
\begin{aligned}
\alpha(1, y^{\prime}_2) &= \sum_{y^{\prime}_1}\psi(y^{\prime}_1, \mathbf{x}, 1) \cdot  \psi(y^{\prime}_1, y^{\prime}_{2}) \\
\log \alpha(1, y^{\prime}_2) &= \log \sum_{y^{\prime}_1}\psi(y^{\prime}_1, \mathbf{x}, 1) \cdot  \psi(y^{\prime}_1, y^{\prime}_{2}) \\
\end{aligned}
$$

Then we convert the recursion equation to log-space, we plug in the CRF factors, and we see why we
get a *logsumexp*:

$$
\begin{aligned}
\log \alpha(t, y^{\prime}_{t+1}) &\leftarrow \log \sum_{y^{\prime}_{t}}\psi(y^{\prime}_t, \mathbf{x}, t) \cdot  \psi(y^{\prime}_t, y^{\prime}_{t+1})\cdot \exp\log\alpha(t-1, y^{\prime}_t) \\
\log \alpha(t, y^{\prime}_{t+1}) &\leftarrow \log \sum_{y^{\prime}_{t}}\exp\left(\boldsymbol{\theta}_1f(y^{\prime}_t, \mathbf{x}, t) + \boldsymbol{\theta}_2f(y^{\prime}_t, y^{\prime}_{t+1})\right) \cdot \exp\log\alpha(t-1, y^{\prime}_t) \\
\log \alpha(t, y^{\prime}_{t+1}) &\leftarrow \underbrace{\log \sum_{y^{\prime}_{t}}\exp}_{\text{logsumexp}}\left(\boldsymbol{\theta}_1f(y^{\prime}_t, \mathbf{x}, t) + \boldsymbol{\theta}_2f(y^{\prime}_t, y^{\prime}_{t+1}) + \log\alpha(t-1, y^{\prime}_t)\right)
\end{aligned}
$$

Now what *logsumexp* does is rewrite this as follows. Let everything inside the $$exp(\cdot)$$ above for simplicity be $$q_t$$:

$$
\begin{aligned}
\log \sum_{y^{\prime}_{t}}\exp(q_t) &= \log \sum_{y^{\prime}_{t}}\exp(q_t - c + c) \\
&= \log \sum_{y^{\prime}_{t}}\exp(q_t - c)\exp(c) \\
&= \log \exp(c)\sum_{y^{\prime}_{t}}\exp(q_t - c) \\
&= c + \log\sum_{y^{\prime}_{t}}\exp(q_t - c) \\
\end{aligned}
$$

If we take the constant $$c$$ to be the maximum value that $$q_t$$ can take in the sum, the summation becomes stable.
See below the code that implements this, adapted from AllenNLP.

{% highlight python %}
def logsumexp(tensor: torch.Tensor, dim: int=-1) -> torch.Tensor:
    """
    A numerically stable computation of logsumexp. This is mathematically
    equivalent to `tensor.exp().sum(dim).log()`. 
    This function is typically used for summing log probabilities.
    :param tensor: A tensor of arbitrary size.
    :param dim: The dimension of the tensor to apply the logsumexp to.
    """
    max_score, _ = tensor.max(dim)
    stable_vec = tensor - max_score.unsqueeze(dim)
    return max_score + (stable_vec.exp().sum(dim)).log()
{% endhighlight %}

Broadcasting over dimensions is conceptually the same thing as matrix multiplication of a column vector with a row vector,
but then by summing. For example, if we sum `a + b = c` where $$a \in \mathbb{R}^{2 \times 1}$$ and $$b \in \mathbb{R}^{1 \times 2}$$
the result will be $$c \in \mathbb{R}^{2 \times 2}$$:

$$
\begin{pmatrix}
1\\ 
2\\ 
\end{pmatrix} + \begin{pmatrix}
3 & 4
\end{pmatrix} = \begin{pmatrix}
4 & 5\\ 
5 & 6
\end{pmatrix}
$$

You can use broadcasting if you have a value that you want to add to every index of a vector, like
in the above case 1 is added to 3 and 4 for the top row and 2 to 3 and 4 for the bottom row.

# <span style="color:#C0392B">Forward Belief Propagation</span>

In this section we will implement the forward-pass of belief propagation, which we need to calculate part of the NLL loss (namely $$\log\left(Z(\mathbf{x}^{(i)})\right)$$). 

We are implementing the forward recursion in a batched version, meaning that the loop over time goes from
$$t=1$$ to $$m-1$$ for the largest $$m$$ in the batch. Some sequences might already end somewhere earlier in the loop,
which is why we will mask out the recursion for those sequences and retain the old $$\alpha$$ variables for them.
Implementing batched forward belief propagation becomes *a lot* easier if we instead of using the recursion
equation as defined above, use the following recursion:

$$
\begin{aligned}
\hat{\alpha}(1, y^{\prime}_2) &= \psi(y^{\prime}_2, \mathbf{x}, 2)\sum_{y^{\prime}_1}\psi(y^{\prime}_1, \mathbf{x}, 1) \cdot  \psi(y^{\prime}_1, y^{\prime}_{2}) \\
\hat{\alpha}(t, y^{\prime}_{t+1}) &\leftarrow \psi(y^{\prime}_{t+1}, \mathbf{x}, t+1)\sum_{y^{\prime}_{t}}\psi(y^{\prime}_t, y^{\prime}_{t+1})\cdot \hat{\alpha}(t-1, y^{\prime}_t)
\end{aligned}
$$

Why this is equivalent in the end becomes apparent when we look at the full partition function that we're trying to compute.
We simply already take the unary features $$\psi(y_2^{\prime}, \mathbf{x}, 2)$$ for the next recursion.

$$
\begin{aligned}
Z(\mathbf{x}) &= \sum_{y^{\prime}_m}\psi(y^{\prime}_m, \mathbf{x}, m)\sum_{y^{\prime}_{m-1}}\psi(y^{\prime}_{m-1}, \mathbf{x}, m-1)\psi(y^{\prime}_{m-1}, y^{\prime}_{m}) \dots \dots \\ 
& \quad \quad \quad \dots \sum_{y^{\prime}_2}\psi(y^{\prime}_2, \mathbf{x}, 2) \cdot  \psi(y^{\prime}_2, y^{\prime}_{3})\underbrace{\sum_{y^{\prime}_1}\psi(y^{\prime}_1, \mathbf{x}, 1) \cdot  \psi(y^{\prime}_1, y^{\prime}_{2})}_{\alpha(1, y^{\prime}_{2})} \\
&= \sum_{y^{\prime}_m}\psi(y^{\prime}_m, \mathbf{x}, m)\sum_{y^{\prime}_{m-1}}\psi(y^{\prime}_{m-1}, \mathbf{x}, m-1)\psi(y^{\prime}_{m-1}, y^{\prime}_{m}) \dots \dots \\ 
& \quad \quad \quad \dots \sum_{y^{\prime}_2} \psi(y^{\prime}_2, y^{\prime}_{3}) \cdot \underbrace{\psi(y^{\prime}_2, \mathbf{x}, 2)\sum_{y^{\prime}_1}\psi(y^{\prime}_1, \mathbf{x}, 1) \cdot  \psi(y^{\prime}_1, y^{\prime}_{2})}_{\hat{\alpha}(1, y^{\prime}_{2})}
\end{aligned}
$$

Remember that in the forward recursion of BP we looped until $$m-1$$, and then for time $$m$$ we don't have transition probabilities to $$m+1$$ anymore, so we separately did the following:

$$Z(\mathbf{x}) = \sum_{y^{\prime}_m}\psi(y^{\prime}_m, \mathbf{x}, m) \cdot\alpha(m-1, y^{\prime}_m)$$

The new recursion results in much easier code because instead of separately keeping track of when each sequence in the batch
ends and writing an if-else statement within the loop over time that calculates the above equations for the sequences that
are ending, we simply already incorporate the unary features of $$y_{t+1}^{\prime}$$ in each recursive calculation.
Then, at the end we only need to sum all new alphas:

$$Z(\mathbf{x}) = \sum_{y^{\prime}_m}\hat{\alpha}(m-1, y^{\prime}_m)$$

The equations we are going to implement now are these recursions in log-space. In the equation below I've annotated
every part of the equations with the corresponding variable name their values are a part of in the implementation below.

$$
\begin{aligned}
\log\hat{\alpha}(1, y^{\prime}_2) &= \log\psi(y^{\prime}_2, \mathbf{x}, 2) + \log\sum_{y^{\prime}_1}\psi(y^{\prime}_1, \mathbf{x}, 1) \cdot  \psi(y^{\prime}_1, y^{\prime}_{2}) \\
\log\hat{\alpha}(t, y^{\prime}_{t+1}) &\leftarrow \log\left(\psi(y^{\prime}_{t+1}, \mathbf{x}, t+1)\sum_{y^{\prime}_{t}}\psi(y^{\prime}_t, y^{\prime}_{t+1})\cdot \exp\log\hat{\alpha}(t-1, y^{\prime}_t)\right) \\
&\leftarrow \log\left(\sum_{y^{\prime}_{t}}\psi(y^{\prime}_{t+1}, \mathbf{x}, t+1) \cdot \psi(y^{\prime}_t, y^{\prime}_{t+1})\cdot \exp\log\hat{\alpha}(t-1, y^{\prime}_t)\right) \\
&\leftarrow \log\left(\sum_{y^{\prime}_{t}}\exp\left(\underbrace{\boldsymbol{\theta}_1f(y^{\prime}_{t+1}, \mathbf{x}, t+1)}_{\text{unary_features}} + \underbrace{\boldsymbol{\theta}_2(y^{\prime}_t, y^{\prime}_{t+1})}_{\text{transition_scores}} + \underbrace{\hat{\alpha}(t-1, y^{\prime}_t)}_{\text{forward_alpha_e}}\right)\right) \\
\end{aligned}
$$

We discussed above that $$\boldsymbol{\theta}_1f(y^{\prime}_t, \mathbf{x}, t) = \mathbf{H}_{t,y_t}$$ is the t-th row and $$y_t$$-th column of the projected output of our encoder. These values are collected in a vector called `unary_features` for all $$y^{\prime}_t$$ (meaning the vector has size $$|S|$$). Then $$\boldsymbol{\theta}_2f(y^{\prime}_t, y^{\prime}_{t+1}) = \mathbf{T}_{y_t,y_{t+1}}$$
is the $$y_t$$-th row and $$y_{t+1}$$-th column of the matrix of transition probabilities. The vector `transition_scores` in the code holds the probabilities from all $$y_t$$'s to a particular $$y_{t+1}$$. 

Now let's take a look at the code that implements all this. Below the code we'll go through some clarifications.
{% highlight python %}
def forward_belief_propagation(self, input_features: torch.Tensor, 
                                     input_mask: torch.Tensor)
    """
    Efficient inference with BP of the partition function of the ChainCRF.
    :param input_features: the features for each input sequence
            [batch_size, sequence_length, num_tags] 
    :param input_mask: the binary mask determining which of the input 
            entries are padding [batch_size, sequence_length]
    """
    batch_size, sequence_length, num_tags = input_features.size()
    
    # We don't have input features for the tags <ROOT> and <EOS>, 
    # so we artifially add those at the tag-dimension. 
    # See in the class constructor above that the last 
    # two indices are for the <ROOT> and <EOS> tags.
    input_features = torch.cat(
        [input_features, torch.zeros([batch_size, sequence_length, 2]) - 10000.],
        dim=2)

    # Initialize the recursion variables with 
    # transitions from root token + first unary features.
    init_alphas = self.log_transitions[self.root_idx, :] + input_features[:, 0]

    # Set recursion variable.
    forward_alpha = init_alphas

    # Make time major, we will loop over the time-dimension.
    input_features = torch.transpose(input_features, 0, 1)
    # input_features: [sequence_length, batch_size, num_tags]
    input_mask = torch.transpose(input_mask.float(), 0, 1)
    # input_mask [sequence_length, batch_size]

    # Loop over sequence and calculate the recursion alphas.
    for time in range(1, max_time):

      # Get unary features for this time step.
      features = input_features[time]

      # Expand the first dimension so we can broadcast it.
      unary_features = features.view(batch_size, self.num_tags).unsqueeze(1)

      # Expand the batch dimension so we can broadcast the sum
      # over it, the transition scores are the same over the batch dimension.
      transition_scores = self.log_transitions.unsqueeze(0)

      # Calculate next tag probabilities.
      forward_alpha_e = forward_alpha.unsqueeze(2)
      next_forward_alpha = forward_alpha_e + unary_features + transition_scores

      # Calculate next forward alpha by taking logsumexp over current tag axis, 
      # mask all instances that ended and keep the old forward alphas
      # for those instances.
      forward_alpha = (
        logsumexp(next_forward_alpha, 1) * input_mask[time].view(batch_size, 1)
        + forward_alpha * (1 - input_mask[time]).view(batch_size, 1)
      )

    final_transitions = self.log_transitions[:, self.end_idx]

    alphas = forward_alpha + final_transitions.unsqueeze(0)
    partition_function = logsumexp(alphas)

    return partition_function
{% endhighlight %}

The code recurses over the time dimension, from $$1$$ to $$m$$, and calculates the alphas in log-space.
The first important line is the following:

`next_forward_alpha = forward_alpha_e + unary_features + transition_scores`

This line basically calculates the recursion above without the *logsumexp*-ing:

$$\underbrace{\boldsymbol{\theta}_1f(y^{\prime}_{t+1}, \mathbf{x}, t+1)}_{\text{unary_features}} + \underbrace{\boldsymbol{\theta}_2(y^{\prime}_t, y^{\prime}_{t+1})}_{\text{transition_scores}} + \underbrace{\hat{\alpha}(t-1, y^{\prime}_t)}_{\text{forward_alpha_e}}$$

But then for all possible POS tags $$y^{\prime}_t$$. This is where we use broadcasting. Then if we logsumexp over the first dimension:

`logsumexp(next_forward_alpha, 1)`

We get our subsequent alpha recursion variables for all $$y^{\prime}_{t+1}$$. The following image shows what happens
in these two lines of code for a single example in a batch graphically:

<img src="/images/broadcast_forward.png" alt="broadcast_forward" width="800" class="center"/>

Note that we add the transition from each tag to the final tag outside of the loop over time, because these are independent
of time and should happen for every sequence in the batch. Then the whole function returns:

`logsumexp(alphas)` which is $$Z(\mathbf{x}) = \sum_{y^{\prime}_m}\hat{\alpha}(m-1, y^{\prime}_m)$$.

Yay, that's calculating the partition function! Now let's look at calculating the nominator of the CRF,
or the function `ChainCRF.score_sentence(...)`.

# <span style="color:#C0392B">Calculating the Nominator</span>

Meep Morp Beep.

{% highlight python %}
def score_sentence(self, input_features: torch.Tensor,
                        target_tags: torch.Tensor, 
                        input_mask: torch.Tensor) -> torch.Tensor:
        batch_size, max_time, feature_dimension = input_features.size()

        # Make time major.
        input_features = input_features.transpose(0, 1)  # (time, batch_size, dim)
        input_mask = input_mask.float().transpose(0, 1)  # (time, batch_size)
        target_tags = target_tags.transpose(0, 1)  # (time, batch_size)

        # Get tensor of root tokens and tensor of next tags (first tags).
        root_tags = torch.LongTensor([self.root_idx] * batch_size)
        if torch.cuda.is_available():
          root_tags.to(device)
        next_tags = target_tags[0].squeeze()

        # Initial transition is from root token to first tags.
        initial_transition = self.log_transitions[root_tags, next_tags]

        # Initialize scores.
        scores = initial_transition

        # Loop over time and at each time calculate the score from t to t + 1.
        for time in range(max_time - 1):

            # Get emission scores, transition scores and calculate score for current time step.
            features_t = input_features[time]
            next_tags = target_tags[time + 1].squeeze()
            current_tags = target_tags[time].squeeze()
            if len(current_tags.shape) == 0:
              current_tags = current_tags.unsqueeze(0)
            emission = torch.gather(features_t, 1, current_tags.unsqueeze(1)).squeeze()
            transition = self.log_transitions[current_tags, next_tags]

            # Add scores.
            scores = scores + transition * input_mask[time + 1] + emission * input_mask[time]

        # Add scores for transitioning to stop tag.
        last_tag_index = input_mask.sum(0).long() - 1
        last_tags = torch.gather(target_tags, 0, last_tag_index.view(1, batch_size)).view(-1)

        # end_tags
        end_tags = torch.LongTensor([self.end_idx] * batch_size)
        end_tags = end_tags.to(device) if torch.cuda.is_available() else end_tags
        last_transition = self.log_transitions[last_tags, end_tags]

        # Add the last input if its not masked.
        last_inputs = input_features[-1]
        last_input_score = last_inputs.gather(1, last_tags.view(-1, 1))
        last_input_score = last_input_score.squeeze()

        scores = scores + last_transition + last_input_score * input_mask[-1]

        # Sum over sequence length.
        return scores
{% endhighlight %}

# <span style="color:#2874A6">Sources</span>

Natalia Silveira and Timothy Dozat and Marie-Catherine de Marneffe and Samuel Bowman and
    Miriam Connor and John Bauer and Christopher D. Manning (2014).
    <a href="https://www.aclweb.org/anthology/L14-1067/" target="_blank">*A Gold Standard Dependency Corpus for English*</a>
    
Xuezhe Ma and Eduard H. Hovy. 2016. <a href="https://arxiv.org/pdf/1603.01354.pdf" target="_blank">*End-to-end
sequence labeling via bi-directional LSTM-CNN-CRF*</a>. In ACL.

# <span style="color:#2874A6">Further Reading</span>

...
