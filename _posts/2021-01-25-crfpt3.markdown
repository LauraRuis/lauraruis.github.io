---
layout: post
title:  "Structured Prediction part three - Training a linear-chain CRF"
date:   2021-01-25 13:09:17 -0500
usemathjax: true
---
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

![annotated_example](/images/opener_gif/opener.gif)

In this final part of the series on structured prediction with linear-chain CRFs we will use our implementation from [part two]({% post_url 2021-01-25-crfpt2 %})
to train a model on real data.
To learn such a model, we need a dataset with examples consisting of input sentences annotated with POS tags.
We will choose the <a href="http://universaldependencies.org/" target="_blank">Universal Dependencies</a> dataset (<a href="https://www.aclweb.org/anthology/L14-1067/" target="_blank">Silveira et al., 2014</a>).

Then all the things we need to implement are:

- A `Vocabulary` to convert from strings to numerical values for computational models.

- A `TaggingDataset` to convert all our data to `Tensors` that can be processed by <a href="https://pytorch.org/" target="_blank">PyTorch</a>.

- A `train()` loop to train our CRF and feature-extractor end-to-end on data.

- A `test()` loop to test a trained model on new data.

# <span style="color:#C0392B">Imports</span>
Let's install and import the libraries we need (`TorchNLP` isn't part of the default runtime in Google Colab,
which I used for the implementation):

`!pip install torchnlp pytorch-nlp`

{% highlight python %}
import torch.nn as nn
import torch
import torchnlp
from torchnlp.datasets import ud_pos_dataset

from typing import List, Tuple, Iterator
from torch import Tensor
from collections import defaultdict, Counter
import numpy as np

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
{% endhighlight %}

To make sure that CUDA is in fact available (which is definitely nice and maybe even necessary for training
on the universal dependencies dataset), Google Colab offers sessions with a GPU! Select this in the runtime in the top-right
corner if you're coding everything yourself.

## <span style="color:#C0392B">The Vocabulary & Dataset</span>

First, we'll implement the vocabulary, which is a class that reads sentences as lists of strings, and
converts them to indices. Something pragmatical for sequence prediction with neural methods is that we often use an `<UNK>`-token.
In our training set, if a word occurs very infrequently, we probably cannot learn meaningful embeddings for it
and we can replace the occurrences of that word by `<UNK>`. This means that we will learn a kind of average embedding
for all infrequent words, and we can use this token again at test-time. At test-time there will inevitably be words
that don't occur in the training set, and since we don't have trained embeddings for those, they will map onto the `<UNK>`-token.

Both the code for the `Vocabulary` and the `TaggingDataset` below is very straightforward, so if you're familiar with
these kind of methods just skip them and go to the part below where we look at the Universal Dependencies dataset.

{% highlight python %}
class Vocabulary(object):
    """
    Object that maps words to indices to be processed by numerical models.
    """

    def __init__(self, 
                 pad_token="<PAD>",
                 unk_token="<UNK>"):
      """
      <PAD> and <UNK> tokens are by construction idxs 0 and 1.
      """
      self.pad_token = pad_token
      self.unk_token = unk_token
      self._idx_to_word = [pad_token, unk_token]
      self._word_to_idx = defaultdict(
                lambda: self._idx_to_word.index(self.pad_token))
      self._word_to_idx[unk_token] = 1
      self._word_frequencies = Counter()

    def word_to_idx(self, word: str) -> int:
      if word not in self._word_to_idx:
        return self._word_to_idx[self.unk_token]
      else:
        return self._word_to_idx[word]

    def idx_to_word(self, idx: int) -> str:
      return self._idx_to_word[idx]
    
    @property
    def unk_idx(self):
      return self.word_to_idx(self.unk_token)

    @property
    def size(self) -> int:
      return len(self._idx_to_word)
    
    @property
    def pad_idx(self):
      return self.word_to_idx(self.pad_token)

    def add_sentence(self, sentence: List[str]):
      # In this part of the code we read the sentences
      # and if a word in the sentence is already in
      # the vocab we just increase the counter,
      # if it's not we initialize a new index for it.
      for word in sentence:
        if word not in self._word_to_idx:
          self._word_to_idx[word] = self.size
          self._idx_to_word.append(word)
        self._word_frequencies[word] += 1
    
    def most_common(self, n=10):
      return self._word_frequencies.most_common(n=n)
{% endhighlight %}

We will use the above `Vocabulary`-class twice in the following, once for the input data consisting of words,
and once for the target data consisting of POS tags. Then the next class to implement is the class that holds the `TaggingDataset`:

{% highlight python %}
class TaggingDataset(object):
  """
  A class to hold data pairs of input words and target tags.
  """

  def __init__(self, data: List[Tuple[List, List]]):
    self._examples = []
    self._example_lengths = []
    self._test_examples = []
    self._test_example_lengths = []
    self._input_vocabulary = Vocabulary()
    self._target_vocabulary = Vocabulary()
    self.read_dataset(data)

  def read_dataset(self, input_data: List[Tuple[List, List]]):
    """Convert each example to a tensor and save it's length."""
    for input_list, target_list in input_data:
      assert len(input_list) == len(target_list), "Invalid data example."
      self._input_vocabulary.add_sentence(input_list)
      self._target_vocabulary.add_sentence(target_list)
      input_array = self.sentence_to_array(input_list, vocabulary="input")
      target_array = self.sentence_to_array(target_list, vocabulary="target")
      input_tensor = torch.tensor(input_array, dtype=torch.long, device=device)
      target_tensor = torch.tensor(target_array, dtype=torch.long, device=device)
      self._example_lengths.append(len(input_tensor))
      self._examples.append({"input_tensor": input_tensor.unsqueeze(0),
                             "target_tensor": target_tensor.unsqueeze(0)})

  def read_testset(self, input_data: List[Tuple[List, List]]):
    """Convert each example to a tensor and save it's lenght, convert unknown
    tokens to <UNK>."""
    for input_list, target_list in input_data:
      assert len(input_list) == len(target_list), "Invalid data example."
      input_array = self.sentence_to_array(input_list, vocabulary="input")
      target_array = self.sentence_to_array(target_list, vocabulary="target")
      input_tensor = torch.tensor(input_array, dtype=torch.long, device=device)
      target_tensor = torch.tensor(target_array, dtype=torch.long, device=device)
      self._test_example_lengths.append(len(input_tensor))
      self._test_examples.append({"input_tensor": input_tensor.unsqueeze(0),
                                  "target_tensor": target_tensor.unsqueeze(0)})

  def get_vocabulary(self, vocabulary: str) -> Vocabulary:
    if vocabulary == "input":
      vocab = self._input_vocabulary
    elif vocabulary == "target":
      vocab = self._target_vocabulary
    else:
      raise ValueError(
          "Specified unknown vocabulary in sentence_to_array: {}".format(
              vocabulary))
    return vocab
  
  def print_stats(self):
    print("Number of training examples in dataset: %d\n" % len(self._examples))
    print("Number of testing examples in dataset: %d\n" % len(
        self._test_examples))
    print("Input vocabulary size: %d" % self._input_vocabulary.size)
    print("Most common input tokens: ", self._input_vocabulary.most_common(5))
    print("\nTarget vocabulary size: %d" % self._target_vocabulary.size)
    print("Most common target tokens: ", self._target_vocabulary.most_common(5))
    if len(self._examples) > 0:
      print("\nTraining Example: ")
      self.print_example(0)
    if len(self._test_examples) > 0:
      print("\nTesting Example: ")
      self.print_example(0, train=False)

  def get_training_example(self, idx: int):
    if idx >= len(self._examples):
      raise ValueError("Dataset has no example at idx %d" % idx)
    input_tensor = self.array_to_sentence(self._examples[idx]["input_tensor"],
                                          "input")
    target_tensor = self.array_to_sentence(self._examples[idx]["target_tensor"],
                                           "target")
    return input_tensor, target_tensor
  
  def get_test_example(self, idx: int):
    if idx >= len(self._test_examples):
      raise ValueError("Test dataset has no example at idx %d" % idx)
    input_tensor = self.array_to_sentence(
        self._test_examples[idx]["input_tensor"], "input")
    target_tensor = self.array_to_sentence(
        self._test_examples[idx]["target_tensor"], "target")
    return input_tensor, target_tensor

  def print_example(self, idx: int, train=True):
    if train:
      input_tensor, target_tensor = self.get_training_example(idx)
    else:
      input_tensor, target_tensor = self.get_test_example(idx)
    print(" ".join(target_tensor))
    print(" ".join(input_tensor))
    
  def sentence_to_array(self, sentence: List[str], 
                        vocabulary: str) -> List[int]:
    """
    Convert each string word in a sentence to the corresponding integer from 
    the vocabulary.
    :param sentence: the sentence in words (strings).
    :param vocabulary: whether to use the input or target vocabulary.
    :return: the sentence in integers.
    """
    vocab = self.get_vocabulary(vocabulary)
    sentence_array = []
    for word in sentence:
      sentence_array.append(vocab.word_to_idx(word))
    return sentence_array

  def array_to_sentence(self, sentence_array: List[int],
                        vocabulary: str) -> List[str]:
    """
    Translate each integer in a sentence array to the corresponding word.
    :param sentence_array: array with integers representing words 
            from the vocabulary.
    :param vocabulary: whether to use the input or target vocabulary.
    :return: the sentence in words.
    """
    vocab = self.get_vocabulary(vocabulary)
    return [vocab.idx_to_word(word_idx) 
                for word_idx in sentence_array.squeeze(dim=0)]

  def get_batch(self, batch_size=2,
                train=True) -> Tuple[torch.Tensor, List[int], torch.Tensor]:
    if train:
      all_examples = self._examples
      all_example_lengths = self._example_lengths
    else:
      all_examples = self._test_examples
      all_example_lengths = self._test_example_lengths
     
    # Loop over all the data and yield a batch at every iteration.
    for example_i in range(0, len(all_examples), batch_size):
      examples = all_examples[example_i:example_i + batch_size]
      example_lengths = all_example_lengths[example_i:example_i + batch_size]
      
      # This is the part where we make sure the examples are sorted
      # in descending length.
      if len(set(example_lengths)) > 1:
        examples.sort(reverse=True, key=lambda x: len(x["input_tensor"][0]))
        example_lengths.sort(reverse=True)
      max_length = np.max(example_lengths)
      input_batch = []
      target_batch = []
      
      # Pad the examples.
      for example in examples:
          to_pad = max_length - example["input_tensor"].size(1)
          padded_input = torch.cat([
              example["input_tensor"],
              torch.zeros(int(to_pad), dtype=torch.long, 
                          device=device).unsqueeze(0)], dim=1)
          padded_target = torch.cat([
              example["target_tensor"],
              torch.zeros(int(to_pad), dtype=torch.long, 
                          device=device).unsqueeze(0)], dim=1)
          input_batch.append(padded_input)
          target_batch.append(padded_target)

      yield (torch.cat(input_batch, dim=0), example_lengths, 
             torch.cat(target_batch, dim=0))
{% endhighlight %}

Now we'll grab the UD dataset from the awesome
<a href="https://pytorchnlp.readthedocs.io/en/latest/source/torchnlp.datasets.html" target="_blank">TorchNLP</a> library, which has been made *incredibly* easy:

`ud_dataset = ud_pos_dataset(train=True, test=True)`

Let's pass the data we want to our `TaggingDataset` and look at some stats. Now `ud_dataset` holds the training and test
set of Universal Dependencies, which are both lists of data points, where each data point is a dict with the `tokens` (AKA the input sequence),
the `ud_tags` (the UPOS tags), and the `ptb_tags` (the Penn Treebank tags). For our class we need to put these in a list
of tuples with the input sequence and target tags. We'll choose as targets the `ud_tags`, since there are less classes for those
than the Penn Treebank tags, and the task is a bit easier. 

{% highlight python %}
training_data_ud = [(example["tokens"], example["ud_tags"]) 
                        for example in ud_dataset[0]]
test_data_ud = [(example["tokens"], example["ud_tags"]) 
                        for example in ud_dataset[1]]
tagging_set = TaggingDataset(data=training_data_ud)
tagging_set.read_testset(test_data_ud)
tagging_set.print_stats()
{% endhighlight %}

{% highlight terminal %}
Number of training examples in dataset: 12543

Number of testing examples in dataset: 2077

Input vocabulary size: 19674
Most common input tokens:  [('.', 8640), ('the', 8152), (',', 7021), ('to', 5076),
                            ('and', 4855)]

Target vocabulary size: 19
Most common target tokens:  [('NOUN', 34781), ('PUNCT', 23679), ('VERB', 23081),
                             ('PRON', 18577), ('ADP', 17638)]

Training Example: 
PROPN PUNCT PROPN PUNCT ADJ NOUN VERB PROPN PROPN PROPN PUNCT PROPN PUNCT DET NOUN ADP DET NOUN ADP DET NOUN ADP PROPN PUNCT ADP DET ADJ NOUN PUNCT
Al - Zaman : American forces killed Shaikh Abdullah al - Ani , the preacher at the mosque in the town of Qaim , near the Syrian border .

Testing Example: 
PRON SCONJ PROPN VERB ADP PROPN PUNCT
What if Google <UNK> Into <UNK> ?

{% endhighlight %}

As we can see above, we have `12543` training examples, `2077` testing examples, and an input vocabulary of size `19674`.
The most common input tokens are generally common tokens and their POS tags.

## <span style="color:#C0392B">Training & Testing</span>

We can now train our bi-LSTM-CRF on the Universal Dependencies training data! We'll use Adam optimizer with parameters
taken from <a href="https://arxiv.org/abs/1603.01354" target="_blank">Ma & Hovy</a>.
Below, each `epoch` loops over the entire dataset and puts each batch in the data through our model,
calculating the loss and taking a gradient step for the batch.

We're going to choose a batch size of 50 as opposed to 10 in Ma & Hovy, because we want to train a bit faster and don't
care too much about performance now. We anyway will never achieve the same performance as in Ma & Hovy, because they
use many more things that increase performance, most importantly probably a character-based CNN and pre-trained word embeddings.
If we were to optimize for performance we would do many more things than discussed here, some of which we'll briefly discuss
below in the section Disclaimers.

{% highlight python %}
def train(data: TaggingDataset, model: ChainCRF, batch_size: int, 
          num_epochs: int):
  """
  :param data: a TaggingDataset filled with training data.
  :param model: an initialized tagger model.
  :param batch_size: a minibatch size.
  :param num_epochs: how many times to go over the entire training data.
  """
  trainable_parameters = [p for p in model.parameters() if p.requires_grad]
  optimizer = torch.optim.Adam(trainable_parameters,
                               lr=1e-3, betas=(0.9, 0.9))
  for epoch in range(num_epochs):
    if (epoch + 1) % 10 == 0:
      print("Epoch %d" % (epoch + 1))
      print("Epoch loss: ", epoch_loss / num_iterations)
    epoch_loss = 0
    num_iterations = 0
    for iteration, (input_sequence, 
                    example_lengths,
                    target_sequence) in enumerate(
                                         data.get_batch(batch_size=batch_size)):
      input_mask = (input_sequence > 0).long()
      input_lengths = torch.tensor(example_lengths))
      batch_loss, score, output_tags = model(input_sequence=input_sequence, 
                                             target_sequence=target_sequence, 
                                             input_mask=input_mask,
                                             input_lengths=input_lengths
      loss = torch.mean(batch_loss)
      loss.backward()
      optimizer.step()
      optimizer.zero_grad()
      epoch_loss += loss.item()
      num_iterations += 1
{% endhighlight %}

We will also implement the testing loop so below we can immediately test our trained model on the unseen data.

{% highlight python %}
def test(data: TaggingDataset, model: ChainCRF):
  """
  Loops over the test data in `data`, calculates the best scoring
  tag sequence according to the trained `model` for each example,
  and returns the sequences, predictions, and the accuries for
  all examples.
  :param data: An instance of TaggingDataset containing test data.
  :param model: A (trained) ChainCRF.
  
  Returns: a tuple of inputs, targets, predicted targets, accuracies, 
            and mean accuracy.
  """
  input_sequences = []
  target_sequences = []
  decoded_tags = []
  accuracies = []
  total_accs = 0
  n_examples = 0
  for i, (input_sequence, 
          example_lengths, 
          target_sequence) in enumerate(data.get_batch(batch_size=1,
                                                       train=False)):
      # Save the sequences in string-form instead of numerical form.
      input_sequences.append(data.array_to_sentence(input_sequence, "input"))
      target_sequences.append(data.array_to_sentence(target_sequence, "target"))
      input_mask = (input_sequence > 0).long()
      input_lengths = torch.tensor(example_lengths))
      
      # Get the predicted output sequence of tags.
      batch_loss, score, output_tags = model(input_sequence=input_sequence, 
                                             target_sequence=target_sequence, 
                                             input_mask=input_mask,
                                             input_lengths=input_lengths
      
      # Calculate the accuracy and save it.
      target_sequence = target_sequence[0]
      accuracy = (target_sequence 
                   == output_tags[0]).long().sum().float() / len(output_tags[0])
      accuracies.append(accuracy.item())
      total_accs += accuracy.item()
      n_examples += 1
      decoded_tags.append(data.array_to_sentence(output_tags, "target"))
  mean_acc = total_accs / n_examples
  return input_sequences, target_sequences, decoded_tags, accuracies, mean_acc
{% endhighlight %}

Allright, now let's initialize our model and train it for 30 epochs. 

{% highlight python %}
model = Tagger(input_vocabulary=tagging_set.get_vocabulary("input"),
               target_vocabulary=tagging_set.get_vocabulary("target"),
               embedding_dimension=50, 
               hidden_dimension=200)
model.to(device)  # Get the model to the GPU.
train(data=tagging_set, model=model, batch_size=50, num_epochs=30)
{% endhighlight %}

The loss seems to steadily go down each epoch. We can test our model on the test data of the UD dataset and have
a look at the mean accuracy per example. The accuracy for one example is calculated (above in the test loop) as:

$$
\text{acc} = \frac{1}{m}\sum_{t=1}^{m} \mathbb{1}(\hat{y}_t, y_t)
$$

Where $$\mathbb{1}(\hat{y}_t, y_t)$$ denotes the indicator function that equals 1 if the current predicted tag $$\hat{y}_t$$ equals
the ground-truth target tag $$y_t$$
and 0 otherwise. The mean accuracy is then the mean of this metric over the entire testset.

# <span style="color:#C0392B">Disclaimers</span>

There are many things that are actually good practice in deep learning, or things that might
 improve performance, that we didn't do here. For example, for every epoch we looped over that data in the same order,
 making it not really SGD. We didn't optimize at all for hyperparameters and randomly chose some things.
Dropout, learning rate decay, pre-trained embedings, etc.

# <span style="color:#2874A6">Sources</span>

Natalia Silveira and Timothy Dozat and Marie-Catherine de Marneffe and Samuel Bowman and
    Miriam Connor and John Bauer and Christopher D. Manning (2014).
    <a href="https://www.aclweb.org/anthology/L14-1067/" target="_blank">*A Gold Standard Dependency Corpus for English*</a>